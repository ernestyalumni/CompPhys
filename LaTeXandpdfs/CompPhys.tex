% file: CompPhys.tex
% Computational Physics (notes), in unconventional ``grande'' format; fitting a widescreen format
% 
% github        : ernestyalumni
% gmail         : ernestyalumni 
% linkedin      : ernestyalumni 
% wordpress.com : ernestyalumni
%
% This code is open-source, governed by the Creative Common license.  Use of this code is governed by the Caltech Honor Code: ``No member of the Caltech community shall take unfair advantage of any other member of the Caltech community.'' 

\documentclass[10pt]{amsart}
\pdfoutput=1
\usepackage{mathtools,amssymb,lipsum,caption}

\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage[table]{xcolor}
\usepackage{pdfpages}
\usepackage{tikz}
\usetikzlibrary{matrix,arrows}

\usepackage{multicol}

\hypersetup{colorlinks=true,citecolor=[rgb]{0,0.4,0}}

\oddsidemargin=15pt
\evensidemargin=5pt
\hoffset-45pt
\voffset-55pt
\topmargin=-4pt
\headsep=5pt
\textwidth=1120pt
\textheight=595pt
\paperwidth=1200pt
\paperheight=700pt
\footskip=40pt








\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
%\newtheorem*{main}{Main Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}

\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

\newenvironment{claim}[1]{\par\noindent\underline{Claim:}\space#1}{}
\newenvironment{claimproof}[1]{\par\noindent\underline{Proof:}\space#1}{\hfill $\blacksquare$}

%This defines a new command \questionhead which takes one argument and
%prints out Question #. with some space.
\newcommand{\questionhead}[1]
  {\bigskip\bigskip
   \noindent{\small\bf Question #1.}
   \bigskip}

\newcommand{\problemhead}[1]
  {
   \noindent{\small\bf Problem #1.}
   }

\newcommand{\exercisehead}[1]
  { \smallskip
   \noindent{\small\bf Exercise #1.}
  }

\newcommand{\solutionhead}[1]
  {
   \noindent{\small\bf Solution #1.}
   }


\title{Computational Physics: includes Parallel Computing/Parallel Programming}
\author{Ernest Yeung \href{mailto:ernestyalumni@gmail.com}{ernestyalumni@gmail.com}}
\date{23 mai 2016}
\keywords{Computational Physics, Parallel Computing, Parallel Programming}
\begin{document}

\definecolor{darkgreen}{rgb}{0,0.4,0}
\lstset{language=C++,
  keywordstyle=\color{blue},
  stringstyle=\color{red},
 commentstyle=\color{darkgreen}
 }
%\lstlistoflistings

\maketitle

\tableofcontents


\begin{multicols*}{2}

\begin{abstract}
Everything about Computational Physics, including Parallel computing/ Parallel programming.  
\end{abstract}

\part{Introduction}

\section{Parallel Computing}

\subsection{Udacity Intro to Parallel Programming : Lesson 1 - The GPU Programming Model}

Owens and Luebki pound fists at the end of this video.  $=))))$  \href{https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/658304810923}{Intro to the class}.

\subsubsection{Running CUDA locally}
Also, \href{https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/658304810923}{Intro to the class}, in Lesson 1 - The GPU Programming Model, has links to documentation for running CUDA locally; in particular, for Linux: \url{http://docs.nvidia.com/cuda/cuda-getting-started-guide-for-linux/index.html}.  That guide told me to go download the NVIDIA CUDA Toolkit, which is the \href{NVIDIA CUDA Developer Toolkit}{https://developer.nvidia.com/cuda-downloads}.  

For \emph{Fedora}, I chose Installer Type \verb|runfile (local)|.  

Afterwards, installation of CUDA on Fedora 23 workstation had been nontrivial.  Go see either my github repository \href{https://github.com/ernestyalumni/MLgrabbag/blob/master/README.md}{MLgrabbag} (which will be updated) or my \href{https://ernestyalumni.wordpress.com/2016/05/07/fedora-23-workstation-linuxnvidia-geforce-gtx-980-ti-my-experience-log-of-what-i-do-and-find-out/#CUDAinstall}{wordpress blog} (which may not be upgraded frequently).  


$P=VI = I^2R$ heating.

\subsubsection{Definitions of Latency and throughput (or bandwidth)}

cf. 
\href{https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/669874580923}{Building a Power Efficient Processor}

\href{https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/667559300923}{Latency vs Bandwidth}

latency $[\text{sec}]$.  From the title ``Latency vs. bandwidth'', I'm thinking that throughput $=$ bandwidth (???).  throughput $ = $ job$/$time (of job).  

Given total task, velocity $v$, \\
total task $/v = $ latency.  throughput $=$ latency$/(\text{jobs per total task})$.  


Also, in \href{https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/669874580923}{Building a Power Efficient Processor}.  Owens recommends the article David Patterson, ``Latency...''

cf. \href{https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/671181630923}{GPU from the Point of View of the Developer}

$n_{\text{core}} \equiv $ number of cores \\
$n_{\text{vecop}} \equiv$ ($n_{\text{vecop}}-$wide axial vector operations$/core$ core) \\
$n_{\text{thread}} \equiv $ threads$/$core (hyperthreading)
\[
n_{\text{core}} \cdot n_{\text{vecop}} \cdot n_{\text{thread}}  \text{ parallelism  }
\]

There were various websites that I looked up to try to find out the capabilities of my video card, but so far, I've only found these commands (and I'll print out the resulting output):
{\scriptsize
\begin{lstlisting}
$ lspci -vnn | grep VGA -A 12
03:00.0 VGA compatible controller [0300]: NVIDIA Corporation GM200 [GeForce GTX 980 Ti] [10de:17c8] (rev a1) (prog-if 00 [VGA controller])
	Subsystem: eVga.com. Corp. Device [3842:3994]
	Physical Slot: 4
	Flags: bus master, fast devsel, latency 0, IRQ 50
	Memory at fa000000 (32-bit, non-prefetchable) [size=16M]
	Memory at e0000000 (64-bit, prefetchable) [size=256M]
	Memory at f0000000 (64-bit, prefetchable) [size=32M]
	I/O ports at e000 [size=128]
	[virtual] Expansion ROM at fb000000 [disabled] [size=512K]
	Capabilities: <access denied>
	Kernel driver in use: nvidia
	Kernel modules: nouveau, nvidia

$ lspci | grep VGA -E
03:00.0 VGA compatible controller: NVIDIA Corporation GM200 [GeForce GTX 980 Ti] (rev a1)

$ grep driver /var/log/Xorg.0.log
[    18.074] Kernel command line: BOOT_IMAGE=/vmlinuz-4.2.3-300.fc23.x86_64 root=/dev/mapper/fedora-root ro rd.lvm.lv=fedora/root rd.lvm.lv=fedora/swap rhgb quiet LANG=en_US.UTF-8 nouveau.modeset=0 rd.driver.blacklist=nouveau nomodeset gfxpayload=vga=normal
[    18.087] (WW) Hotplugging is on, devices using drivers 'kbd', 'mouse' or 'vmmouse' will be disabled.
[    18.087] 	X.Org XInput driver : 22.1
[    18.192] (II) Loading /usr/lib64/xorg/modules/drivers/nvidia_drv.so
[    19.088] (II) NVIDIA(GPU-0): Found DRM driver nvidia-drm (20150116)
[    19.102] (II) NVIDIA(0):     ACPI event daemon is available, the NVIDIA X driver will
[    19.174] (II) NVIDIA(0): [DRI2]   VDPAU driver: nvidia
[    19.284] 	ABI class: X.Org XInput driver, version 22.1
...

$ lspci -k | grep -A 8 VGA
03:00.0 VGA compatible controller: NVIDIA Corporation GM200 [GeForce GTX 980 Ti] (rev a1)
	Subsystem: eVga.com. Corp. Device 3994
	Kernel driver in use: nvidia
	Kernel modules: nouveau, nvidia
03:00.1 Audio device: NVIDIA Corporation GM200 High Definition Audio (rev a1)
	Subsystem: eVga.com. Corp. Device 3994
	Kernel driver in use: snd_hda_intel
	Kernel modules: snd_hda_intel
05:00.0 USB controller: VIA Technologies, Inc. VL805 USB 3.0 Host Controller (rev 01)
  \end{lstlisting}
}
\href{https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/671181640923}{CUDA Program Diagram}

\[
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=3em, column sep=4em, minimum width=1em]
  {
    & \text{ CUDA program in C with extensions } &  \\
    \text{ CPU ``Host'' } & & \text{ GPU ``Device'' } \\
    \text{ Memory } & & \text{Memory} \\
};
  \path[->]
  (m-1-2) edge node [above] {} (m-2-1)
  edge node [above] {} (m-2-3)
  (m-2-1) edge node [above] { coprocessor } (m-2-3)
  edge node [above] {} (m-3-1)
  (m-3-1) edge node [above] {$1$} (m-3-3)
  (m-3-3) edge [bend left=20] node [below] {$2$} (m-3-1)
  edge [loop right] node [right] {$3$} (m-3-3)
  (m-2-3) edge [loop right] node [right] {$4$} (m-2-3)
  ;
\end{tikzpicture}
\]
CPU ``host'' is the boss (and issues commands) -Owen.

$\text{Coprocessor} : \text{ CPU ``host'' } \to \text{ GPU ``device'' } $ \\
$\text{Coprocessor} : \text{ CPU process } \mapsto \text{ (co)-process out to GPU } $ \\

With
\begin{enumerate}
  \item[1] data cpu $\to $ gpu
  \item[2] data gpu $\to$ cpu \qquad (initiated by cpu host) \\

$1.,2.,$ uses \verb|cudaMemcpy| 
  \item[3] allocate GPU memory: \verb|cudaMalloc|
  \item[4] launch kernel on GPU
  \end{enumerate}
Remember that for 4., this launching of the kernel, while it's acting on GPU ``device'' onto itself, it's initiated by the boss, the CPU ``host''.

Hence, cf. \href{https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/670489380923}{Quiz: What Can GPU Do in CUDA}, GPUs can respond to CPU request to receive and send Data CPU $\to $ GPU and Data GPU $\to $ CPU, respectively (1,2, respectively), and compute a kernel launched by the CPU (3).


\href{https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/670742800923}{A CUDA Program}
A typical GPU program

\begin{itemize}
\item \verb|cudaMalloc| - CPU allocates storage on GPU 
\item \verb|cudaMemcpy| - CPU copies input data from CPU $\to $ GPU 
\item \emph{kernel launch} - CPU launches kernel(s) on GPU to process the data 
\item \verb|cudaMemcpy| - CPU copies results back to CPU from GPU
  \end{itemize}

Owens advises minimizing ``communication'' as much as possible (e.g. the \verb|cudaMemcpy| between CPU and GPU), and do a lot of computation in the CPU and GPU, each separately.

\href{https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/672300540923}{Defining the GPU Computation}

Owens circled this
{\Large
  \[
  \begin{gathered}
\text{ BIG IDEA } \qquad \, \boxed{ \text{ This is Important } }  \\
\begin{aligned} 
& \text{ Kernels look like serial programs } \\
  & \text{ Write your program as if it will run on \textbf{ one } thread } \\
  & \text{The GPU will run that program on \textbf{ many } threads}
  \end{aligned}
\end{gathered}
\]
}

\href{https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/670742840923}{Squaring A Number on the CPU}

Note
\begin{enumerate}
\item Only 1 thread of execution: (``thread'' $:=$ one independent path of execution through the code) e.g. the \verb|for| loop
  \item no explicit parallelism; it's serial code e.g. the \verb|for| loop through 64 elements in an array
  \end{enumerate}


\href{https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/670742870923}{GPU Code A High Level View}

CPU:
\begin{itemize}
  \item Allocate Memory 
  \item Copy Data to/from GPU
    \item Launch Kernel - species degree of parallelism
\end{itemize}

GPU:
\begin{itemize}
\item Express Out $=$ In $\cdot $ In  - says \emph{nothing} about the degree of parallelism
  \end{itemize}

Owens reiterates that in the GPU, everything looks serial, but it's only in the CPU that anything parallel is specified.  

pseudocode: CPU code: square kernel $<<< 64 >>>$ (outArray,inArray)

\href{https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/670742940923}{Squaring Numbers Using CUDA Part 3}

From the example
\begin{lstlisting}
  // launch the kernel
  square<<<1, ARRAY_SIZE>>>(d_out, d_in)
  \end{lstlisting}
we're introduced to the ``CUDA launch operator'', initiating a kernel of 1 block of 64 elements (\verb|ARRAY_SIZE| is 64) on the GPU.  Remember that \verb|d_| prefix (this is naming comvention) tells us it's on the device, the GPU, solely.  

With CUDA launch operator $\equiv <<<>>>$, then also looking at this explanation on \verb|stackexchange| (so surely others are confused as well, of those who are learning this (cf. \href{http://stackoverflow.com/questions/19240658/cuda-kernel-launch-parameters-explained-right}{CUDA kernel launch parameters explained right?}).  From \href{http://stackoverflow.com/users/1957265/eric}{Eric}'s answer, \\

threads are grouped into blocks.  all the threads will execute the invoked kernel function.

Certainly,
\[
\begin{aligned}
  & <<<>>>:(n_{\text{block}}, n_{\text{threads}})\times \text{kernelfunctions} \mapsto \text{kernelfunction}<<<n_{\text{block}},n_{\text{threads}}>>> \in \text{End}:\text{Dat}_{\text{GPU}} \\ 
  & <<<>>>: \mathbb{N}^+ \times \mathbb{N}^+ \times \text{Mor}_{\text{GPU}} \to \text{End}\text{Dat}_{\text{GPU}}
  \end{aligned}
\]
where I propose that GPU can be modeled as a category containing objects $\text{Dat}_{\text{GPU}}$, the collection of all possible data inputs and outputs into the GPU, and $\text{Mor}_{\text{GPU}}$, the collection of all kernel functions that run (exclusively, and this \emph{must} be the class, as reiterated by Prof. Owen) on the GPU.

Next,
\[
\begin{aligned}
  & \text{kernelfunction}<<<n_{\text{block}},n_{\text{threads}}>>>: \text{din}\mapsto \text{dout} \qquad \, (\text{as given in the ``square'' example, and so I propose}) \\ 
  & \text{kernelfunction}<<<n_{\text{block}},n_{\text{threads}}>>>:(\mathbb{N}^+)^{n_{\text{threads}}} \to (\mathbb{N}^+)^{n_{\text{threads}}}
  \end{aligned}
\]
But keep in mind that $\text{dout}$, $\text{din}$ are pointers in the C program, pointers to the place in the memory.  

\verb|cudaMemcopy| is a functor category, s.t. e.g. $\text{Obj}_{\text{CudaMemcopy}} \ni \text{cudaMemcpyDevicetoHost}$ where
\[
\text{cudaMemcopy}(-,-,n_{\text{thread}},\text{cudaMemcpyDeviceToHost}): \text{Memory}_{\text{GPU}} \to \text{Memory}_{\text{CPU}} \in \text{Hom}(\text{Memory}_{\text{GPU}}, \text{Memory}_{\text{CPU}})
\]

\href{https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/670742910923}{Squaring Numbers Using CUDA 4}

Note the C language construct \emph{declaration specifier} - denotes that this is a kernel (for the GPU) and not CPU code.  Pointers need to be allocated on the GPU (otherwise your program will crash spectacularly -Prof. Owen).  

\subsubsection{What are C pointers?}

Is $\langle \text{ type } \rangle \, *$, a pointer, then a mapping from the category, namely the objects of types, to a mapping from the specified value type to a memory address?

e.g.
\[
\begin{aligned}
  \langle \, \rangle \, * & : \text{float} \mapsto \text{float} \, * \\ 
  \text{float } \, * & : \text{din} \mapsto \text{ some memory address }
\end{aligned}
\]
and then we pass in mappings, not values, and so we're actually declaring a square \emph{functor}.

What is \verb|threadIdx|?  What is it mathematically?  Consider that $\exists \,$ 3 ``modules'':

\[
\begin{aligned}
  & \text{threadIdx}.x \\
  & \text{threadIdx}.y \\
  & \text{threadIdx}.z 
\end{aligned}
\]
And then the line
\begin{lstlisting}
int idx = threadIdx.x;
  \end{lstlisting}
says that idx is an integer, ``declares'' it to be so, and then assigns idx to $\text{threadIdx}.x$ which surely has to also have the same type, integer.  So (perhaps)
\[
idx \equiv \text{threadIdx}.x \in \mathbb{Z}
\]
is the same thing.

Then suppose threadIdx $\subset \mathbf{\text{FinSet}}$, a subcategory of the category of all (possible) finite sets, s.t. threadIdx has 3 particular morphisms, $x,y,z\in \text{Mor}threadIdx$,
\[
\begin{aligned}
  & x : \text{threadIdx} \mapsto \text{threadIdx}.x \in \text{Obj}_{\mathbf{\text{FinSet}}} \\ 
  & y : \text{threadIdx} \mapsto \text{threadIdx}.x \in \text{Obj}_{\mathbf{\text{FinSet}}} \\ 
  & z : \text{threadIdx} \mapsto \text{threadIdx}.x \in \text{Obj}_{\mathbf{\text{FinSet}}}  
\end{aligned}
\]

\href{https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/670742980923}{Configuring the Kernel Launch Parameters Part 1}

$n_{\text{blocks}}$, $n_{\text{threads}}$ with $n_{\text{threads}} \geq 1024$ (this maximum constant is GPU dependent).  You should pick the $(n_{\text{blocks}}, n_{\text{threads}})$ that makes sense for your problem, says Prof. Owen.  

\subsubsection{Memory layout of blocks and threads}

$\forall \, (n_{\text{blocks}}, n_{\text{threads}}) \in \mathbb{Z} \times \lbrace 1 \dots 1024 \rbrace$, $\lbrace 1 \dots n_{\text{block}} \times \lbrace 1 \dots n_{\text{threads}} \rbrace$ is now an ordered index (with lexicographical ordering).  This is just 1-dimensional (so possibly there's a 1-to-1 mapping to a finite subset of $\mathbb{Z}$).

I propose that ``adding another dimension'' or the 2-dimension, that Prof. Owen mentions is being able to do the Cartesian product, up to 3 Cartesian products, of the block-thread index.  

\href{https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/668398860923}{Quiz: Configuring the Kernel Launch Parameters 2 }

Most general syntax:

Configuring the kernel launch
\begin{lstlisting}
  kernel<<<grid of blocks, block of threads >>>(...)

  // for example

  square<<<dim3(bx,by,bz), dim3(tx,ty,tz), shmem>>>(...)
  \end{lstlisting}
where \verb|dim3(tx,ty,tz)| is the grid of blocks $bx\cdot by \cdot bz$ \\
\phantom{ where } \verb|{dim3}(tx,ty,tz)| is the block of threads $tx \cdot ty \cdot tz$ \\
\phantom{ where } \verb|shmem| is the shared memory per block in bytes


\href{https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/967066740923}{Problem Set 1}
``Also, the image is represented as an 1D array in the kernel, not a 2D array like I mentioned in the video.''

Here's part of that code for squaring numbers:
\begin{lstlisting}
  __global__ void square(float *d_out, float *d_in) {
    int idx = threadIdx.x;
    float f = d_in[idx];
    d_out[idx] = f*f;
    }
  \end{lstlisting}

\subsubsection{Grid of blocks, block of threads, thread that's indexed; (mathematical) structure of it all}

Let
\[
\begin{gathered}
  \text{grid} = \prod_{I=1}^N (\text{block})^{n_I^{\text{block}}}
\end{gathered}
\]
where $N=1,2,3$ (for CUDA) and by naming convention $\begin{aligned} & \quad \\
  & I = 1 \equiv x \\
  & I = 2 \equiv y \\
  & I = 3 \equiv z \end{aligned}$

Let's try to make it explicity (as others had difficulty understanding the grid, block, thread model, cf. \href{http://stackoverflow.com/questions/14711668/colored-image-to-greyscale-image-using-cuda-parallel-processing}{colored image to greyscale image using CUDA parallel processing}, \href{http://stackoverflow.com/questions/16619274/cuda-griddim-and-blockdim}{Cuda gridDim and blockDim}) through commutative diagrams and categories (from math):

\[
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=4em, column sep=5em, minimum width=2em]
  {
\prod_{I=1}^N \mathbb{Z}^+ & \ni (N_x^{\text{blocks}}, N_y^{\text{blocks}} , N_z^{\text{blocks}}) \\
\text{grid} & \ni \text{gridSize}(N_x^{\text{blocks}}, N_y^{\text{blocks}} , N_z^{\text{blocks}} ) \\
};
  \path[->]
  (m-1-1) edge node [right] {$\text{dim}3$} (m-2-1)
  (m-2-1) edge [bend left=40, thick] node [left] {$\text{gridDim}$} (m-1-1)
  ;
  \path[|->]
  (m-1-2) edge node [left] {$\text{dim}3$} (m-2-2)
  (m-2-2) edge [bend right=40, thick] node [right] {$(\text{gridDim}.x$, $\text{gridDim}.y$, $\text{gridDim}.z)$} (m-1-2)
  ;  
\end{tikzpicture}
\]

\[
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=4em, column sep=5em, minimum width=2em]
  {
    \text{grid} & \ni \verb|d_rgbaImage| \\
    \prod_{I=1}^N \mathbb{Z} \supset \prod_{I=1}^N \lbrace 1 \dots N_I^{\text{blocks}} \rbrace & \ni (i^{\text{blocks}},j^{\text{blocks}}, k^{\text{blocks}} ) \\
};
  \path[->]
  (m-1-1) edge node [right] {$\text{blockIdx}$} (m-2-1)
  ;
  \path[|->]
  (m-1-2) edge node [right] {($\text{blockIdx}.x$,$\text{blockIdx}.y$,$\text{blockIdx}.z$)} (m-2-2)
  ;
  \end{tikzpicture}
\]

and then similar relations (i.e. arrows, i.e. relations) go for a block of threads:

\[
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=4em, column sep=5em, minimum width=2em]
  {
\prod_{I=1}^N \mathbb{Z}^+ & \ni (N_x^{\text{threads}}, N_y^{\text{threads}} , N_z^{\text{threads}}) \\
\text{block} & \ni \text{blockSize}(N_x^{\text{threads}}, N_y^{\text{threads}} , N_z^{\text{threads}} ) \\
};
  \path[->]
  (m-1-1) edge node [right] {$\text{dim}3$} (m-2-1)
  (m-2-1) edge [bend left=40, thick] node [left] {$\text{blockDim}$} (m-1-1)
  ;
  \path[|->]
  (m-1-2) edge node [left] {$\text{dim}3$} (m-2-2)
  (m-2-2) edge [bend right=40, thick] node [right] {$(\text{blockDim}.x$, $\text{blockDim}.y$, $\text{blockDim}.z)$} (m-1-2)
  ;  
\end{tikzpicture}
\]

\[
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=4em, column sep=5em, minimum width=2em]
  {
    \mathbf{\text{block}} & \ni \text{block} \\
    \prod_{I=1}^N \mathbb{Z} \supset \prod_{I=1}^N \lbrace 1 \dots N_I^{\text{threads}} \rbrace & \ni (i^{\text{threads}},j^{\text{threads}}, k^{\text{threads}} ) \\
};
  \path[->]
  (m-1-1) edge node [right] {$\text{threadIdx}$} (m-2-1)
  ;
  \path[|->]
  (m-1-2) edge node [right] {($\text{threadIdx}.x$,$\text{threadIdx}.y$,$\text{threadIdx}.z$)} (m-2-2)
  ;
  \end{tikzpicture}
\]

\href{https://discussions.udacity.com/t/gridsize-help-assignment-1-pp/124701}{gridsize help assignment 1 Pp} explains how threads per block is variable, and remember how Owens said Luebki says that a GPU doesn't get up for more than a 1000 threads per block.  

\subsubsection{Generalizing the model of an image}

Consider vector space $V$, e.g. $\text{dim}V=4$, vector space $V$ over field $\mathbb{K}$, so $V= \mathbb{K}^{\text{dim}V}$.

Each pixel represented by $\forall \, v \in V$.

Consider an image, or space, $M$.  $\text{dim}M = 2$ (image), $\text{dim}M=3$.  Consider a local chart (that happens to be global in our case):
\[
\begin{aligned}
  & \varphi : M \to \mathbb{Z}^{\text{dim}M} \supset \lbrace 1 \dots N_1 \rbrace \times \lbrace 1 \dots N_2 \rbrace \times \dots \times \lbrace 1 \dots N_{\text{dim}M} \rbrace \\ 
  & \varphi : x \mapsto (x^1(x), x^2(x), \dots , x^{\text{dim}M}(x) )
  \end{aligned}
\]
\[
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=4em, column sep=5em, minimum width=2em]
  {
    E & M \times V \\ 
    M &  \\
};
  \path[->]
  (m-1-1) edge node [auto] {$\varphi$} (m-1-2)
          edge node [left] {$\pi$} (m-2-1)
  (m-1-2) edge node [auto] {$$} (m-2-1)
              ;
  \end{tikzpicture}
\qquad \, 
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=4em, column sep=5em, minimum width=2em]
  {
    E & \text{grid} \times \text{ block of threads } \\ 
    \text{ grid } & \\
};
  \path[->]
  (m-1-1) edge node [auto] {$\varphi$} (m-1-2)
          edge node [left] {$\pi$} (m-2-1)
  (m-1-2) edge node [auto] {$$} (m-2-1)
              ;
  \end{tikzpicture}
\]

Consider a ``coarsing'' of underlying $M$:
\[
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=2em, column sep=3em, minimum width=1em]
  {
    M\times V & \text{proj}(M) \times \text{proj}(V) \\ 
    M = \lbrace 1 \dots N_1 \rbrace \times \lbrace 1 \dots N_2 \rbrace \times \dots \times \lbrace 1 \dots N_{\text{dim}M} \rbrace  & \text{proj}(M)  = \lbrace 1 \dots \frac{N_1}{N_1^{\text{threads}} } \rbrace \times \lbrace 1 \dots \frac{N_2}{N_2^{\text{threads}} } \rbrace \times \dots \times \lbrace 1 \dots \frac{N_{\text{dim}M}}{N_{\text{dim}M}^{\text{threads}} } \rbrace \\  
};
  \path[->]
  (m-1-1) edge node [auto] {$\text{proj}$} (m-1-2)
          edge node [left] {$\pi$} (m-2-1)
  (m-1-2) edge node [auto] {$\text{proj}(\pi)$} (m-2-2)
  (m-2-1) edge node [auto] {$\text{proj}$} (m-2-2)        
          ;
  \end{tikzpicture}
\]
e.g. $\begin{aligned} & \quad \\
  & N_1^{\text{thread}} = 12 \\
  & N_2^{\text{thread}} = 12 \end{aligned}$

Just note that in terms of syntax, you have the ``block'' model, in which you allocate blocks along each dimension.  So in
\[
\begin{aligned}
  & const \; dim3 \; blockSize(n^b_x, n^b_y, n^b_z) \\
  & const \; dim3 \; gridSize(n^{\text{gr}}_x, n^{\text{gr}}_y, n^{\text{gr}}_z)
  \end{aligned}
\]
Then the condition is
$n_x^b/\text{dim}V , n_y^b/\text{dim}V, n_z^b/\text{dim}V \in \mathbb{Z}$ (condition), \qquad \, $(n_x^{\text{gr}}-1)/\text{dim}V , n_y^{\text{gr}}/\text{dim}V, n_z^{\text{gr}}/\text{dim}V \in \mathbb{Z}$

\href{https://classroom.udacity.com/courses/cs344/lessons/77202674/concepts/773931440923}{Transpose Part 1}

Now
\[
\begin{gathered}
  \begin{aligned}
    & \text{Mat}_{\mathbb{F}}(n,n) \xrightarrow{T} \text{Mat}_{\mathbb{F}}(n,n) \\ 
    & A\mapsto A^T \text{ s.t. } (A^T)_{ij} = A_{ji}
    \end{aligned} \\ 
\begin{aligned}
  &  \text{Mat}_{\mathbb{F}} \xrightarrow{T} \mathbb{F}^{n^2} \\
  & A_{ij} \mapsto A_{ij} = A_{in + j }
  \end{aligned}
\end{gathered}
\]
\[
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=2em, column sep=3em, minimum width=1em]
  {
    \text{Mat}_{\mathbb{F}}(n,n) & \mathbb{F}^{n^2} \\
    \text{Mat}_{\mathbb{F}}(n,n) & \mathbb{F}^{n^2} \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] {$$} (m-1-2)
          edge node [left] {$T$} (m-2-1)
  (m-1-2) edge node [auto] {$T$} (m-2-2)
  (m-2-1) edge node [auto] {$$} (m-2-2)        
          ;
  \end{tikzpicture} \qquad \, \begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=2em, column sep=3em, minimum width=1em]
  {
A_{ij} & A_{in+j} \\ 
(A^T)_{ij} = A_{ji} & A_{jn+i} \\
  };
  \path[|->]
  (m-1-1) edge node [auto] {$$} (m-1-2)
          edge node [left] {$T$} (m-2-1)
  (m-1-2) edge node [auto] {$T$} (m-2-2)
  (m-2-1) edge node [auto] {$$} (m-2-2)        
          ;
  \end{tikzpicture}
\]

\href{https://classroom.udacity.com/courses/cs344/lessons/77202674/concepts/773153710923}{Transpose Part 2}

Possibly, transpose is a functor.

Consider struct as a category.  In this special case, $\text{Obj}\text{struct} = \lbrace \text{arrays} \rbrace$ (a struct of arrays).  Now this struct already has a hash table for indexing upon declaration (i.e. ``creation''): so this category struct will need to be equipped with a ``diagram'' from the category of indices $J$ to struct: $J\to $ struct.

So possibly
\[
\begin{aligned}
  \text{struct} & \xrightarrow{T} & \text{ array } \\ 
 \text{Obj}\text{Struct} = \lbrace \text{ arrays } \rbrace & \xrightarrow{T} & \text{Obj}\text{array} = \lbrace \text{ struct } \rbrace \\ 
 J\to \text{ struct } & \xrightarrow{T} & J \to \text{ array } 
  \end{aligned}
\]







\href{https://classroom.udacity.com/courses/cs344/lessons/77202674/concepts/787012800923}{Quiz: What Kind Of Communication Pattern}
This quiz made a few points that clarified the characteristics of these so-called communication patterns (amongst the memory?)

\begin{itemize}
  \item map is bijective, and map $:\text{Idx} \to \text{Idx}$
  \item gather - not necessarily surjective
  \item scatter - not necessarily surjective 
  \item stencil - surjective
  \item transpose (see before)
  \end{itemize}




\href{https://classroom.udacity.com/courses/cs344/lessons/77202674/concepts/773153720923}{Parallel Communication Patterns Recap}

\begin{itemize}
\item map - bijective
\item transpose - bijective
\item gather - not necessarily surjective, and is many-to-one (by def.)
\item scatter - one-to-many (by def.) and is not necessarily surjective
\item stencil - several-to-one (not injective, by definition), and is surjective
\item reduce - all-to-one
  \item scan/sort - all-to-all
\end{itemize}

\href{https://classroom.udacity.com/courses/cs344/lessons/77202674/concepts/773153760923}{Programmer View of the GPU}

thread blocks: group of threads that cooperate to solve a (sub)problem

\href{https://classroom.udacity.com/courses/cs344/lessons/77202674/concepts/773153770923}{Thread Blocks And GPU Hardware}

CUDA GPU is a bunch of SMs:

Streaming Multiprocessors (SM)s

SMs have a bunch of simple processors and memory.

Dr. Luebki:
\[
\boxed{ \begin{gathered}
    \text{Let me say that again because it's really important} \\
    \text{GPU is responsible for allocating blocks to SMs}
  \end{gathered}
  }
\]
Programmer only gives GPU a pile of blocks.

\href{https://classroom.udacity.com/courses/cs344/lessons/77202674/concepts/787721730923}{Quiz: What Can The Programmer Specify}

I myself thought this was a revelation and was not intuitive at first:

Given a single kernel that's launched on many thread blocks include $X$, $Y$, the programmer cannot specify the sequence the blocks, e.g. block $X$, block $Y$, run (same time, or run one after the other), and which SM the block will run on (GPU does all this).  

\href{https://classroom.udacity.com/courses/cs344/lessons/77202674/concepts/787981160923}{Quiz: A Thread Block Programming Example}

Open up \verb|hello blockIdx.cu| in Lesson 2 Code Snippets (I got the repository from github, repo name is cs344).

At first, I thought you can do a single file compile and run in Eclipse without creating a new project.  No.  cf. \href{http://stackoverflow.com/questions/17164197/eclipse-creating-projects-every-time-to-run-a-single-file}{Eclipse creating projects every time to run a single file?}.  

%I opened it up in Eclipse with File, Open File (no need to create a new project).

I ended up creating a new CUDA C/C$++$ project from File -> New project, and then chose project type Executable, Empty Project, making sure to include Toolchain CUDA Toolkit (my version is 7.5), and chose an arbitrary project name (I chose cs344single).  Then, as suggested by \href{http://stackoverflow.com/users/3720356/kenny-nguyen}{Kenny Nguyen}, I dragged and dropped files into the folder, from my file directory program.

I ran the program with the ``Play'' triangle button, clicking on the green triangle button, and it ran as expected.  I also turned off Build Automatically by deselecting the option (no checkmark).

\href{https://classroom.udacity.com/courses/cs344/lessons/77202674/concepts/773883100923}{GPU Memory Model}

\[
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=4em, column sep=5em, minimum width=2em]
  {
\text{ thread } & \text{ local memory } \\
};
  \path[->]
  (m-1-2) edge node [auto] {$\text{read}$} (m-1-1)
  (m-1-2) edge [loop right] node [right] {$\text{write}$} (m-1-2)
  ;
  \end{tikzpicture}
\]

Then consider threadblock $\equiv$  thread block \\
\phantom{Then consider } $\text{Obj}\text{threadblock} \supset \lbrace \text{ threads } \rbrace$ \\
\phantom{Then consider } $\text{FinSet} \xrightarrow{ \text{ threadIdx} } \text{ thread } \in \text{Mor}\text{threadblock}$

\[
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=4em, column sep=5em, minimum width=2em]
  {
\text{ threadblock } & \text{ shared memory } \\
};
  \path[->]
  (m-1-2) edge node [auto] {$\text{read}$} (m-1-1)
  (m-1-2) edge [loop right] node [right] {$\text{write}$} (m-1-2)
  ;
  \end{tikzpicture}
\]
$\forall \, $ thread,
\[
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=4em, column sep=5em, minimum width=2em]
  {
\text{ thread } & \text{ global memory } \\
};
  \path[->]
  (m-1-2) edge node [auto] {$\text{read}$} (m-1-1)
  (m-1-2) edge [loop right] node [right] {$\text{write}$} (m-1-2)
  ;
  \end{tikzpicture}
\]

\href{https://classroom.udacity.com/courses/cs344/lessons/77202674/concepts/773883130923}{Synchronization - Barrier}

\href{https://classroom.udacity.com/courses/cs344/lessons/77202674/concepts/785776150923}{Quiz: The Need For Barriers}

3 barriers were needed (wasn't obvious to me at first).  All threads need to finish the write, or initialization, so it'll need a barrier.

While
\begin{lstlisting}
array[idx] = array[idx+1];
  \end{lstlisting}
is 1 line, it'll actually need 2 barriers; first read.  Then write.

So \emph{actually} we'll need to \emph{rewrite} this code:
\begin{lstlisting}
  int temp = array[idx+1];
  __syncthreads();
  array[idx] = temp;
  __syncthreads();
  \end{lstlisting}

kernels have implicit barrier for each.  

\href{https://classroom.udacity.com/courses/cs344/lessons/77202674/concepts/774332060923}{Writing Efficient Programs}

\begin{enumerate}
\item Maximize \emph{arithmetic intensity}
  arithmetic intensity $:= \frac{ \text{ math } }{ \text{ memory }}$
  \end{enumerate}

\href{https://classroom.udacity.com/courses/cs344/lessons/77202674/concepts/774332070923}{video: Minimize Time Spent On Memory}

local memory is fastest; global memory is slower

\[
\text{local} > \text{ shared} >> \text{global} >> \text{CPU}
\]

kernel we know (in the code) is tagged with \verb|__global__|

\href{https://classroom.udacity.com/courses/cs344/lessons/77202674/concepts/814086830923}{quiz: A Quiz on Coalescing Memory Access}

Work it out as Dr. Luebki did to figure out if it's coalesced memory access or not.  


\href{https://classroom.udacity.com/courses/cs344/lessons/77202674/concepts/774332150923}{Atomic Memory Operations}

Atomic Memory Operations

atomicadd atomicmin atomicXOR atomicCAS Compare And Swap



\section{Pointers in C; Pointers in C categorified (interpreted in Category Theory)}

Suppose $v\in \text{ObjData}$, category of data \textbf{Data}, \\
\phantom{ Suppose} e.g. $v\in \text{Int} \in \text{Obj}\mathbf{\text{Type}}$, category of types $\mathbf{\text{Type}}$.

\[
\begin{aligned}
  & \text{Data}  \xrightarrow{ \& } \text{Memory}  \\
  & v \overset{\&}{\mapsto} \& v 
\end{aligned}
\]
with address $\& v \in $ Memory.

With \\
\phantom{With } assignment $pv = \& v$,
\[
\begin{aligned}
  & pv \in \text{Obj}\text{pointer}, \, \text{ category of pointers, pointer} \\ 
  & pv \in \text{Memory} \qquad \, (\text{i.e. not $pv \in \text{Dat}$, i.e. $pv \notin \text{Dat}$})
\end{aligned}
\]

\[
\text{ pointer } \ni pv \overset{ * }{ \mapsto } *pv \in \text{Dat}
\]

\[
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=4em, column sep=5em, minimum width=2em]
  {
    v & \& v \\
    *pv & pv \\
};
  \path[|->]
  (m-1-1) edge node [auto] {$\&$} (m-1-2)
  (m-1-2) edge node [right] {$=$} (m-2-2)
  (m-2-2) edge node [auto] {$*$} (m-2-1)
  (m-2-1) edge node [left] {$==$} (m-1-1)
  ;
  \end{tikzpicture}
\qquad \, \begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=4em, column sep=5em, minimum width=2em]
  {
    \text{Data} & \text{Memory} \\
    \text{Data} & \text{pointer} \\
};
  \path[->]
  (m-1-1) edge node [auto] {$\&$} (m-1-2)
  (m-1-2) edge node [right] {$=$} (m-2-2)
  (m-2-2) edge node [auto] {$*$} (m-2-1)
  (m-2-1) edge node [left] {$==$} (m-1-1)
  ;
  \end{tikzpicture}
\]

Examples.  Consider \verb|passfunction.c| in Fitzpatrick \cite{Fitz}.

Consider the type \verb|double|, \verb|double| $\in \text{Obj}\text{Types}$.  \\
\phantom{ Consider } $\text{fun1, fun2} \in \text{Mor}\text{Types} \qquad \, \text{ namely }$ \\
\phantom{ Consider } $\text{fun1, fun2} \in \text{Hom}(\text{double},\text{double}) \equiv \text{Hom}_{\text{Types}}(\text{double},\text{double})$

Recall that
\[
\begin{aligned}
  & \text{ pointer } \xrightarrow{ * } \text{ Dat } \\ 
  & \text{ pointer } \xrightarrow{ \& } \text{ Memory }
\end{aligned}
\]
$*, \&$ are functors with domain on the category pointer.

Pointers to functions is the ``extension'' of functor $*$ to the codomain of $\text{Mor}\text{Types}$:

\[
\begin{aligned}
  & \text{ pointer} & \xrightarrow{ * } \text{Mor}\text{Types} \\ 
  & \text{ fun1 } & \overset{*}{ \mapsto } *\text{fun}1 \in \text{Hom}_{\text{Types}}(\text{double},\text{double})
  \end{aligned}
\]

\[
 \begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=4em, column sep=5em, minimum width=2em]
  {
    \text{double} & \text{Memory} \\
    \text{double} & \text{pointer} \\
    \text{double} & \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] {$\&$} (m-1-2)
  (m-1-2) edge node [right] {$\cong$} (m-2-2)
  (m-2-2) edge node [auto] {$*$} (m-2-1)
  edge node [auto] {$*$} (m-3-1)
  (m-2-1) edge node [left] {$\text{cube}$} (m-3-1)
  ;
 \end{tikzpicture} \qquad \qquad \,
  \begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=4em, column sep=5em, minimum width=2em]
  {
    \text{res1} & \&\text{res1} \\
    *\text{res1} & \text{res1} \\
    *\text{res1} = y^3 & \\ 
  };
  \path[|->]
  (m-1-1) edge node [auto] {$\&$} (m-1-2)
  (m-1-2) edge node [right] {$\cong$} (m-2-2)
  (m-2-2) edge node [auto] {$*$} (m-2-1)
  edge node [auto] {$*$} (m-3-1)
  (m-2-1) edge node [left] {$\text{cube}$} (m-3-1)
  ;
  \end{tikzpicture}
\]

It's unclear to me how \verb|void cube| can be represented in terms of category theory, as surely it cannot be represented as a mapping (it acts upon a functor, namely the $*$ functor for pointers).  It doesn't return a value, and so one cannot be confident to say there's explicitly a domain and codomain, or range for that matter.

But what is going on is that
\[
\begin{gathered}
  \text{ pointer }, \text{ double } , \text{ pointer } \xrightarrow{ \text{ cube } } \text{ pointer }, \text{ pointer } \\ 
  \text{fun}1, x , \text{res}1 \overset{\text{cube}}{\mapsto} \text{fun}1, \text{res}1
\end{gathered}
\]
s.t. $*\text{res}1 = y^3=(*\text{fun}1(x))^3$


So I'll speculate that in this case, \verb|cube| is a functor, and in particular, is acting on $*$, the so-called deferencing operator:
\[
\begin{gathered}
  \text{ pointer } \xrightarrow{ * } \text{float} \in \text{Data} \\
  \text{ res}1 \overset{*}{\mapsto} *\text{res}1
\end{gathered} \xrightarrow{ \text{ cube } } \begin{gathered}
  \text{ pointer } \xrightarrow{ \text{cube}(*) } \text{float} \in \text{Data} \\
  \text{ res}1 \overset{\text{cube}(*)}{\mapsto} \text{cube}(*\text{res}1)=y^3
\end{gathered}
\]

cf.  Arrays, from Fitzpatrick \cite{Fitz}

\[
\text{Types} \xrightarrow{ \text{ declaration } } \text{arrays}
\]
If $x\in \text{Obj}\text{arrays}$,
\[
\& x[0] \in \text{Memory} \xrightarrow{ == } x \in \text{ pointer } (\text{to 1st element of array})
\]


cf. Section 2.13 Character Strings from Fitzpatrick \cite{Fitz}

\begin{lstlisting}
  char word[20] = ``four''
  char *word = ``four''
\end{lstlisting}

cf. C$++$ extensions for C according to Fitzpatrick \cite{Fitz}
\begin{itemize}
\item simplified syntax to pass by reference pointers into functions
\item inline functions
\item variable size arrays \begin{lstlisting}
  int n;
  double x[n];
  \end{lstlisting}
\item complex number class
\end{itemize}


\part{C++ and Computational Physics}

cf. 2.1.1 Scientific hello world from Hjorth-Jensen (2015) \cite{Hjor2015}

in C, 
\begin{lstlisting}
  int main (int argc, char* argv[])
\end{lstlisting}
\verb|argc| stands for number of command-line arguments \\
\verb|argv| is vector of strings containing the command-line arguments with \\
\phantom{argv} \verb|argv[0]| containing name of program \\
\phantom{argv} \verb|argv[1] , argv[2], ... | are command-line args, i.e. the number of lines of input to the program

``To obtain an executable file for a C++ program'' (i.e. compile (???)), 
\begin{lstlisting}
  gcc -c -Wall myprogram.c
  gcc -o myprogram myprogram.o
\end{lstlisting}
\verb|-Wall| means warning is issued in case of non-standard language \\
\verb|-c| means compilation only \\
\verb|-o| links produced object file \verb|myprogram.o| and produces executable \verb|myprogram|

\subsubsection{Create \verb|makefile|}

\begin{lstlisting}
  # General makefile for c - choose PROG =  name of given program
  
  # Here we define compiler option, libraries and the target
  CC= c++ -Wall
  PROG= myprogram

  # Here we make the executable file
  ${PROG} :          ${PROG}.o
                     ${CC} ${PROG}.o -o ${PROG}

  # whereas here we create the object file

  #{PROG}.o :        ${PROG}.cpp
                     ${CC} -c ${PROG}.cpp
\end{lstlisting}

Here's what worked for me:
\begin{lstlisting}
CC= g++ -Wall
PROG= program1

# Here we make the executable file
${PROG} :          ${PROG}.o
        ${CC} ${PROG}.o -o ${PROG}

# whereas here we create the object file

${PROG}.o :        ${PROG}.cpp
        ${CC} -c ${PROG}.cpp

# EY : 20160602notice the different suffixes, and we see the pattern for the syntax

# (note: the <tab> in the command line is necessary formake towork)
# target: dependency1 dependency2 ...
#         <tab> command
\end{lstlisting}


cf. 2.3.2 Machine numbers of Hjorth-Jensen (2015) \cite{Hjor2015}


cf. 2.5.2 Pointers and arrays in C++ of Hjorth-Jensen (2015) \cite{Hjor2015}

Initialization (diagram):
\[
  \begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=1.4em, column sep=4em, minimum width=1em]
  {
    \& \text{var} = \verb|0x7ffc97efbd8c| & \text{pointer} = \& \text{var} = \verb|0x7ffc97efbd8c| \\
     \text{Memory} & \text{pointer} \\
    \text{ (memory) addresses } & \text{Obj}(\text{pointer}) \\
  };
  \path[|->]
  (m-1-1) edge node [auto] {$=$} (m-1-2)
  ;
  \path[->]
  (m-2-1) edge node [auto] {$=$} (m-2-2)
  (m-3-1) edge node [auto] {$=$} (m-3-2)
  ;
  \end{tikzpicture}
\]

Referencing and deferencing operations on pointers to variables
\[
\begin{aligned}
  \begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{var} & \& \text{var} = \verb|0x7ffc97egbd8c| \\
    \text{var}, 421 & \text{True} \\
    421 &  \\
  };
  \path[|->]
  (m-1-1) edge node [auto] {$=$} (m-1-2)
  (m-2-1) edge node [auto] {$$} (m-1-1)
  edge node [auto] {$==$} (m-2-2)
  edge node [auto] {$$} (m-3-1)
  ;
  \end{tikzpicture}
    & 
  \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{pointer} = \verb|0x7ffc97egbd8c| & 421 & \text{int} \\
    \& \text{pointer} = \verb|0x7ffc97egbd8c| & & \\ 
  };
  \path[|->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  edge node [auto] {\&} (m-2-1)
  (m-1-2) edge node [auto] {\text{typedef}} (m-1-3)
;  
  \end{tikzpicture}   \\
    \begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{Types} &  \textbf{Memory}  \\
    \textbf{Types} \otimes \mathbf{\textbf{Dat}} & \text{Boolean} = \lbrace \text{True}, \text{False} \rbrace  \\
    \mathbf{\textbf{Dat}} &  \\
  };
  \path[->]
  (m-1-1) edge node [auto] {$=$} (m-1-2)
  (m-2-1) edge node [auto] {$$} (m-1-1)
  edge node [auto] {$==$} (m-2-2)
  edge node [auto] {$$} (m-3-1)
  ;
  \end{tikzpicture}
    & 
  \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{pointer}  & \mathbf{\textbf{Dat}} & \mathbf{\textbf{Types}} \\
    \mathbf{\textbf{Memory}} & & \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  edge node [auto] {\&} (m-2-1)
  (m-1-2) edge node [auto] {\text{typedef}} (m-1-3)
;  
  \end{tikzpicture}   
  \end{aligned}
\]

\subsection{Numerical differentiation and interpolation (in C++)}

cf. Chapter 3 ``Numerical differentiation and interpolation'' of Hjorth-Jensen (2015) \cite{Hjor2015}.

This is how I understand it.

Consider the Taylor expansion for $f(x) \in C^{\infty}(\mathbb{R})$:
\[
f(x) = f(x_0)  + \sum_{j=1}^{\infty} \frac{ f^{(j)}(x_0)}{j!} h^j 
\]
For $x = x_0 \pm h$,
\[
\begin{gathered}
  f(x) = f(x_0 \pm h ) = f(x_0) + \sum_{j=1}^{\infty} \frac{ f^{(2j)}(x_0) }{(2j)!} h^{2j} \pm \sum_{j=1}^{\infty} \frac{ f^{(2j-1)}(x_0) }{(2j-1)!} h^{2j-1}
\end{gathered}
\]
Then
\[
\begin{gathered}
  f(x_0 + 2^kh) - f(x_0 - 2^kh) = 2\sum_{j=1}^{\infty} \frac{ f^{(2j-1)}}{(2j-1)!}(x_0) 2^{k(2j-1)} h^{2j-1} = \\
  = 2\left[ f^{(1)}(x_0) 2^k h + \sum_{j=2}^{\infty} \frac{ f^{(2j-1)}(x_0)}{ (2j-1)!} 2^{k(2j-1)} h^{2j-1} \right] =  \\
  = 2 \left[ f^{(1)}(x_0) 2^k h + \frac{ f^{(3)}(x_0)}{3!} 2^{k(3)} h^3 + \sum_{j=3}^{\infty} \frac{ f^{(2j-1)}(x_0) }{(2j-1)!} 2^{k(2j-1) } h^{2j-1} \right]
\end{gathered}
\]

So for $k=1$,
\[
f(x_0 + h ) - f(x_0 - h ) = 2 \left[ f^{(1)}(x_0)h + \sum_{j=1}^{\infty} \frac{ f^{(2j+1)}(x_0) }{ (2j+1)!} h^{2j+1} \right]
\]

Now
\[
\begin{gathered}
  f(x_0 + 2^kh) + f(x_0 - 2^kh) - 2f(x_0) = \\
  = 2 \sum_{j=1}^{\infty} \frac{ f^{(2j)}(x_0) }{(2j)!} 2^{2jk} h^{2j} = \\
  = 2 \left[ \frac{ f^{(2)}(x_0)}{2} 2^{2k} h^2 + \sum_{j=2}^{\infty} \frac{ f^{(2j)}(x_0) }{ (2j)!} 2^{2jk} h^{2j} \right] = \\
  = 2 \left[ \frac{ f^{(2)}(x_0) }{2} 2^{2k} h^2 + \frac{ f^{(4)}(x_0)}{4!} 2^{4k} h^4 + \sum_{j=3}^{\infty} \frac{ f^{(2j)}(x_0) }{ (2j)!} 2^{2jk} h^{2j} \right]
  \end{gathered}
\]
Thus for the case of $k=1$,
\[
f(x_0 + h )  + f(x_0 - h) - 2f(x_0) = f^{(2)}(x_0)h^2 + 2 \sum_{j=2}^{\infty} \frac{ f^{(2j)}(x_0)}{(2j)!} h^{2j}
\]
\[
\begin{aligned}
  & \frac{ f(x_0 +h) - f(x_0-h) }{ 2h} = f^{(1)}(x_0) + \sum_{j=1}^{\infty} \frac{ f^{(2j+1)}(x_0) }{(2j+1)!}h^{2j} \\ 
  & \frac{ f(x_0 + h) + f(x_0 - h) - 2f(x_0) }{h^2} = f^{(2)}(x_0) + 2\sum_{j=2}^{\infty} \frac{ f^{(2(j+1) ) }(x_0)}{(2(j+1))!} h^{2j}
  \end{aligned}
\]

A pattern now emerges on how to include more calculations at points $x_0, x_0 \pm 2^kh$ so to obtain better accuracy $O(h^l)$.  For instance,

Given 5 pts. $\lbrace x_0, x_0 \pm h, x_0 \pm 2h \rbrace$,
\[
\begin{gathered}
  f(x_0 + 2h) - f(x_0 -2h) = 2[ f^{(1)}(x_0) 2^1h + \frac{ f^{(3)}(x_0)}{3!} 2^3 h^3 + O(h^5) ] \\ 
  f(x_0+ h) - f(x_0 - h) = 2[ f^{(1)}(x_0) h + \frac{ f^{(3)}(x_0)}{3!} h^3 + O(h^5)] \\
  \Longrightarrow f'(x_0) = \frac{ f(x_0 - 2h) - 8f(x_0-h ) + 8f(x_0 + h) - f(x_0 + 2h) }{ 12h } + O(h^4)
\end{gathered}
\]

Hjorth-Jensen (2015) \cite{Hjor2015} argues, on pp. 46-47, that the additional evaluations are time consuming, to obtain further accuracy, so it's a balance.

To summarize, for $O(h^2)$ accuracy,
\[
\begin{aligned}
  & \frac{ f(x_0 + h) - f(x_0-h) }{2h} = f^{(1)}(x_0) + \sum_{j=1}^{\infty} \frac{ f^{(2j+1)}(x_0 )}{(2j+1)!} h^{2j} & \qquad \, O(h^2) \\ 
  & \frac{ f(x_0 + h) + f(x_0 -h) - 2f(x_0) }{h^2} = f^{(2)}(x_0) + 2\sum_{j=1}^{\infty} \frac{ f^{(2j+2)}(x_0) }{ (2j+2)!} h^{2j} & \qquad \, O(h^2)
  \end{aligned}
\]

\section{Interpolation}

cf. 3.2 Numerical Interpolation and Extrapolation of Hjorth-Jensen (2015) \cite{Hjor2015}

Given $N+1$ pts. $\begin{aligned} & \quad \\
  y_0 & = f(x_0) \\
  y_1 & = f(x_1) \\
  & \vdots \\
  y_N & = f(x_N) \end{aligned}$, $x_i$'s distinct (none of $x_i$ values equal)

We want a polynomial of degree $n$ s.t. $p(x)  \in \mathbb{R}[x]$

\[
p(x_i) = f(x_i) = y_i \qquad \, i = 0,1\dots N
\]

\[
p(x) = a_0 + a_1(x-x_0) + \dots + a_i \prod_{j=0}^{i-1}(x-x_j) + \dots + a_N(x-x_0) \dots (x-x_{N-1}) = a_0 + \sum_{i=1}^N a_i \prod_{j=0}^{i-1}(x-x_j)
\]
\[
\begin{aligned}
  & a_0 = f(x_0) \\ 
  & a_0 + a_1(x_1-x_0) = f(x_1) \\
  & \vdots \\
  & a_0 + \sum_{i=1}^k a_i \prod_{j=0}^{i-1} (x_k - x_j) = f(x_k)
  \end{aligned}
\]

Hjorth-Jensen (2015) \cite{Hjor2015} mentions this Lagrange interpolation formula (I haven't found a good proof for it).  
\begin{equation}
\boxed{ p_N(x) = \sum_{i=0}^N \prod_{k\neq i} \frac{ x-x_k}{x_i - x_k} y_i  }
\end{equation}

\section{Classes (C++)}

cf. \href{http://stackoverflow.com/questions/6377786/c-operator-overloading-in-expression}{C++ Operator Overloading in expression}

Take a look at this link: \href{http://stackoverflow.com/questions/6377786/c-operator-overloading-in-expression}{C++ Operator Overloading in expression}.  This point isn't emphasized enough, as in Hjorth-Jensen (2015) \cite{Hjor2015}.  This makes doing something like
\[
d = a*c + d/b
\]
work the way we expect.  Kudos to user \href{http://stackoverflow.com/users/252000/fredoverflow}{fredoverflow} for his answer:

``The expression \verb|(e_x*u_c)| is an rvalue, and references to non-const won't bind to rvalues.

Also, member functions should be marked \verb|const| as well.''  

\subsection{What are lvalues and rvalues in C and C++?}

\href{http://thbecker.net/articles/rvalue_references/section_01.html}{C++ Rvalue References Explained}

Original definition of \emph{lvalues} and \emph{rvalues} from \emph{C}: \\
\emph{lvalue} - expression $e$ that may appear on the left or on the right hand side of an assignment \\
\emph{rvalue} - expression that can only appear on right hand side of assignment $=$.

Examples:

\begin{lstlisting}
  int a = 42;
  int b = 43;

  // a and b are both l-values
  a = b; // ok
  b = a; // ok
  a = a * b; // ok

  // a * b is an rvalue:
  int c = a * b; // ok, rvalue on right hand side of assignment
  a * b = 42; // error, rvalue on left hand side of assignment
  
  \end{lstlisting}

In \emph{C++}, this is still useful as a first, intuitive approach, but \\
\emph{lvalue} - expression that refers to a memory location and allows us to take the address of that memory location via the $\&$ operator. \\
\emph{rvalue} - expression that's not a lvalue

So $\&$ reference \emph{functor} can't act on rvalue's.



\section{Numerical Integration}

\subsubsection{Trapezoid rule (or trapezoidal rule)}

See \href{https://github.com/ernestyalumni/CompPhys/blob/master/Cpp/Integrate.ipynb}{Integrate.ipynb}.

From there, consider integration on $[a,b]$, considering $h := \frac{b-a}{N}$, and $N+1$ (grid) points, $\lbrace a, a+h, a+2h, \dots , a+ jh, \dots , a+Nh = b\rbrace_{j=0 \dots N }$.

Then $\frac{N}{2}$ pts. are our ``$x_0$''; $x_0$'s $= \lbrace a +h , a+3h, \dots , a+(2j-1)h, \dots , a+ \left( \frac{2 N}{2} - 1 \right)h \rbrace_{j=1 \dots \frac{N}{2} }$.

Notice how we really need to care about if $N$ is even or not.  If $N$ is not even, then we'd have to deal with the integration at the integration limits and choosing what to do.  

Then
\[
\begin{gathered}
  \int_a^b f(x) dx = \sum_{j=1}^{N/2} \int_{a + (2j-1)h-h}^{a+(2j-1)h+h} f(x)dx = \sum_{j=1}^{N/2} \frac{h}{2} ( 2f(a+(2j-1)h ) + f(a+2(j-1)h) + f(a+2jh) ) = \\
   = h(f(a)/2 + f(a+h) + \dots + f(b-h) + \frac{f(b)}{2} ) = h \left( \frac{f(a)}{2} + \sum_{j=1}^{N-1} f(a+jh) + \frac{f(b)}{2} \right)
  \end{gathered}
\]

\subsubsection{Midpoint method or rectangle method}.

Let $h := \frac{b-a}{N}$ be the step size.  The grid is as follows:
\[
\lbrace a , a +h , \dots , a + jh , \dots , a+Nh = b\rbrace_{j=0\dots N}
\]
The desired midpoint values are at the following $N$ points:
\[
\lbrace a + \frac{h}{2} , a + \frac{3}{2} h , \dots , a+\frac{(2j-1 ) h }{2} , \dots , a + \left( N - \frac{1}{2} \right) h \rbrace_{j=1 \dots N}
\]
and so
\begin{equation}
  \int_a^b f(x) dx \approx \sum_{j=1}^N f(x_j) h = \sum_{j=1}^N f\left( a + \frac{(2j-1) h}{2} \right)h 
\end{equation}


\subsubsection{Simpson rule}

The idea is to take the next ``order'' in the Lagrange interpolation formula, the second-order polynomial, and then we can rederive Simpson's rule.  The algebra is worked out in \href{https://github.com/ernestyalumni/CompPhys/blob/master/Cpp/Integrate.ipynb}{Integrate.ipynb}.

From there, then we can obtain Simpson's rule,
\[
\begin{gathered}
  \int_a^b f(x)dx = \sum_{j=1}^{N/2} \int_{a + 2(j-1)h }^{a+2jh} f(x) dx = \sum_{j=1}^{N/2} \frac{h}{3} ( 4 f(a+(2j-1)h ) + f(a+2(j-1)h) + f(a+2jh) ) = \\
  = \frac{h}{3} \left[ f(a) + f(b) + \sum_{j=1}^{N/2} 4f(a+(2j-1) h) + 2\sum_{j=1}^{N/2-1} f(a+2jh) \right]
\end{gathered}
\]

\subsection{Gaussian Quadrature}

cf. Hjorth-Jensen (2015) \cite{Hjor2015}, Section 5.3 Gaussian Quadrature, Chapter 5 Numerical Integration





\section{Call by reference - Call by Value, Call by reference (in C and in C++)}

cf. pp. 58, 2.10 Pointers Ch. 2 Scientific Programming in C, Fitzpatrick \cite{Fitz}
\verb|printfact3.c|, \href{https://github.com/ernestyalumni/CompPhys/blob/master/CFitz/printfact3.c}{printfact3.c}

pass pointer, pass by reference, call by pointer, call by reference 

In C: 
\begin{itemize}
  \item  \emph{function prototype} - 
\[
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{pointer}  &  \textbf{Types} \\
    \textbf{Types}  & \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] { \text{function}} (m-1-2)
  edge node [auto] {*} (m-2-1)
;  
  \end{tikzpicture}   
\qquad \qquad \, \,
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{pointer}  &  \text{void} \\
    \text{double}  & \\ 
  };
  \path[|->]
  (m-1-1) edge node [auto] { \text{function}} (m-1-2)
  edge node [auto] {*} (m-2-1)
;  
  \end{tikzpicture}   
\]
$\Longrightarrow$ 
\begin{lstlisting}
void factorial(double *)
  \end{lstlisting}
where for factorial, it's just your choice of name for \emph{function}.  

\item \emph{function definition} - 
  \[
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{pointers}  &  \textbf{Types} \\
    \textbf{Types}  & \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] { \text{function}} (m-1-2)
  edge node [auto] {*} (m-2-1)
;  
\end{tikzpicture}   \qquad  \,
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{pointer}  &  \text{void} \\
    \text{double}  & \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] { \text{function}} (m-1-2)
  edge node [auto] {*} (m-2-1)
;  
  \end{tikzpicture}   \qquad \, \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{fact}  &  \text{void} \\
    *\text{fact}  & \\ 
  };
  \path[|->]
  (m-1-1) edge node [auto] { \text{function}} (m-1-2)
  edge node [auto] {*} (m-2-1)
;  
  \end{tikzpicture}   
\]
  $\Longrightarrow$
\begin{lstlisting}
  void function(double *fact) { ... }
\end{lstlisting}

\emph{Inside} the function definition,
\[
  \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{pointer}  & \textbf{Dat}_{\text{lvalues}} & \textbf{Types} \\
    \textbf{Memory} & & \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  edge node [auto] {\&} (m-2-1)
  (m-1-2) edge node [auto] {\text{typedef}} (m-1-3)
;  
  \end{tikzpicture} \qquad \qquad \,   
  \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{fact}  & *\text{fact}  & \text{double} \\
    \& \text{fact} & & \\ 
  };
  \path[|->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  edge node [auto] {\&} (m-2-1)
  (m-1-2) edge node [auto] {\text{typedef}} (m-1-3)
;  
  \end{tikzpicture}   
  \]
  and so, for instance, in the function definition, you can do things like this:
  \begin{lstlisting}
    *fact = 1
    *fact *= (double) n 
    \end{lstlisting}
and so notice that from \verb|*fact = 1|, \verb|*fact| is a lvalue.  
  \begin{itemize}
  \item \emph{ function procedure }
\[
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{pointer}  & \textbf{Dat}_{\text{lvalues}}     \\
     \textbf{Memory} &  \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  edge node [auto] {\&} (m-2-1)
  (m-1-2) [loop right] edge node [right] {\text{function procedure}} (m-1-2)
;  
  \end{tikzpicture}    \qquad \qquad \, 
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{fact}  & * \text{fact}     \\
    \& \text{fact} &  \\ 
  };
  \path[|->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  edge node [auto] {\&} (m-2-1)
  (m-1-2) [loop right] edge node [right] {\text{function procedure}} (m-1-2)
;  
    \end{tikzpicture}
\]
$\Longrightarrow $
\begin{lstlisting}
  *fact *= (double) n 
\end{lstlisting}
    \end{itemize}

\item ``Using'' the function, function ``instantiation'', ``calling'' the function, i.e. ``running'' the function
  \[
  \begin{aligned} 
& \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{Types}  &  \textbf{Memory} &     \\
                    & \textbf{pointers} & \textbf{Types}  \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] {$\&$} (m-1-2)
  (m-1-2) edge node [auto] {$\cong$ } (m-2-2)
  (m-2-2)  edge node [auto] {\text{function}} (m-2-3)
  (m-1-1) [loop left] edge node [left] { \text{ function procedure } } (m-1-1)
  ;  
\end{tikzpicture} \qquad \, \\
    & 
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{double}  &  \text{Memory} (\text{Obj}{Memory}) &     \\
                    & \text{pointer} & \text{void}  \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] {$\&$} (m-1-2)
  (m-1-2) edge node [auto] {$\cong$ } (m-2-2)
  (m-2-2)  edge node [auto] {\text{function}} (m-2-3)
  (m-1-1) [loop left] edge node [left] { \text{ function procedure } } (m-1-1)
  ;  
\end{tikzpicture} \qquad \, \\
&
 \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{fact}  &  \& \text{fact}  &     \\
                    & \& \text{fact} & \text{function}(\& \text{ fact})  \\ 
  };
  \path[|->]
  (m-1-1) edge node [auto] {$\&$} (m-1-2)
  (m-1-2) edge node [auto] {$\cong$ } (m-2-2)
  (m-2-2)  edge node [auto] {\text{function}} (m-2-3)
  (m-1-1) [loop left] edge node [left] { \text{ function procedure } } (m-1-1)
  ;  
\end{tikzpicture}
\end{aligned}
    \]
    where, again simply note the notation, that we're using \emph{function} and \emph{factorial}, \emph{fact} for \emph{nameofpointer}, interchangeably: see \href{https://github.com/ernestyalumni/CompPhys/blob/master/CFitz/printfact3.c}{printfact3.c} for the example I'm referring to.

    
\end{itemize}

Again, \emph{in C}, consider \emph{a pointer to a function} passed to another function as an argument.  Take a look at \href{https://github.com/ernestyalumni/CompPhys/blob/master/CFitz/passfunction.c}{passfunction.c} simultaneously.

\begin{itemize}
\item  \emph{function prototype} -
\[
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{pointer}  &  \textbf{Types} \\
    \text{Mor}_{\textbf{Types}}  & \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] { \text{hostfunction}} (m-1-2)
  edge node [auto] {*} (m-2-1)
;  
  \end{tikzpicture}   
\qquad \qquad \, \,
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{pointer}  &  \text{void} \\
    \text{Mor}_{\textbf{Types}}(\text{double},\text{double})  & \\ 
  };
  \path[|->]
  (m-1-1) edge node [auto] { \text{hostfunction}} (m-1-2)
  edge node [auto] {*} (m-2-1)
;  
  \end{tikzpicture}   
\]
$\Longrightarrow$
\begin{lstlisting}
  void hostfunction(double (*)(double))
\end{lstlisting}
We could further generalize this syntax, simply for syntax and notation sake, as such:
\[
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{pointer}  &  \textbf{Types} \\
    \text{Mor}_{\textbf{Types}}  & \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] { \text{hostfunction}} (m-1-2)
  edge node [auto] {*} (m-2-1)
;  
  \end{tikzpicture}   
\qquad \qquad \, \,
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{pointer}  &  \text{data-type} \\
    \text{Mor}_{\textbf{Types}}(\text{typei},\text{typef})  & \\ 
  };
  \path[|->]
  (m-1-1) edge node [auto] { \text{hostfunction}} (m-1-2)
  edge node [auto] {*} (m-2-1)
;  
  \end{tikzpicture}   
\]
$\Longrightarrow$
\begin{lstlisting}
  data-type hostfunction(typef (*)(typei))
\end{lstlisting}

For practice, consider more than 1 argument in our function, and the other argument, for practice, is a pointer, we're ``passing by reference.''

\[
\begin{aligned}
  & 
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    & \textbf{pointers} \times \textbf{pointers}  &   & \textbf{Types} \\
    \textbf{pointers} &  & \textbf{pointers} & \\      
    \text{Mor}_{\textbf{Types}}  &  & \textbf{Types} &  \\ 
  };
  \path[->]
  (m-1-2) edge node [auto] { \text{hostfunction}} (m-1-4)
  (m-1-2) edge node [auto] { $\text{pr}_1$ } (m-2-1)
  edge node [auto] { $\text{pr}_2$ } (m-2-3)
  (m-2-1) edge node [auto] {*} (m-3-1)
  (m-2-3) edge node [auto] {*} (m-3-3)
  ;  
  \end{tikzpicture}   
\qquad \qquad \, \, \\
&
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    & pointer \times pointer  &   & \text{void} \\
    pointer &  & pointers & \\      
    \text{Mor}_{\textbf{Types}}(\text{double},\text{double})  &  & \text{double} &  \\ 
  };
  \path[|->]
  (m-1-2) edge node [auto] { \text{hostfunction}} (m-1-4)
  (m-1-2) edge node [auto] { $\text{pr}_1$ } (m-2-1)
  edge node [auto] { $\text{pr}_2$ } (m-2-3)
  (m-2-1) edge node [auto] {*} (m-3-1)
  (m-2-3) edge node [auto] {*} (m-3-3)
  ;  
  \end{tikzpicture}   
\end{aligned}
\]
$\Longrightarrow$
\begin{lstlisting}
void hostfunction( double (*)(double), double *)
\end{lstlisting}
\item \emph{function definition}

  \[
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{pointers}  &  \textbf{Types} \\
    \text{Mor}_{\textbf{Types}}  & \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] { \text{hostfunction}} (m-1-2)
  edge node [auto] {*} (m-2-1)
;  
\end{tikzpicture}   \qquad  \,
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{pointer}  &  \text{void} \\
    \text{Mor}_{\textbf{Types}}(\text{double},\text{double})  & \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] { \text{hostfunction}} (m-1-2)
  edge node [auto] {*} (m-2-1)
;  
  \end{tikzpicture}   \qquad \, \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{fun}  &  \text{void} \\
    *\text{fun}  & \\ 
  };
  \path[|->]
  (m-1-1) edge node [auto] { \text{hostfunction}} (m-1-2)
  edge node [auto] {*} (m-2-1)
;  
  \end{tikzpicture}   
\]
  $\Longrightarrow$
\begin{lstlisting}
  void hostfunction(double (*fun)(double)) { ... }
\end{lstlisting}

\item \emph{Inside} the function definition,
\[
\begin{aligned}
  & 
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{Types}  &  \textbf{Types} & \textbf{Types} \\
  };
  \path[->]
  (m-1-1) edge node [auto] { $*\text{fun}$} (m-1-2)
  (m-1-2) edge node [auto] {$=$} (m-1-3)
;  
\end{tikzpicture} \quad \, \\
& 
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{double}  &  \text{double} & \text{double} \\
  };
  \path[->]
  (m-1-1) edge node [auto] { $*\text{fun}$} (m-1-2)
  (m-1-2) edge node [auto] {$=$} (m-1-3)
;  
\end{tikzpicture}   \quad \, \\
&
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    x  &  (*\text{fun})(x) & y = (*\text{fun})(x) \\
  };
  \path[|->]
  (m-1-1) edge node [auto] { $*\text{fun}$} (m-1-2)
  (m-1-2) edge node [auto] {$=$} (m-1-3)
;  
  \end{tikzpicture}   
\end{aligned}
\]
$\Longrightarrow $
\begin{lstlisting}
y = (*fun)(x)
  \end{lstlisting}
\item ``Using'' the function - the \emph{actual} syntax for ``passing'' a function into a function is interesting (peculiar?): you only need the \emph{name} of the function.

  Let's quickly recall how a function is prototyped, ``declared'' (or, i.e., defined), and used:
  \begin{itemize}
  \item \emph{function prototype} -
    \[
\begin{aligned}
  &
  \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{Types} & \textbf{Types} \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] { $\text{fun}1$} (m-1-2)
;  
  \end{tikzpicture}   
  \qquad \, \\
  &
  \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{double} & \text{double} \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] { $\text{fun}1$} (m-1-2)
;  
  \end{tikzpicture}   
\end{aligned}
    \]
    $\Longrightarrow $
    \begin{lstlisting}
      double fun1(double)
    \end{lstlisting}
  \item \emph{ function definition } -
    \[
\begin{aligned}
  &
  \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{Types} & \textbf{Types} \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] { $\text{fun}1$} (m-1-2)
;  
  \end{tikzpicture}   
  \qquad \, \\
  &
  \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{double} & \text{double} \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] { $\text{fun}1$} (m-1-2)
;  
  \end{tikzpicture}
  \qquad \, \\
  &
    \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    z & 3.0 z*z - z (= 3z^2 - z) \\ 
  };
  \path[|->]
  (m-1-1) edge node [auto] { $\text{fun}1$} (m-1-2)
;  
  \end{tikzpicture}   
\end{aligned}
    \]
    $\Longrightarrow $
    \begin{lstlisting}
      double fun1(double z) { ... }
    \end{lstlisting}
    \item Using function - \verb| fun1(z) |

        \end{itemize}
  and so
  \[
\text{fun}1 \in \text{Mor}_{\textbf{Types}}(\text{double},\text{double})
\]

And so again, it's interesting in terms of syntax that all you need is the \emph{name} of the function to pass into the arguments of the ``host function'' when using the host function:
\[
\begin{gathered}
    \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
  \text{Mor}_{\textbf{Types}} & \textbf{Types} \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] { $\text{hostfunction}$} (m-1-2)
;  
  \end{tikzpicture}   
    \\
        \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
  \text{Mor}_{\textbf{Types}}(\text{double},\text{double}) & \text{void} \\
  };
  \path[|->]
  (m-1-1) edge node [auto] { $\text{hostfunction}$} (m-1-2)
;  
        \end{tikzpicture}   \\
                \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
  \text{fun}1 & \text{hostfunction}(\text{fun}1) \\
  };
  \path[|->]
  (m-1-1) edge node [auto] { $\text{hostfunction}$} (m-1-2)
;  
  \end{tikzpicture}   
  \end{gathered}
\]
$\Longrightarrow$
\begin{lstlisting}
hostfunction(fun1)
  \end{lstlisting}

\end{itemize}

\subsubsection{C++ extensions, or how C++ pass by reference (pass a pointer to argument) vs. C}

Recall how C passes by reference, and look at Fitzpatrick \cite{Fitz}, pp. 83-84 for the \verb|square| function:

\begin{itemize}
  \item  \emph{function prototype} - 
\[
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{pointer}  &  \textbf{Types} \\
    \textbf{Types}  & \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] { \text{square}} (m-1-2)
  edge node [auto] {*} (m-2-1)
;  
  \end{tikzpicture}   
\qquad \qquad \, \,
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{pointer}  &  \text{void} \\
    \text{double}  & \\ 
  };
  \path[|->]
  (m-1-1) edge node [auto] { \text{square}} (m-1-2)
  edge node [auto] {*} (m-2-1)
;  
  \end{tikzpicture}   
\]
$\Longrightarrow$ 
\begin{lstlisting}
void square(double *)
  \end{lstlisting}

\item \emph{function definition} - 
  \[
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{pointers}  &  \textbf{Types} \\
    \textbf{Types}  & \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] { \text{square}} (m-1-2)
  edge node [auto] {*} (m-2-1)
;  
\end{tikzpicture}   \qquad  \,
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{pointer}  &  \text{void} \\
    \text{double}  & \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] { \text{function}} (m-1-2)
  edge node [auto] {*} (m-2-1)
;  
  \end{tikzpicture}   \qquad \, \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    y  &  \text{void} \\
    *y  & \\ 
  };
  \path[|->]
  (m-1-1) edge node [auto] { \text{square}} (m-1-2)
  edge node [auto] {*} (m-2-1)
;  
  \end{tikzpicture}   
\]
  $\Longrightarrow$
\begin{lstlisting}
  void square(double *y) { ... }
\end{lstlisting}

\emph{Inside} the function definition,
\[
  \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{pointer}  & \textbf{Dat}_{\text{lvalues}} & \textbf{Types} \\
  };
  \path[->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  (m-1-2) edge node [auto] {\text{typedef}} (m-1-3)
;  
  \end{tikzpicture} \qquad \qquad \,   
  \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    y  & *y  & \text{double} \\
  };
  \path[|->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  (m-1-2) edge node [auto] {\text{typedef}} (m-1-3)
;  
  \end{tikzpicture}   
  \]
  and so, for instance, in the function definition, you can do things like this:
  \begin{lstlisting}
    *y = x*x
    \end{lstlisting}

\item ``Using'' the function, function ``instantiation'', ``calling'' the function, i.e. ``running'' the function
  \[
  \begin{aligned} 
& \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{Types}  &  \textbf{Memory} &     \\
                    & \textbf{pointers} & \textbf{Types}  \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] {$\&$} (m-1-2)
  (m-1-2) edge node [auto] {$\cong$ } (m-2-2)
  (m-2-2)  edge node [auto] {\text{square}} (m-2-3)
  (m-1-1) [loop left] edge node [left] { \text{ function procedure } } (m-1-1)
  ;  
\end{tikzpicture} \qquad \, \\
    & 
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{double}  &  \text{Memory} (\text{Obj}{Memory}) &     \\
                    & \text{pointer} & \text{void}  \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] {$\&$} (m-1-2)
  (m-1-2) edge node [auto] {$\cong$ } (m-2-2)
  (m-2-2)  edge node [auto] {\text{square}} (m-2-3)
  (m-1-1) [loop left] edge node [left] { \text{ function procedure } } (m-1-1)
  ;  
\end{tikzpicture} \qquad \, \\
&
 \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{res}  &  \& \text{res}  &     \\
                    & \& \text{res} & \text{square}(\& \text{res})  \\ 
  };
  \path[|->]
  (m-1-1) edge node [auto] {$\&$} (m-1-2)
  (m-1-2) edge node [auto] {$\cong$ } (m-2-2)
  (m-2-2)  edge node [auto] {\text{square}} (m-2-3)
  (m-1-1) [loop left] edge node [left] { \text{ function procedure } } (m-1-1)
  ;  
\end{tikzpicture}
\end{aligned}
    \]
    
\end{itemize}

\subsubsection{C++ syntax for dealing with passing pointers (and arrays) into functions}

However, in \emph{C++}, a lot of the dereferencing $*$ and referencing $\&$ is not explicitly said so in the syntax.  In this syntax, passing by reference is indicated by prepending the $\&$ ampersand to the variable name, in function declaration (prototype and definition).  We don't have to explicitly deference the argument in the function (it's done behind the scene) and syntax-wise (it seems), we only have to refer to the argument by regular local name.

Indeed, the syntax appears ``shortcutted'' greatly:
\begin{itemize}
  \item  \emph{function prototype} - 
\[
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{pointer} \times \textbf{Types}  &  \textbf{Types} \\
  };
  \path[->]
  (m-1-1) edge node [auto] { \text{function}} (m-1-2)
;  
  \end{tikzpicture}   
\qquad \qquad \, \,
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{pointer}, \text{double}  &  \text{void} \\
  };
  \path[|->]
  (m-1-1) edge node [auto] { \text{function}} (m-1-2)
;  
  \end{tikzpicture}   
\]
$\Longrightarrow$ 
\begin{lstlisting}
void function(double &)
  \end{lstlisting}

\item \emph{function definition} - 
  \[
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{pointers}\times \textbf{Types}  &  \textbf{Types} \\
  };
  \path[->]
  (m-1-1) edge node [auto] { \text{square}} (m-1-2)
;  
\end{tikzpicture}   \qquad  \,
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{pointer},\text{double}  &  \text{void} \\
  };
  \path[->]
  (m-1-1) edge node [auto] { \text{function}} (m-1-2)
;  
  \end{tikzpicture}   \qquad \, \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
   \&, y  &  \text{function}(\text{double} \; \& y) \\
  };
  \path[|->]
  (m-1-1) edge node [auto] { \text{function}} (m-1-2)
;  
  \end{tikzpicture}   
\]
  $\Longrightarrow$
\begin{lstlisting}
  void function(double &y) { ... }
\end{lstlisting}

\emph{Inside} the function definition,
\[
  \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{double}   & \text{double} \\
  };
  \path[->]
  (m-1-1) edge node [auto] {$\text{End}(\text{double},\text{double})$} (m-1-2)
;  
  \end{tikzpicture} \qquad \qquad \,   
  \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    y  & y = x*x \\
  };
  \path[|->]
  (m-1-1) edge node [auto] {$\text{End}(\text{double},\text{double})$} (m-1-2)
;  
  \end{tikzpicture}   
  \]
  and so, for instance, in the function definition, you can do things like this:
  \begin{lstlisting}
    y = x*x
    \end{lstlisting}
with no deferencing needed.  
\item ``Using'' the function, function ``instantiation'', ``calling'' the function, i.e. ``running'' the function
  \[
  \begin{aligned} 
& \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{Types}  &  \textbf{Types}      \\
  };
  \path[->]
  (m-1-1) edge node [auto] {$\text{function}$} (m-1-2)
  ;  
\end{tikzpicture} \qquad \,      
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{double}  &  \text{void}  \\
  };
  \path[->]
  (m-1-1) edge node [auto] {$\text{function}$} (m-1-2)
  ;  
\end{tikzpicture} \qquad \, 
 \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{res}  &   \text{function}(\text{res})       \\
                     };
  \path[|->]
  (m-1-1) edge node [auto] {$\text{function}$} (m-1-2)
  ;  
\end{tikzpicture}
\end{aligned}
    \]
    
\end{itemize}

\subsubsection{C++ note on arrays}

For dealing with arrays, Stroustrup (2013) \cite{Stro2013}, on pp. 12 of Chapter 1 The Basics, Section 1.8 Pointers, Arrays, and References, does the following:

\begin{itemize}
\item \emph{array declaration} -
  \begin{lstlisting}
    type a[n]; // type[n]; array of n type's
  \end{lstlisting}
\item ``Using'' arrays in function prototypes, i.e. passing into arguments of functions for \emph{function prototypes}
  \begin{lstlisting}
    data-type function( type * arrayname)
    \end{lstlisting}
\item ``Using'' arrays when ``using'' functions, i.e. passing into arguments when a function is ``called'' or ``executed''
  \begin{lstlisting}
    function( arrayname )
  \end{lstlisting}
  \end{itemize}


Fitzpatrick \cite{Fitz} mentions using \verb|inline| for short functions, no more than 3 lines long, because of memory cost of calling a function.  




\subsubsection{Need a CUDA, C, C$++$, IDE?  Try Eclipse!}

This website has a clear, lucid, and pedagogical tutorial for using Eclipse: \href{https://www.fayewilliams.com/2011/06/28/creating-your-first-c-program-in-eclipse/}{Creating Your First C++ Program in Eclipse}.  But it looks like I had to pay.  Other than the well-written tips on the webpage, I looked up stackexchange for my Eclipse questions (I had difficulty with the Eclipse documentation).  

Others, like myself, had questions on how to use an IDE like Eclipse when learning CUDA, and ``building'' (is that the same as compiling?) and running only single files.  

My workflow: I have a separate, in my file directory, folder with my github repository clone that's local.

I start a New Project, CUDA Project, in Eclipse.  I type up my single file (I right click on the \verb|src| folder and add a `Source File`).  I build it (with the Hammer, Hammer looking icon; yes there are a lot of new icons near the top) and it runs.  I can then run it again with the Play, triangle, icon.

I found that if I have more than 1 (2 or more) file in the \verb|src| folder, that requires the \verb|main| function, it won't build right.

So once a file builds and it's good, I, in Terminal, \verb|cp| the file into my local github repository.  Note that from there, I could use the \verb|nvcc| compiler to build, from there, if I wanted to.

Now with my file saved (for example, \verb|helloworldkernel.cu|), then I can delete it, without fear, from my, say, \verb|cuda-workplace|, from the right side, ``C/C$++$ Projects'' window in Eclipse.   

\section{On CUDA By Example}
Take a look at 3.2.2 A Kernel Call, a Hello World in CUDA C, with a simple kernel, on pp. 23 of Sanders and Kandrot (2010) \cite{SK2010} and on github, [helloworldkernel.cu](https://github.com/ernestyalumni/CompPhys/blob/master/CUDA-By-Example/helloworldkernel.cu).  Let's work out the functor interpretation for practice.

\begin{itemize}
\item \emph{function definition} - \[
  \begin{gathered}
    \textbf{Types} \xrightarrow{ \text{ kernel } } \textbf{Types } \\ 
    \text{void} \xrightarrow{ \text{kernel } } \text{ void }
  \end{gathered}
  \]
  where \verb|kernel| $\in $ \verb|__global__| \\
  $\Longrightarrow $
  \begin{lstlisting}
    __global__ void kernel(void) { }
  \end{lstlisting}

CUDA C adds the \verb|__global__| qualifier to standard C to \emph{alert the compiler that the function}, \verb|kernelfunction|, should be compiled to run on the \emph{device}, not the host (pp. 24 \cite{SK2010}).    
\item ``Using'', ``calling'', ``running'' function -
  \[
\begin{aligned}
  & <<<>>>: (n_{\text{block}} , n_{\text{threads}}) \times \text{kernelfunction} \mapsto \text{kernelfunction}<<<n_{\text{block}}, n_{\text{threads}}>>> \in \text{End}(\text{Dat}_{\textbf{Types}}) \\
  & <<<>>>:\mathbb{N}^+ \times \mathbb{N}^+ \times \text{Mor}_{\text{GPU}} \to \text{End}(\text{Dat}_{GPU})
  \end{aligned}
\]
$\Longrightarrow$
\begin{lstlisting}
  kernel<<<1,1>>>();
  \end{lstlisting}
  \end{itemize}

cf. 3.2.3 Passing Parameters of Sanders and Kandrot (2010) \cite{SK2010}

Taking a look at [add-passb.cu](https://github.com/ernestyalumni/CompPhys/blob/master/CUDA-By-Example/add-passb.cu), let's work out the functor interpretation of \verb|cudaMalloc|, \verb|cudaMemcpy|.

In \verb|main|, ``declaring'' a pointer:
\begin{lstlisting}
  int *dev_c
\end{lstlisting}
$\Longleftarrow$
\[
\begin{gathered}
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{pointers}  &   \textbf{Dat}_{\text{lvalues}} & \textbf{Types}      \\    
  };
  \path[->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  (m-1-2) edge node [auto] {\text{typedef}} (m-1-3)
  ;  
\end{tikzpicture} \\
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \verb|dev_c|  &   * \verb|dev_c| & \text{int}      \\    
  };
  \path[|->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  (m-1-2) edge node [auto] {\text{typedef}} (m-1-3)
  ;  
\end{tikzpicture}
  \end{gathered}
\]
We can also do, note, the \verb|sizeof| function (which is a well-defined mapping, for once) on $\text{Obj}\textbf{Types}$:
\[
\begin{gathered}
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{pointers}  &   \textbf{Dat}_{\text{lvalues}} & \textbf{Types} & \mathbb{N}^+     \\    
  };
  \path[->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  (m-1-2) edge node [auto] {\text{typedef}} (m-1-3)
  (m-1-3) edge node [auto] {\text{sizeof}} (m-1-4)
  ;  
\end{tikzpicture} \\
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \verb|dev_c|  &   * \verb|dev_c| & \text{int} & \text{sizeof}(\text{int})     \\    
  };
  \path[|->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  (m-1-2) edge node [auto] {\text{typedef}} (m-1-3)
  (m-1-3) edge node [auto] {\text{sizeof}} (m-1-4)
  ;  
\end{tikzpicture}
  \end{gathered}
\]

Consider what Sanders and Kandrot says about the pointer to the pointer that (you want to) holds the address of the newly allocated memory. \cite{SK2010}  Consider this diagram:

\[
\begin{aligned}
  & \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{pointers}  &   \textbf{pointers} & \textbf{Types}      \\    
  };
  \path[->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  (m-1-2) edge node [auto] {$*$} (m-1-3)
  ;  
\end{tikzpicture} \\
& \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{pointer}  &   \textbf{pointer} & \text{void}      \\    
  };
  \path[->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  (m-1-2) edge node [auto] {$*$} (m-1-3)
  ;  
  \end{tikzpicture} \\
  & \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \& \verb|dev_c|  &   *(\& \verb|dev_c| ) & (\text{void} **)(\& \verb|dev_c|)      \\    
  };
  \path[->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  (m-1-2) edge node [auto] {$*$} (m-1-3)
  ;  
\end{tikzpicture}
\end{aligned}
\]
I propose that what \verb|cudaMalloc| does (actually) is the following:

\begin{equation}\label{Eq:cudaMallocmodel}
\begin{gathered}
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=3.5em, column sep=4em, minimum width=1em]
  {
    \textbf{Memory}_{\text{GPU}}  &   \textbf{pointers} & \textbf{pointers} & \textbf{Types}      \\    
    & \textbf{pointers}_{\text{GPU}} & \textbf{Types} & \\
  };
  \path[->]
  (m-1-1) edge node [auto] {$\text{cudaMalloc}$} (m-1-2)
  (m-1-2) edge node [auto] {$*$} (m-1-3)
  edge node [right] {$*$} (m-2-2)
  (m-2-2) edge node [auto] {$*$} (m-2-3)
  (m-1-3) edge node [auto] {$*$} (m-1-4)
  ;  
\end{tikzpicture} \\
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=3.5em, column sep=4em, minimum width=1em]
  {
    \text{Memory address}_{\text{GPU}}  &   \& \verb|dev_c|  & *(\& \verb|dev_c| ) & (\text{void} **)(\& \verb|dev_c|)      \\    
    & \verb|dev_c| & *\verb|dev_c| & \\
  };
  \path[|->]
  (m-1-1) edge node [auto] {$\text{cudaMalloc}$} (m-1-2)
  (m-1-2) edge node [auto] {$*$} (m-1-3)
  edge node [right] {$*$} (m-2-2)
  (m-2-2) edge node [auto] {$*$} (m-2-3)
  (m-1-3) edge node [auto] {$*$} (m-1-4)
  ;  
\end{tikzpicture} 
  \end{gathered}
\end{equation}

\verb|dev_c| is now a \emph{device pointer}, available to kernel functions on the GPU.

Syntax-wise, we can relate this diagram to the corresponding function ``usage'':
\[
\begin{gathered}
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=3.5em, column sep=4em, minimum width=1em]
  {
    \textbf{pointers} \times \mathbb{N}^+  & \verb|cudaError_r|  \\
  };
  \path[->]
  (m-1-1) edge node [auto] {$\text{cudaMalloc}$} (m-1-2)
  ;  
\end{tikzpicture} 
\\
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=3.5em, column sep=4em, minimum width=1em]
  {
((\text{void} **)(\& \verb|dev_c|), (\text{sizeof}(\text{int})) ) & \text{cudaSuccess (for example)} \\
  };
  \path[|->]
  (m-1-1) edge node [auto] {$\text{cudaMalloc}$} (m-1-2)
  ;  
\end{tikzpicture} 
  \end{gathered}
\] $\Longrightarrow$
\begin{lstlisting}
  cudaMalloc((void**)&dev_c, sizeof(int))
  \end{lstlisting}

For practice, consider now \verb|cudaMemcpy| in the functor interpretation, and its definition as such:

\verb|cudaMemcpy| is a ``functor category'', s.t. we equip the functor \verb|cudaMemcpy| with a collection of objects $\text{Obj}_{\text{cudaMemcpy}}$, s.t., for example, \verb|cudaMemcpyDevicetoHost| $\in \text{Obj}_{\text{cudaMemcpy}}$, where
\[
( \,  \text{cudaMemcpy}(-,-,n_{\text{thread}}, \text{cudaMemcpyDevicetoHost}): \textbf{Memory}_{GPU} \to \textbf{Memory}_{CPU} \, ) \in \text{Hom}(\textbf{Memory}_{GPU}, \textbf{Memory}_{CPU} )
\]
where $\text{Obj}\textbf{Memory}_{GPU}  \equiv $ collection of all possible memory (addresses) on GPU.

It should be noted that, syntax-wise, $\& c \in \text{Obj}\textbf{Memory}_{CPU}$ and $\& c$ belongs in the ``first slot'' of the arguments for $\text{cudaMemcpy}$, whereas \verb|dev_c| $\in \textbf{pointers}_{GPU}$ a \emph{device pointer}, is ``passed in'' to the ``second slot'' of the arguments for $\text{cudaMemcpy}$.  

\subsection{Threads, Blocks, Grids}

cf. Chapter 5 Thread Cooperation, Section 5.2. Splitting Parallel Blocks of Sanders and Kandrot (2010) \cite{SK2010}.

Consider first a 1-dimensional block.

\begin{itemize}
\item \verb|threadIdx.x| $\Longleftarrow$ $M_x \equiv $ number of threads per block in $x$-direction.  Let $j_x = 0 \dots M_x-1$ be the index for the thread.  Note that $1 \leq M_x \leq M_x^{\text{max}}$, e.g. $M_x^{\text{max}} = 1024$, max. threads per block 
\item \verb|blockIdx.x| $\Longleftarrow$ $N_x \equiv $ number of blocks in $x$-direction.  Let $i_x = 0\dots N_x-1$
\item \verb|blockDim| stores number of threads along each dimension of the block $M_x$.  
  \end{itemize}

Then if we were to ``linearize'' or ``flatten'' in this $x$-direction,
\[
 k = j_x + i_x M_x
 \]
 where $k$ is the $k$th thread.  $k=0\dots N_xM_x -1$.

 Take a look at \href{https://github.com/ernestyalumni/CompPhys/blob/master/CUDA-By-Example/heattexture1.cu}{heattexture1.cu} which uses the GPU texture memory.  Look at how \verb|threadIdx|/\verb|blockIdx| is mapped to pixel position.

 As an exercise, let's again rewrite the code in mathematical notation:
 \begin{itemize}
\item \verb|threadIdx.x| $\Longleftarrow j_x$, $0\leq j_x \leq M_x -1$ 
\item \verb|blockIdx.x| $\Longleftarrow i_x$, $0\leq i_x \leq N_x -1$
\item \verb|blockDim.x| $\Longleftarrow M_x$, number of threads along each dimension (here dimension $x$) of a block, $1 \leq M_x \leq M_x^{\text{max}} = 1024$
\item \verb|gridDim.x| $\Longleftarrow N_x$, $1\leq N_x$
   \end{itemize}
 resulting in
 \begin{itemize}
 \item $k_x = j_x +i_x M_x$ $\Longrightarrow$
   \begin{lstlisting}
   int x =  threadIdx.x + blockIdx.x * blockDim.x ;
     \end{lstlisting}
\item $k_y = j_y +i_y M_y$ $\Longrightarrow$
   \begin{lstlisting}
   int y =  threadIdx.y + blockIdx.y * blockDim.y ;
     \end{lstlisting}
 \end{itemize}
 and so for a ``flattened'' thread index $J \in \mathbb{N}$,
 \[
J = k_x + N_x\cdot M_x \cdot k_y
\]
$\Longrightarrow $
\begin{lstlisting}
  offset = x + y * blockDim.x * gridDim.x ;
  \end{lstlisting}
 
 
 Suppose vector is of length $N$.  So we \emph{need} $N$ parallel threads to launch, in total. \\
 e.g. if $M_x = 128$ threads per block, $N/128 = N/M_x$ blocks to get our total of $N$ threads running.

 Wrinkle: integer division!  e.g. if $N=127 $, $\frac{N}{128} = 0$.

 Solution: consider $\frac{N+127}{128}$ blocks.  If $N = l\cdot 128 + r$, $l\in \mathbb{N}$, $r = 0 \dots 127$.
 \[
 \begin{gathered}
   \frac{N+127}{128} = \frac{ l \cdot 128 + r + 127 }{128} = \frac{ (l+1)128 + r- 1}{128} = \\
   = l+1 + \frac{r-1}{128} = \begin{cases}
     l & \text{ if } r= 0 \\
     l+1 & \text{ if } r = 1 \dots 127
     \end{cases}
 \end{gathered}
 \]
 \[
 \begin{gathered}
   \frac{ N + (M_x - 1) }{M_x} = \frac{ l\cdot M_x + r+ M_x - 1}{M_x} = \frac{ (l+1)M_x + r-1 }{M_x} = \\
   = l+1 + \frac{r-1}{M_x} = \begin{cases}
     l & \text{ if } r = 0 \\
     l +1 & \text{ if } r = 1 \dots M_x -1 
     \end{cases}
 \end{gathered}
 \]
 
 So $\frac{N+(M_x-1)}{M_x}$ is the smallest multiple of $M_x$ greater than or equal to $N$, so $\frac{N + (M_x- 1)}{M_x}$ \textbf{blocks are needed or more than needed to run a total of $N$ threads.}

 
 Problem: Max. grid dim. in 1-direction is 65535, $\equiv N_i^{\text{max}}$.

 So $\frac{ N+ (M_x-1)}{M_x} = N_i^{\text{max}} \Longrightarrow N = N_i^{\text{max}} M_x - (M_x-1) \leq N_i^{\text{max}} M_x$.  i.e. number of threads $N$ is limited by $N_i^{\text{max}} M_x$.

 Solution.

 \begin{itemize}
 \item number of threads per block in $x$-direction $\equiv M_x \Longrightarrow $ \verb|blockDim.x| \\
 \item number of blocks in grid $\equiv N_x \Longrightarrow $ \verb|gridDim.x| 
 \item $N_x M_x$ total number of threads in $x$-direction.  Increment by $N_xM_x$.  So next scheduled execution by GPU at the $k= N_xM_x$ thread.  
   \end{itemize}

 Sanders and Kandrot (2010) \cite{SK2010} made an important note, on pp. 176-177 Ch. 9 Atomics of Section 9.4 Computing Histograms, an important \emph{rule of thumb} on the number of blocks.

 First, consider $N^{\text{threads}}$ total threads.  The extremes are either $N^{\text{threads}}$ threads on a single block, or $N^{\text{threads}}$ blocks, each with a single thread.

 Sanders and Kandrot gave this tip:  \\

 number of blocks, i.e. \verb|gridDim.x| $\Longleftarrow N_x$ $\sim 2 \times $ number of GPU multiprocessors, i.e. twice the number of GPU multiprocessors.  In the case of my GeForce GTX 980 Ti, it has 22 Multiprocessors.  
 
\subsection{(CUDA) Constant Memory}
 
cf. Chapter 6 Constant Memory of Sanders and Kandrot (2010) \cite{SK2010}

Refer to the ray tracing examples in Sanders and Kandrot (2010) \cite{SK2010}, and specifically, here: \href{https://github.com/ernestyalumni/CompPhys/blob/master/CUDA-By-Example/raytrace.cu}{raytrace.cu}, \href{https://github.com/ernestyalumni/CompPhys/blob/master/CUDA-By-Example/rayconst.cu}{rayconst.cu}.

Without constant memory, then this had to be done: 

\begin{itemize}
\item \emph{definition} (in the code) - Consider $\textbf{struct}$ as a subcategory of $\textbf{Types}$ since $\textbf{struct}$ itself is a category, equipped with objects and functions (i.e. methods, modules, etc.).

  So for $\textbf{struct}$, $\text{Obj}\textbf{struct} \ni $ \verb|Sphere|.
  $\Longrightarrow $
  \begin{lstlisting}
    struct sphere { ... }
  \end{lstlisting}
\item Usage, ``instantiation'', i.e. creating, or ``making'' it (the \verb|struct|):

  \[
\begin{gathered}
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=3.5em, column sep=4em, minimum width=1em]
  {
    \textbf{pointers}     & \textbf{Types}  \\
  };
  \path[->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  ;  
\end{tikzpicture}  \\
\begin{aligned}
  & \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=3.5em, column sep=4em, minimum width=1em]
  {
    \textbf{pointer}     & \textbf{struct}  \\
    \textbf{Memory}_{CPU} &  \\
  };
  \path[->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  edge node [left] {$\&$} (m-2-1)
  ;  
\end{tikzpicture} \\
& \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=3.5em, column sep=4em, minimum width=1em]
  {
    s     & *s & \verb|Sphere|  \\
    \textbf{Memory}\text{address}_{CPU} &  \\
  };
  \path[|->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  edge node [left] {$\&$} (m-2-1)
  (m-1-2) edge node [auto] {$\text{typedef}$} (m-1-3)
  ;  
\end{tikzpicture} 
  \end{aligned}
  \end{gathered}
\]
$\Longrightarrow $
\begin{lstlisting}
  Sphere *s
  \end{lstlisting}
  \end{itemize}

Recalling Eq. \ref{Eq:cudaMallocmodel}, for \verb|SPHERES| $== 40$ (i.e. for example, 40 spheres)
\begin{lstlisting}
  cudaMalloc((void **) &s, sizeof(Sphere)*SPHERES)
  \end{lstlisting}
$\Longleftarrow$

\[
\begin{gathered}
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=3.5em, column sep=4em, minimum width=1em]
  {
    \textbf{Memory}_{\text{GPU}}  &   \textbf{pointers} & \textbf{pointers} & \textbf{Types}      \\    
    & \textbf{pointers}_{\text{GPU}} & \textbf{Types} & \\
  };
  \path[->]
  (m-1-1) edge node [auto] {$\text{cudaMalloc}$} (m-1-2)
  (m-1-2) edge node [auto] {$*$} (m-1-3)
  edge node [right] {$*$} (m-2-2)
  (m-2-2) edge node [auto] {$*$} (m-2-3)
  (m-1-3) edge node [auto] {$*$} (m-1-4)
  ;  
\end{tikzpicture} \\
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=3.5em, column sep=4em, minimum width=1em]
  {
    \text{Memory address}_{\text{GPU}}  &   \& s  & *(\& s ) & (\text{void} **)(\& s )      \\    
    & s & *s & \\
  };
  \path[|->]
  (m-1-1) edge node [auto] {$\text{cudaMalloc}$} (m-1-2)
  (m-1-2) edge node [auto] {$*$} (m-1-3)
  edge node [right] {$*$} (m-2-2)
  (m-2-2) edge node [auto] {$*$} (m-2-3)
  (m-1-3) edge node [auto] {$*$} (m-1-4)
  ;  
\end{tikzpicture} 
  \end{gathered}
\]
and syntax-wise,
\[
\begin{gathered}
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=3.5em, column sep=4em, minimum width=1em]
  {
    \textbf{pointers} \times \mathbb{N}^+     & \verb|cudaError_r|  \\
  };
  \path[->]
  (m-1-1) edge node [auto] {\text{cudaMalloc}} (m-1-2)
  ;  
\end{tikzpicture}  \\
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=3.5em, column sep=4em, minimum width=1em]
  {
    ((\text{void} **)(\* s), \text{sizeof}(\text{Sphere}) * \text{SPHERES})     & \text{cudaSuccess (for example) }  \\
  };
  \path[|->]
  (m-1-1) edge node [auto] {\text{cudaMalloc}} (m-1-2)
  ;  
\end{tikzpicture}  \\
\end{gathered}
\]

Now consider
\begin{lstlisting}
  cudaMemcpy(s, temp_s, sizeof(Sphere) * SPHERES, cudaMemcpyHostToDevice)
  \end{lstlisting}

\[
\begin{gathered}
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=3.5em, column sep=12em, minimum width=1em]
  {
    \textbf{Memory}_{CPU}     & \textbf{Memory}_{GPU}  \\
  };
  \path[->]
  (m-1-1) edge node [auto] {$\text{cudaMemcpy}(s, temps ,\text{sizeof}(\text{Sphere}) * \text{SPHERES}, \text{cudaMemcpyHostToDevice} ) $ } (m-1-2)
  ;  
\end{tikzpicture} \\
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=3.5em, column sep=12em, minimum width=1em]
  {
    \textbf{Memory}\text{address}_{CPU}     & \textbf{Memory}\text{address}_{GPU}     \\
    \verb|temp_s| & s \\
  };
  \path[|->]
  (m-1-1) edge node [auto] {$$} (m-1-2)
  edge node [left] {$*$} (m-2-1)
  (m-1-2) edge node [left] {$*$} (m-2-2)
  (m-2-1) edge node [auto] {$$} (m-2-2)
  ;  
\end{tikzpicture} 
  \end{gathered}
\]

The lesson then is this, in light of how long ray tracing takes with constant memory and without constant memory - \verb|cudaMemcpy| between host to device, CPU to GPU, is a costly operation.  Here, in this case, we're copying from the host memory to  memory on the GPU.  It copies to a global memory on the GPU.

Now, using \textbf{constant memory}, \\
we no longer need to do \verb|cudaMalloc|, allocate memory on the GPU, for $s$, pointer to a \verb|Sphere|.

Instead, we have

\begin{lstlisting}
  __constant__ Sphere s[SPHERES];
  \end{lstlisting}
In this particular case, we want it to have global scope.  \\
Note, it is still on host memory.

Notice that
\[
\begin{gathered}
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=3.5em, column sep=18em, minimum width=1em]
  {
    \textbf{Memory}_{CPU}     & \textbf{Memory}_{GPU}  \\
  };
  \path[->]
  (m-1-1) edge node [auto] {$\text{cudaMemcpyHostToDevice}(-, - ,\text{sizeof}(\text{Sphere}) * \text{SPHERES}) $ } (m-1-2)
  ;  
\end{tikzpicture} \\
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=3.5em, column sep=18em, minimum width=1em]
  {
    \textbf{Memory}\text{address}_{CPU}     & \textbf{Memory}\text{adddress}_{GPU}  \\
    \verb|temp_s| & s \\
    \text{array of Sphere's} & \text{array of Sphere's} \\
  };
  \path[|->]
  (m-1-1) edge node [auto] {$\text{cudaMemcpyHostToDevice}(-, - ,\text{sizeof}(\text{Sphere}) * \text{SPHERES}) $ } (m-1-2)
  edge node [left] {$*$} (m-2-1)
  (m-1-2) edge node [left] {$*$} (m-2-2)
  (m-2-1) edge node [auto] {$$} (m-2-2)
  edge node [left] {$\text{typedef}$} (m-3-1)
  (m-2-2) edge node [left] {$\text{typedef}$} (m-3-2)
  (m-3-1) edge node [auto] {$$} (m-3-2)
  ;  
\end{tikzpicture}
\end{gathered}
\]
So notice that we have a bijection, and on one level, we can think of the bijection from \verb|temp_s|, an array of Sphere's to $s$, an array of Sphere's.  So notice that the types and memory size of \verb|temp_s| and $s$ must match. 

And for this case, that's all there is to \emph{constant memory}.  What's going on involves the so-called \emph{warp}, a collection of threads, ``woven together'' and get executed in lockstep.  NVIDIA hardware broadcasts a single memory read to each half-warp. ``If every thread in a half-warp requests data from the same address in constant memory, your GPU will generate only a single read request and subsequently broadcast the data to every thread.'' (cf. Sanders and Kandrot (2010) \cite{SK2010}).  Furthermore, ``the hardware can aggressively cache the constant data on the GPU.''

\subsection{(CUDA) Texture Memory}

\subsection{Do (smooth) manifolds admit a triangulation?}

Topics in Geometric Topology (18.937)


\href{http://www.math.harvard.edu/~lurie/937notes/937Lecture2.pdf}{Piecewise Linear Topology (Lecture 2)}

\part{Computational Fluid Dynamics (CFD); Computational Methods} 

\subsection{Convection (Discretized)}

While I am following Lecture 7 of Darmofal (2005) \cite{Darm2005}, I will generalize to a ``foliated, spatial'' (smooth) manifold $N$, parametrized by time $t\in \mathbb{R}$, $\mathbb{R}\times N$, with $\text{dim}N = n = 1,2 \text{ or } 3$ and to \emph{CUDA} C/C++ parallel programming.

Consider $n$-form $m \in \Omega^N(\mathbb{R}\times N)$, $\text{dim}N = n$.  Then
\begin{equation}\label{Eq:massconservation}
  \begin{gathered}
    \frac{d}{dt} m = \frac{d}{dt} \int_V \rho \text{vol}^n = \int_V \mathcal{L}_{\frac{ \partial }{ \partial t} + \mathbf{u} } \rho \text{vol}^n = \int_V \frac{ \partial \rho }{ \partial t} \text{vol}^n + \mathbf{d}i_{\mathbf{u}} \rho \text{vol}^n = \int_V \left( \frac{ \partial \rho }{ \partial t} + \text{div}( \rho u ) \right) \text{vol}^n = \\
    = \int_V \frac{ \partial \rho }{ \partial t} \text{vol}^n + \int_{\partial V} \rho i_{\mathbf{u}} \text{vol}^n = \dot{m}
\end{gathered}
  \end{equation}
where recall
\[
\begin{aligned}
  \text{div} : \mathfrak{X}(\mathbb{R}\times N) \to C^{\infty}(\mathbb{R} \times N ) \\ 
 \text{div}(\rho \mathbf{u} ) = \frac{1}{\sqrt{g}} \frac{ \partial ( \sqrt{g} u^i \rho ) }{ \partial x^i } 
  \end{aligned}
\]

\subsubsection{1-dimensional case for Convection from mass (scalar) conservation}

Consider Cell $i$, between $x_{i-\frac{1}{2} }$ and $ x_{i+\frac{1}{2} }$, i.e. $[x_{i-\frac{1}{2} }, x_{i+\frac{1}{2} }]\subset \mathbb{R}$.  In this case, Eq. \ref{Eq:massconservation}, for mass conservation with sources, becomes
\[
\begin{gathered}
  \int_V \frac{ \partial \rho}{ \partial t} \text{vol}^n + \int_{\partial V} \rho i_{\mathbf{u}} \text{vol}^n = \int_V \frac{ \partial \rho }{ \partial t} dx + \int_{\partial V} \rho u^i = \int_{x_L}^{x_R} \frac{ \partial \rho }{ \partial t} dx + (\rho(x_R)u(x_R) - \rho(x_L)u(x_L) ) = \frac{d}{dt} \int_{x_L}^{x_R} \rho(x) dx
  \end{gathered}
\]

In the case of $\frac{d}{dt}m = 0$, on a single cell $i$,
\[
\begin{gathered}
  \int_{x_{i-\frac{1}{2} }}^{x_{i+\frac{1}{2} } } \frac{ \partial \rho }{ \partial t} dx + \left. \rho(x) u(x) \right|_{x_i + \frac{1}{2} } - \left. \rho(x)u(x) \right|_{x_i-\frac{1}{2} } = 0 
  \end{gathered}
\]

This is one of the first main approximations Darmofal (2005) \cite{Darm2005} makes, in Eq. 7.10, Section 7.3 Finite Volume Method for Convection, for the \emph{finite volume method}:
\begin{equation}
  \overline{m}_i := \frac{1}{\Delta x_i} \int_{x_{i - \frac{1}{2} }}^{x_{i+\frac{1}{2} } } \rho(x)dx
\end{equation}
where $\Delta x_i \equiv x_{i+\frac{1}{2}} - x_{i+\frac{1}{2}}$.

And so
\begin{equation}
  \begin{gathered}
    \Delta x_i \frac{\partial }{ \partial t} \overline{m}_i + \left. \rho(x)u(x) \right|_{x_{i+\frac{1}{2} } } - \left. \rho(x)u(x) \right|_{x_{i-\frac{1}{2} } } =0
  \end{gathered}
  \end{equation}
We want to discretize this equation also in time.

Consider as first approximation,
\begin{equation}
  \overline{m}(x,t) = \overline{m}_i(t) \qquad \, \forall \, x_{i-\frac{1}{2}} < x < x_{i+\frac{1}{2}}
\end{equation}

Consider then initial time $t$, time step $\Delta t$.   

\subsubsection{1-dimensional ``Upwind'' Interpolation for Finite Volume}

This is the ``major'' approximation for the so-called ``Upwind'' interpolation approximation:
\begin{equation}\label{Eq:rhoUpwindInterp}
  \rho(x_{i+\frac{1}{2} },t+\Delta t) = \begin{cases} \overline{m}_i(t) & \text{ if } u(x_{i+\frac{1}{2} },t) > 0 \\
    \overline{m}_{i+1}(t) & \text{ if } u(x_{i+\frac{1}{2} },t) < 0
    \end{cases}
\end{equation}

Then use the so-called ``forward'' time approximation for $\frac{d}{dt} \overline{m}_i(t)$:
\[
\begin{gathered}
\Delta x_i \frac{ \overline{m}_i(t+\Delta t) - \overline{m}_i(t) }{ \Delta t} + (\rho u)(t,x_{i+\frac{1}{2} } ) - (\rho u)(t,x_{i-\frac{1}{2} } ) = 0
  \end{gathered}
\]

Darmofal (2005) \cite{Darm2005} didn't make this explicit in Lecture 7, but in the approximation for  $\rho(x_{i+\frac{1}{2} },t+\Delta t)$, Eq. \ref{Eq:rhoUpwindInterp}, it's supposed that it's valid at time $t$: $\rho(x_{i+\frac{1}{2} },t) \approx  \rho(x_{i+\frac{1}{2} },t+\Delta t)$, since it's the value of $\rho$ for time moving forward from $t$ (this is implied in Darmofal's code \href{http://ocw.mit.edu/courses/aeronautics-and-astronautics/16-901-computational-methods-in-aerospace-engineering-spring-2005/lecture-notes/convect1d.m}{convect1d}
  %\verb|convect1d.m|}.
\[
\begin{gathered}
  \rho(x_{i+\frac{1}{2} },t)u(x_{i+\frac{1}{2} },t) = \begin{cases} \overline{m}_i(t)u(x_{i+\frac{1}{2} },t) & \text{ if } u(x_{i+\frac{1}{2}},t )>0 \\
    \overline{m}_{i+1}(t)u(x_{i+\frac{1}{2} },t) & \text{ if } u(x_{i+\frac{1}{2}},t )<0
    \end{cases}
  \end{gathered}
\]

Then
\begin{equation}
\begin{gathered}
\frac{ \Delta x_i }{ \Delta t}( \overline{m}_i(t+\Delta t) - \overline{m}_i(t) ) + \\
+ \begin{cases} \overline{m}_i(t)u(x_{i+\frac{1}{2} },t) & \text{ if } u(x_{i+\frac{1}{2}},t )>0 \\
    \overline{m}_{i+1}(t)u(x_{i+\frac{1}{2} },t) & \text{ if } u(x_{i+\frac{1}{2}},t )<0
\end{cases} - \\
- \begin{cases} \overline{m}_{i-1}(t)u(x_{i-\frac{1}{2} },t) & \text{ if } u(x_{i-\frac{1}{2}},t )>0 \\
    \overline{m}_{i}(t)u(x_{i-\frac{1}{2} },t) & \text{ if } u(x_{i-\frac{1}{2}},t )<0
\end{cases} = \\
= 0
  \end{gathered}
  \end{equation}

A note on 1-dimensional gridding: Consider total length $L_0 \in \mathbb{R}^+$.  \\
For $N^{\text{cells}}$ total cells in $x$-direction.  $i=0\dots N^{\text{cells}}-1$.
\[
\begin{aligned}
  & x_{i-\frac{1}{2}} = i \Delta x \qquad \, & i = 0, 1 \dots N^{\text{cells}} - 1 \\ 
  & x_{i+\frac{1}{2}} = (i+1) \Delta x \qquad \, & i = 0, 1 \dots N^{\text{cells}} - 1 \\ 
  & x_i = x_{i-\frac{1}{2} } + \frac{ x_{i+\frac{1}{2}} - x_{i-\frac{1}{2} } }{2} = \frac{ x_{i + \frac{1}{2} } + x_{i-\frac{1}{2} } }{2} = (2 i + 1)\frac{  \Delta x }{2} \qquad \, & i = 0, 1 \dots N^{\text{cells}} - 1 \\ 
\end{aligned}
\]



At this point, instead of what is essentially the so-called ``Upwind Interpolation'', which Darmofal is doing in Lecture 7 of Darmofal (2005) \cite{Darm2005}, and on pp. 76, Chapter 4 Finite Volume Methods, Subsection 4.4.1 Upwind Interpolation (UDS) of Ferziger and Peric (2002) \cite{FP2013}, which is essentially a zero-order approximation, let's try to do better.  

Consider the interval $[x_{i-\frac{1}{2}}, x_{i+\frac{1}{2} } ] \subset \mathbb{R}$.

For the 1-dimensional case of (pure) convection,
\[
\begin{gathered}
  \int_{x_{i - \frac{1}{2}}}^{x_{i+\frac{1}{2} } } \frac{ \partial \rho (t,x) }{ \partial t } dx + \rho(t,x_{ i +\frac{1}{2} } ) u(t,x_{i +\frac{1}{2} } ) - \rho(t, x_{i -\frac{1}{2} } )u(t,x_{i -\frac{1}{2}} ) = \frac{d}{dt} \int_{x_{i -\frac{1}{2} } }^{ x_{i +\frac{1}{2} } } \rho(x) dx
  \end{gathered}
\]
Given $\rho(t,x_{i -\frac{1}{2} }), \rho(t, x_{i +\frac{1}{2} }) \in \mathbb{R}$, do (polynomial) interpolation:
\[
\begin{gathered}
  \mathbb{R} \times \mathbb{R} \xrightarrow{ \text{ interpolation }} \mathbb{R}[x] \equiv \mathcal{P}_{n=1}(\mathbb{R}) \\ 
 \rho(t,x_{i -\frac{1}{2} }), \rho(t, x_{i +\frac{1}{2} }) \mapsto \frac{ (x - x_{ i -\frac{1}{2}}) \rho(t, x_{i +\frac{1}{2}}) - (x - x_{i +\frac{1}{2}} )\rho(t,x_{i -\frac{1}{2}}) }{ h } = \rho_{n=1}(t,x)
\end{gathered}
\]
where $h \equiv x_{i +\frac{1}{2}} - x_{i -\frac{1}{2}}$ and $\mathcal{P}_{n=1}(\mathbb{R})$ is the set of all polynomials of order $n=1$ over field $\mathbb{R}$ (real numbers).

In general,
\[
\begin{gathered}
  \mathbb{R} \times \mathbb{R} \xrightarrow{ \text{ interpolation }} \mathbb{R}[x] \equiv \mathcal{P}_{n=1}(\mathbb{R}) \\ 
 \rho(t,x_L), \rho(t, x_R) \mapsto \frac{ (x - x_L ) \rho(t, x_R) - (x - x_R )\rho(t,x_L) }{ (x_R-x_L) } = \rho_{n=1}(t,x)
\end{gathered}
\]


We interchange the operations of integration and partial derivative - I (correct me if I'm wrong) give two possible reasons why we can do this: the spatial manifold $N$ is fixed in time $t$, and if the grid cell itself is fixed in time, then the partial derivative in time can be moved out of the integration limits.

So, interchanging $\int_{x_{i -\frac{1}{2}}}^{x_{i +\frac{1}{2}}} dx$ and $\frac{ \partial }{ \partial t}$:
\[
\int_{x_{i -\frac{1}{2}}}^{x_{i +\frac{1}{2}}} \frac{ \partial \rho(t,x) }{ \partial t} dx = \frac{ \partial }{ \partial t} \int_{x_{i -\frac{1}{2}}}^{x_{i +\frac{1}{2}}} \rho(t,x)dx
\]
So then
\[
\Longrightarrow \frac{ \partial }{ \partial t} \int_{x_{i -\frac{1}{2}}}^{x_{i +\frac{1}{2}}} \rho_{n=1}(t,x) = \frac{ \partial }{ \partial t} ( \rho(t,x_{i+\frac{1}{2} }  )  + \rho(t,x_{i-\frac{1}{2} } ) ) \frac{ \Delta x}{2}
\]
where $\Delta x = x_{i+\frac{1}{2} } - x_{i+\frac{1}{2} } $.

In general,
\[
\frac{ \partial }{ \partial t} \int_{x_L}^{x_R} \rho_{n=1}(t,x) = \frac{ \partial }{ \partial t}( \rho(t,x_R) + \rho(t,x_L) ) \frac{ (x_R - x_L) }{2}
\]


Then, discretizing,
\begin{equation}
  \Longrightarrow \begin{gathered}
    \left[ (\rho(t+\Delta t, x_{i+\frac{1}{2} } ) + \rho(t+\Delta t, x_{i-\frac{1}{2} } ) ) - (\rho(t, x_{i+\frac{1}{2} } ) + \rho(t, x_{i-\frac{1}{2} } ) ) \right] \frac{\Delta x}{2} \left( \frac{1}{\Delta t} \right) + \rho(t,x_{i+\frac{1}{2} } )u(t,x_{i+\frac{1}{2} } ) - \rho(t,x_{i-\frac{1}{2} } )u(t,x_{i-\frac{1}{2} } ) = \\
    = \dot{m}_{[x_{i-\frac{1}{2} }, x_{i+\frac{1}{2} }] }(t)
  \end{gathered}
  \end{equation}

To obtain $\rho(t,x_{i-\frac{1}{2}})$, consider
\[
\frac{ \partial \rho}{ \partial t} + \text{div}(\rho u) = \frac{d\rho}{dt} = 0 
\]
which is valid at every point on $N$.

Consider for $\text{dim}N=1$,
\[
\frac{ \partial \rho}{ \partial t}(t,x) + \frac{ \partial (\rho u) }{ \partial x}(t,x)
\]

Now, we want $x = x_{i-\frac{1}{2} }$.

Consider
\[
\frac{ \partial \rho(t,x_{i-\frac{1}{2} }) }{ \partial t} \approx \frac{ \rho(t+\Delta t,x_{i-\frac{1}{2} })  - \rho(t,x_{i-\frac{1}{2} } ) }{\Delta t}
\]

Next, consider the (polynomial) interpolation for the $ \frac{ \partial (\rho u) }{ \partial x}(t,x)$ term:
\[
\begin{gathered}
  \mathbb{R} \times \mathbb{R}\times\mathbb{R} \xrightarrow{ \text{interpolate} } \mathbb{R}[x] \equiv \mathcal{P}_{n=2}(\mathbb{R}) \\ 
 \rho(t,x_{i-\frac{3}{2} } ) u(t,x_{i-\frac{3}{2} }) , \rho(t,x_{i-\frac{1}{2} })u(t,x_{i-\frac{1}{2} } ), \rho(t,x_{i+\frac{1}{2} })u(t,x_{i+\frac{1}{2} } ) \xmapsto{ \text{interpolate} } (\rho u)_{n=2}(t,x)
  \end{gathered}
\]
Thus, we can calculate, by plugging into,
\[
\frac{ \partial (\rho u)_{n=2}(t,x_{i-\frac{1}{2} } ) }{ \partial x}
\]

In general, for
\[
\begin{gathered}
  \mathbb{R} \times \mathbb{R}\times\mathbb{R} \xrightarrow{ \text{interpolate} } \mathbb{R}[x] \equiv \mathcal{P}_{n=2}(\mathbb{R}) \\ 
 \rho(t,x_{LL } ) u(t,x_{LL }) , \rho(t,x_{L })u(t,x_{L } ), \rho(t,x_{R })u(t,x_{R } ) \xmapsto{ \text{interpolate} } (\rho u)_{n=2}(t,x)
  \end{gathered}
\]
we have
\[
\begin{gathered}
  \frac{ \partial (\rho u)_{n=2}(t,x_{L } ) }{ \partial x} = 
  \frac{1}{\left(x_{L} - x_{LL}\right) \left(x_{L} - x_{R}\right) \left(x_{LL} - x_{R}\right)} \cdot \\
  \cdot \left(\left(x_{L} - x_{LL}\right)^{2} (\rho u){\left (x_{R} \right )} + \left(x_{L} - x_{LL}\right) \left(x_{LL} - x_{R}\right) (\rho u){\left (x_{L} \right )} - \left(x_{L} - x_{R}\right)^{2} (\rho u){\left (x_{LL} \right )} + \left(x_{L} - x_{R}\right) \left(x_{LL} - x_{R}\right) (\rho u){\left (x_{L} \right )}\right)
\end{gathered}
\]

Thus,
\begin{equation}
\Longrightarrow \begin{gathered}
  \rho(t+\Delta t, x_{i-\frac{1}{2} } ) - \rho(t,x_{i-\frac{1}{2} } ) + \frac{ \partial (\rho u)_{n=2} }{\partial x}(t,x_{i-\frac{1}{2} } )\Delta t = 0  \text{ or } \\
  \rho(t+\Delta t,x_{i-\frac{1}{2} } ) = \rho(t,x_{i-\frac{1}{2} } ) - \frac{ \partial (\rho u)_{n=2} }{ \partial x}(t,x_{i-\frac{1}{2} })\Delta t
  \end{gathered}
\end{equation}

Now a note on the 1-dimensional grid, ``gridding'': for cell $i=0, \dots N^{\text{cell}} -1$, $N^{\text{cell}}$ cells total in the $x$-direction, then
\[
\begin{aligned}
  & x_{i-\frac{1}{2} } = ih \\ 
  & x_{i-\frac{1}{2} } = (i+1)h 
  \end{aligned}
\]
and so $x_{i+\frac{1}{2} } - x_{i-\frac{1}{2} } = h$, meaning the cell width or cell size is $h$.

Thus, in summary, 
\begin{equation}
\begin{gathered}
  \rho(t+\Delta t,x_{i-\frac{1}{2} }) = \rho(t,x_{i-\frac{1}{2} }) - ( \rho(t,x_{i+\frac{1}{2} })u(t,x_{i+\frac{1}{2} } ) - \rho(t,x_{i-\frac{3}{2} })u(t,x_{i-\frac{3}{2} } ) )\left( \frac{1}{2h} \right) \Delta t \\
  \left[ (\rho(t+\Delta t, x_{i+\frac{1}{2} } ) + \rho(t+\Delta t, x_{i-\frac{1}{2} } ) ) - (\rho(t, x_{i+\frac{1}{2} } ) + \rho(t, x_{i-\frac{1}{2} } ) ) \right] \frac{h}{2} \left( \frac{1}{\Delta t} \right) + \rho(t,x_{i+\frac{1}{2} } )u(t,x_{i+\frac{1}{2} } ) - \rho(t,x_{i-\frac{1}{2} } )u(t,x_{i-\frac{1}{2} } ) = \\
    = \dot{m}_{[x_{i-\frac{1}{2} }, x_{i+\frac{1}{2} }] }(t)
  \end{gathered}
  \end{equation}

If one was to include Newtonian gravity, consider this general expression for the time derivative of the momentum flux $\Pi$:
\begin{equation}
\begin{aligned}
  & \Pi = \int_{B(t)} \rho u^i \text{vol}^n \otimes e_i \\ 
  & \dot{\Pi} = \int_{B(t)} \frac{ \partial (\rho u^i ) }{ \partial t} \text{vol}^n \otimes e_i + \int_{B(t)} d(\rho u^i i_u \text{vol}^n ) \otimes e_i = \int_{B(t)} \frac{ \partial (\rho u^i )}{ \partial t} \text{vol}^n \otimes e_i + \int_{\partial B(t)} \rho u^i i_u \text{vol}^n \otimes e_i
\end{aligned}
  \end{equation}
In 1-dim.,
\[
\dot{\Pi} = \int_{B(t)} \frac{ \partial (\rho u)}{\partial t } dx + \int_{\partial B} \rho u^2 = \int_B \frac{GM dm }{r^2} = GM \int_B \frac{ \rho \text{vol}^n }{r^2} = GM \int_B \frac{ \rho dx}{(R-x)^2}
\]
Considering a first-order polynomial interpolation for $\rho$,$\rho_{n=1}$,
\[
\begin{gathered}
  \frac{ \partial }{ \partial t} ((\rho u)(t,x_{i+\frac{1}{2} } ) + (\rho u)(t,x_{i-\frac{1}{2} }) )\frac{h}{2} + \rho u^2(t,x_{i+\frac{1}{2} }) - \rho u^2(t,x_{i-\frac{1}{2} }) = GM \int \frac{ \rho_{n=2} dx}{ (R-x)^2}
  \end{gathered}
\]
Note that we need another equation, at $x=x_{i-\frac{1}{2}}$, similar to above:
\[
\begin{gathered}
  \frac{ \partial (\rho u) }{ \partial t} + \frac{ \partial (\rho u^2) }{ \partial x} = \frac{GM\rho}{(R-x)^2 } \\ 
    \Longrightarrow \begin{gathered}
      \rho u(t+\Delta t,x_{i-\frac{1}{2} } ) - \rho u(t,x_{i-\frac{1}{2} } ) + (\rho u^2(t,x_{i + \frac{1}{2} } ) - \rho u^2(t,x_{i - \frac{3}{2} } ) ) \left( \frac{1}{2h } \right) \Delta t = \Delta t \int GM \frac{ \rho dx }{ (R-x)^2 }
      \end{gathered}
  \end{gathered}
\]

As a recap, the 1-dimensional setup is as follows:
\[
\begin{aligned}
  & \mathbb{R} \times N = \mathbb{R} \times \mathbb{R} \xrightarrow{ \text{discretization }} \mathbb{Z} \times \mathbb{Z} \\ 
  & (t,x) \xmapsto{ \text{discretization} } (t_0 + (\Delta t)j, x_{i-\frac{1}{2} } = ih), \qquad \, i,j \in \mathbb{Z}
  \end{aligned}
\]
Initial conditions for $\rho \in C^{\infty}(\mathbb{R}\times \mathbb{R})$: $\rho(t_0,x) \in C^{\infty}(\mathbb{R}\times \mathbb{R})$.

Choices for $u\in \mathfrak{X}(\mathbb{R}\times \mathbb{R})$:
\begin{itemize}
\item $u(t,x) = u(x)$ (i.e. time-independent velocity vector field)
\item $u(t,x)$ determined by Newtonian gravity (that's an external force on the fluid)
  \end{itemize}

\subsubsection{Note on 1-dimensional gridding}

For, $[0,1] \subset \mathbb{R}$ \\
\phantom{For, } $N$ cells,

Then $1/N = \Delta x$.  Then consider
\[
x_j = j\Delta x \qquad \, j = 0, 1, \dots N
\]



\end{multicols*}
\begin{thebibliography}{9}
\bibitem{HTF2009}
Trevor Hastie, Robert Tibshirani, Jerome Friedman.   \textbf{The Elements of Statistical Learning: Data Mining, Inference, and Prediction}, Second Edition (Springer Series in Statistics) 2nd ed. 2009. Corr. 7th printing 2013 Edition.  ISBN-13: 978-0387848570.  \url{https://web.stanford.edu/~hastie/local.ftp/Springer/OLD/ESLII_print4.pdf}

\bibitem{CS2013}
Jared Culbertson, Kirk Sturtz.  \emph{Bayesian machine learning via category theory}.  \href{http://arxiv.org/abs/1312.1445}{arXiv:1312.1445} [math.CT]

\bibitem{CS344}
John Owens.  David Luebki.  \emph{Intro to Parallel Programming}.  \emph{CS344}.  \textbf{\href{https://www.udacity.com/}{Udacity}}  
  
\url{http://arxiv.org/abs/1312.1445} Also, \url{https://github.com/udacity/cs344}  

\bibitem{CS229}
CS229 Stanford University.  \url{http://cs229.stanford.edu/materials.html}


\bibitem{Fitz}
Richard Fitzpatrick.  ``Computational Physics.''  \url{http://farside.ph.utexas.edu/teaching/329/329.pdf}

\bibitem{Hjor2015}
 M. Hjorth-Jensen, \textbf{Computational Physics}, University of Oslo (2015) \url{http://www.mn.uio.no/fysikk/english/people/aca/mhjensen/}

\bibitem{Stro2013}
 Bjarne Stroustrup.  \textbf{A Tour of C++} (C++ In-Depth Series). Addison-Wesley Professional.   2013. 
 
\bibitem{SK2010}
Jason Sanders, Edward Kandrot.  \textbf{CUDA by Example: An Introduction to General-Purpose GPU Programming} 1st Edition.  Addison-Wesley Professional; 1 edition (July 29, 2010).  ISBN-13: 978-0131387683

\bibitem{Darm2005}
  David Darmofal. *16.901 Computational Methods in Aerospace Engineering, Spring 2005.* (Massachusetts Institute of Technology: MIT OpenCourseWare), \url{http://ocw.mit.edu} (Accessed 12 Jun, 2016). \href{http://creativecommons.org/licenses/by-nc-sa/4.0/}{License: Creative Commons BY-NC-SA}

\bibitem{FP2013}
Joel H. Ferziger and Milovan Peric.  \textbf{Computational Methods for Fluid Dynamics}.  Springer; 3rd edition (October 4, 2013).  ISBN-13: 978-3540420743

I used the 2002 edition since that was the only copy I had available.  

\end{thebibliography}

\end{document}

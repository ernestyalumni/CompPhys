% file: CompPhys.tex
% Computational Physics (notes), in unconventional ``grande'' format; fitting a widescreen format
% 
% github        : ernestyalumni
% gmail         : ernestyalumni 
% linkedin      : ernestyalumni 
% wordpress.com : ernestyalumni
%
% This code is open-source, governed by the Creative Common license.  Use of this code is governed by the Caltech Honor Code: ``No member of the Caltech community shall take unfair advantage of any other member of the Caltech community.'' 

\documentclass[10pt]{amsart}
\pdfoutput=1
\usepackage{mathtools,amssymb,lipsum,caption}

\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage[table]{xcolor}
\usepackage{pdfpages}
\usepackage{tikz}
\usetikzlibrary{matrix,arrows}

\usepackage{multicol}

\hypersetup{colorlinks=true,citecolor=[rgb]{0,0.4,0}}

\oddsidemargin=15pt
\evensidemargin=5pt
\hoffset-45pt
\voffset-55pt
\topmargin=-4pt
\headsep=5pt
\textwidth=1120pt
\textheight=595pt
\paperwidth=1200pt
\paperheight=700pt
\footskip=40pt








\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
%\newtheorem*{main}{Main Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}

\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

\newenvironment{claim}[1]{\par\noindent\underline{Claim:}\space#1}{}
\newenvironment{claimproof}[1]{\par\noindent\underline{Proof:}\space#1}{\hfill $\blacksquare$}

%This defines a new command \questionhead which takes one argument and
%prints out Question #. with some space.
\newcommand{\questionhead}[1]
  {\bigskip\bigskip
   \noindent{\small\bf Question #1.}
   \bigskip}

\newcommand{\problemhead}[1]
  {
   \noindent{\small\bf Problem #1.}
   }

\newcommand{\exercisehead}[1]
  { \smallskip
   \noindent{\small\bf Exercise #1.}
  }

\newcommand{\solutionhead}[1]
  {
   \noindent{\small\bf Solution #1.}
   }


\title{Computational Physics: includes Parallel Computing/Parallel Programming}
\author{Ernest Yeung \href{mailto:ernestyalumni@gmail.com}{ernestyalumni@gmail.com}}
\date{23 mai 2016}
\keywords{Computational Physics, Parallel Computing, Parallel Programming}
\begin{document}

\definecolor{darkgreen}{rgb}{0,0.4,0}
\lstset{language=C++,
  keywordstyle=\color{blue},
  stringstyle=\color{red},
 commentstyle=\color{darkgreen}
 }
%\lstlistoflistings

\maketitle

\tableofcontents


\begin{multicols*}{2}

\begin{abstract}
Everything about Computational Physics, including Parallel computing/ Parallel programming.  
\end{abstract}

\part{Introduction}

\section{Parallel Computing}

\subsection{Udacity Intro to Parallel Programming : Lesson 1 - The GPU Programming Model}

Owens and Luebki pound fists at the end of this video.  $=))))$  \href{https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/658304810923}{Intro to the class}.

\subsubsection{Running CUDA locally}
Also, \href{https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/658304810923}{Intro to the class}, in Lesson 1 - The GPU Programming Model, has links to documentation for running CUDA locally; in particular, for Linux: \url{http://docs.nvidia.com/cuda/cuda-getting-started-guide-for-linux/index.html}.  That guide told me to go download the NVIDIA CUDA Toolkit, which is the \href{NVIDIA CUDA Developer Toolkit}{https://developer.nvidia.com/cuda-downloads}.  

For \emph{Fedora}, I chose Installer Type \verb|runfile (local)|.  

Afterwards, installation of CUDA on Fedora 23 workstation had been nontrivial.  Go see either my github repository \href{https://github.com/ernestyalumni/MLgrabbag/blob/master/README.md}{MLgrabbag} (which will be updated) or my \href{https://ernestyalumni.wordpress.com/2016/05/07/fedora-23-workstation-linuxnvidia-geforce-gtx-980-ti-my-experience-log-of-what-i-do-and-find-out/#CUDAinstall}{wordpress blog} (which may not be upgraded frequently).  


$P=VI = I^2R$ heating.

\subsubsection{Definitions of Latency and throughput (or bandwidth)}

cf. 
\href{https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/669874580923}{Building a Power Efficient Processor}

\href{https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/667559300923}{Latency vs Bandwidth}

latency $[\text{sec}]$.  From the title ``Latency vs. bandwidth'', I'm thinking that throughput $=$ bandwidth (???).  throughput $ = $ job$/$time (of job).  

Given total task, velocity $v$, \\
total task $/v = $ latency.  throughput $=$ latency$/(\text{jobs per total task})$.  


Also, in \href{https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/669874580923}{Building a Power Efficient Processor}.  Owens recommends the article David Patterson, ``Latency...''

cf. \href{https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/671181630923}{GPU from the Point of View of the Developer}

$n_{\text{core}} \equiv $ number of cores \\
$n_{\text{vecop}} \equiv$ ($n_{\text{vecop}}-$wide axial vector operations$/core$ core) \\
$n_{\text{thread}} \equiv $ threads$/$core (hyperthreading)
\[
n_{\text{core}} \cdot n_{\text{vecop}} \cdot n_{\text{thread}}  \text{ parallelism  }
\]

There were various websites that I looked up to try to find out the capabilities of my video card, but so far, I've only found these commands (and I'll print out the resulting output):
{\scriptsize
\begin{lstlisting}
$ lspci -vnn | grep VGA -A 12
03:00.0 VGA compatible controller [0300]: NVIDIA Corporation GM200 [GeForce GTX 980 Ti] [10de:17c8] (rev a1) (prog-if 00 [VGA controller])
	Subsystem: eVga.com. Corp. Device [3842:3994]
	Physical Slot: 4
	Flags: bus master, fast devsel, latency 0, IRQ 50
	Memory at fa000000 (32-bit, non-prefetchable) [size=16M]
	Memory at e0000000 (64-bit, prefetchable) [size=256M]
	Memory at f0000000 (64-bit, prefetchable) [size=32M]
	I/O ports at e000 [size=128]
	[virtual] Expansion ROM at fb000000 [disabled] [size=512K]
	Capabilities: <access denied>
	Kernel driver in use: nvidia
	Kernel modules: nouveau, nvidia

$ lspci | grep VGA -E
03:00.0 VGA compatible controller: NVIDIA Corporation GM200 [GeForce GTX 980 Ti] (rev a1)

$ grep driver /var/log/Xorg.0.log
[    18.074] Kernel command line: BOOT_IMAGE=/vmlinuz-4.2.3-300.fc23.x86_64 root=/dev/mapper/fedora-root ro rd.lvm.lv=fedora/root rd.lvm.lv=fedora/swap rhgb quiet LANG=en_US.UTF-8 nouveau.modeset=0 rd.driver.blacklist=nouveau nomodeset gfxpayload=vga=normal
[    18.087] (WW) Hotplugging is on, devices using drivers 'kbd', 'mouse' or 'vmmouse' will be disabled.
[    18.087] 	X.Org XInput driver : 22.1
[    18.192] (II) Loading /usr/lib64/xorg/modules/drivers/nvidia_drv.so
[    19.088] (II) NVIDIA(GPU-0): Found DRM driver nvidia-drm (20150116)
[    19.102] (II) NVIDIA(0):     ACPI event daemon is available, the NVIDIA X driver will
[    19.174] (II) NVIDIA(0): [DRI2]   VDPAU driver: nvidia
[    19.284] 	ABI class: X.Org XInput driver, version 22.1
...

$ lspci -k | grep -A 8 VGA
03:00.0 VGA compatible controller: NVIDIA Corporation GM200 [GeForce GTX 980 Ti] (rev a1)
	Subsystem: eVga.com. Corp. Device 3994
	Kernel driver in use: nvidia
	Kernel modules: nouveau, nvidia
03:00.1 Audio device: NVIDIA Corporation GM200 High Definition Audio (rev a1)
	Subsystem: eVga.com. Corp. Device 3994
	Kernel driver in use: snd_hda_intel
	Kernel modules: snd_hda_intel
05:00.0 USB controller: VIA Technologies, Inc. VL805 USB 3.0 Host Controller (rev 01)
  \end{lstlisting}
}
\href{https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/671181640923}{CUDA Program Diagram}

\[
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=3em, column sep=4em, minimum width=1em]
  {
    & \text{ CUDA program in C with extensions } &  \\
    \text{ CPU ``Host'' } & & \text{ GPU ``Device'' } \\
    \text{ Memory } & & \text{Memory} \\
};
  \path[->]
  (m-1-2) edge node [above] {} (m-2-1)
  edge node [above] {} (m-2-3)
  (m-2-1) edge node [above] { coprocessor } (m-2-3)
  edge node [above] {} (m-3-1)
  (m-3-1) edge node [above] {$1$} (m-3-3)
  (m-3-3) edge [bend left=20] node [below] {$2$} (m-3-1)
  edge [loop right] node [right] {$3$} (m-3-3)
  (m-2-3) edge [loop right] node [right] {$4$} (m-2-3)
  ;
\end{tikzpicture}
\]
CPU ``host'' is the boss (and issues commands) -Owen.

$\text{Coprocessor} : \text{ CPU ``host'' } \to \text{ GPU ``device'' } $ \\
$\text{Coprocessor} : \text{ CPU process } \mapsto \text{ (co)-process out to GPU } $ \\

With
\begin{enumerate}
  \item[1] data cpu $\to $ gpu
  \item[2] data gpu $\to$ cpu \qquad (initiated by cpu host) \\

$1.,2.,$ uses \verb|cudaMemcpy| 
  \item[3] allocate GPU memory: \verb|cudaMalloc|
  \item[4] launch kernel on GPU
  \end{enumerate}
Remember that for 4., this launching of the kernel, while it's acting on GPU ``device'' onto itself, it's initiated by the boss, the CPU ``host''.

Hence, cf. \href{https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/670489380923}{Quiz: What Can GPU Do in CUDA}, GPUs can respond to CPU request to receive and send Data CPU $\to $ GPU and Data GPU $\to $ CPU, respectively (1,2, respectively), and compute a kernel launched by the CPU (3).


\href{https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/670742800923}{A CUDA Program}
A typical GPU program

\begin{itemize}
\item \verb|cudaMalloc| - CPU allocates storage on GPU 
\item \verb|cudaMemcpy| - CPU copies input data from CPU $\to $ GPU 
\item \emph{kernel launch} - CPU launches kernel(s) on GPU to process the data 
\item \verb|cudaMemcpy| - CPU copies results back to CPU from GPU
  \end{itemize}

Owens advises minimizing ``communication'' as much as possible (e.g. the \verb|cudaMemcpy| between CPU and GPU), and do a lot of computation in the CPU and GPU, each separately.

\href{https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/672300540923}{Defining the GPU Computation}

Owens circled this
{\Large
  \[
  \begin{gathered}
\text{ BIG IDEA } \qquad \, \boxed{ \text{ This is Important } }  \\
\begin{aligned} 
& \text{ Kernels look like serial programs } \\
  & \text{ Write your program as if it will run on \textbf{ one } thread } \\
  & \text{The GPU will run that program on \textbf{ many } threads}
  \end{aligned}
\end{gathered}
\]
}

\href{https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/670742840923}{Squaring A Number on the CPU}

Note
\begin{enumerate}
\item Only 1 thread of execution: (``thread'' $:=$ one independent path of execution through the code) e.g. the \verb|for| loop
  \item no explicit parallelism; it's serial code e.g. the \verb|for| loop through 64 elements in an array
  \end{enumerate}


\href{https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/670742870923}{GPU Code A High Level View}

CPU:
\begin{itemize}
  \item Allocate Memory 
  \item Copy Data to/from GPU
    \item Launch Kernel - species degree of parallelism
\end{itemize}

GPU:
\begin{itemize}
\item Express Out $=$ In $\cdot $ In  - says \emph{nothing} about the degree of parallelism
  \end{itemize}

Owens reiterates that in the GPU, everything looks serial, but it's only in the CPU that anything parallel is specified.  

pseudocode: CPU code: square kernel $<<< 64 >>>$ (outArray,inArray)

\href{https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/670742940923}{Squaring Numbers Using CUDA Part 3}

From the example
\begin{lstlisting}
  // launch the kernel
  square<<<1, ARRAY_SIZE>>>(d_out, d_in)
  \end{lstlisting}
we're introduced to the ``CUDA launch operator'', initiating a kernel of 1 block of 64 elements (\verb|ARRAY_SIZE| is 64) on the GPU.  Remember that \verb|d_| prefix (this is naming comvention) tells us it's on the device, the GPU, solely.  

With CUDA launch operator $\equiv <<<>>>$, then also looking at this explanation on \verb|stackexchange| (so surely others are confused as well, of those who are learning this (cf. \href{http://stackoverflow.com/questions/19240658/cuda-kernel-launch-parameters-explained-right}{CUDA kernel launch parameters explained right?}).  From \href{http://stackoverflow.com/users/1957265/eric}{Eric}'s answer, \\

threads are grouped into blocks.  all the threads will execute the invoked kernel function.

Certainly,
\[
\begin{aligned}
  & <<<>>>:(n_{\text{block}}, n_{\text{threads}})\times \text{kernelfunctions} \mapsto \text{kernelfunction}<<<n_{\text{block}},n_{\text{threads}}>>> \in \text{End}:\text{Dat}_{\text{GPU}} \\ 
  & <<<>>>: \mathbb{N}^+ \times \mathbb{N}^+ \times \text{Mor}_{\text{GPU}} \to \text{End}\text{Dat}_{\text{GPU}}
  \end{aligned}
\]
where I propose that GPU can be modeled as a category containing objects $\text{Dat}_{\text{GPU}}$, the collection of all possible data inputs and outputs into the GPU, and $\text{Mor}_{\text{GPU}}$, the collection of all kernel functions that run (exclusively, and this \emph{must} be the class, as reiterated by Prof. Owen) on the GPU.

Next,
\[
\begin{aligned}
  & \text{kernelfunction}<<<n_{\text{block}},n_{\text{threads}}>>>: \text{din}\mapsto \text{dout} \qquad \, (\text{as given in the ``square'' example, and so I propose}) \\ 
  & \text{kernelfunction}<<<n_{\text{block}},n_{\text{threads}}>>>:(\mathbb{N}^+)^{n_{\text{threads}}} \to (\mathbb{N}^+)^{n_{\text{threads}}}
  \end{aligned}
\]
But keep in mind that $\text{dout}$, $\text{din}$ are pointers in the C program, pointers to the place in the memory.  

\verb|cudaMemcopy| is a functor category, s.t. e.g. $\text{Obj}_{\text{CudaMemcopy}} \ni \text{cudaMemcpyDevicetoHost}$ where
\[
\text{cudaMemcopy}(-,-,n_{\text{thread}},\text{cudaMemcpyDeviceToHost}): \text{Memory}_{\text{GPU}} \to \text{Memory}_{\text{CPU}} \in \text{Hom}(\text{Memory}_{\text{GPU}}, \text{Memory}_{\text{CPU}})
\]

\href{https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/670742910923}{Squaring Numbers Using CUDA 4}

Note the C language construct \emph{declaration specifier} - denotes that this is a kernel (for the GPU) and not CPU code.  Pointers need to be allocated on the GPU (otherwise your program will crash spectacularly -Prof. Owen).  

\subsubsection{What are C pointers?}

Is $\langle \text{ type } \rangle \, *$, a pointer, then a mapping from the category, namely the objects of types, to a mapping from the specified value type to a memory address?

e.g.
\[
\begin{aligned}
  \langle \, \rangle \, * & : \text{float} \mapsto \text{float} \, * \\ 
  \text{float } \, * & : \text{din} \mapsto \text{ some memory address }
\end{aligned}
\]
and then we pass in mappings, not values, and so we're actually declaring a square \emph{functor}.

What is \verb|threadIdx|?  What is it mathematically?  Consider that $\exists \,$ 3 ``modules'':

\[
\begin{aligned}
  & \text{threadIdx}.x \\
  & \text{threadIdx}.y \\
  & \text{threadIdx}.z 
\end{aligned}
\]
And then the line
\begin{lstlisting}
int idx = threadIdx.x;
  \end{lstlisting}
says that idx is an integer, ``declares'' it to be so, and then assigns idx to $\text{threadIdx}.x$ which surely has to also have the same type, integer.  So (perhaps)
\[
idx \equiv \text{threadIdx}.x \in \mathbb{Z}
\]
is the same thing.

Then suppose threadIdx $\subset \mathbf{\text{FinSet}}$, a subcategory of the category of all (possible) finite sets, s.t. threadIdx has 3 particular morphisms, $x,y,z\in \text{Mor}threadIdx$,
\[
\begin{aligned}
  & x : \text{threadIdx} \mapsto \text{threadIdx}.x \in \text{Obj}_{\mathbf{\text{FinSet}}} \\ 
  & y : \text{threadIdx} \mapsto \text{threadIdx}.x \in \text{Obj}_{\mathbf{\text{FinSet}}} \\ 
  & z : \text{threadIdx} \mapsto \text{threadIdx}.x \in \text{Obj}_{\mathbf{\text{FinSet}}}  
\end{aligned}
\]

\href{https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/670742980923}{Configuring the Kernel Launch Parameters Part 1}

$n_{\text{blocks}}$, $n_{\text{threads}}$ with $n_{\text{threads}} \geq 1024$ (this maximum constant is GPU dependent).  You should pick the $(n_{\text{blocks}}, n_{\text{threads}})$ that makes sense for your problem, says Prof. Owen.  

\subsubsection{More thoughts on Squaring Numbers Using CPU, and then using CUDA}

Note that this squaring of numbers is really element-wise multiplication of a vector.

I sought an isomorphism between abstract algebra and computer code.

Consider
\[
\begin{aligned}
 &  \mathbb{R}^N \ni x \qquad \, N \in \mathbb{Z}^+ \\ 
 &  \mathbb{R} \ni x[i] \qquad i = 1 \dots N \to i = 0, \dots N-1 
  \end{aligned}
\]
Then the element-wise squaring of numbers is
\[
(x[i])^2 = x[i] \cdot x[i]
\]
In general,
\[
(x[i])^p = \underbrace{x[i]\cdot x[i] \dots x[i]}_{ p \text{ times } }
\]

\subsubsection{Memory layout of blocks and threads}

$\forall \, (n_{\text{blocks}}, n_{\text{threads}}) \in \mathbb{Z} \times \lbrace 1 \dots 1024 \rbrace$, $\lbrace 1 \dots n_{\text{block}} \times \lbrace 1 \dots n_{\text{threads}} \rbrace$ is now an ordered index (with lexicographical ordering).  This is just 1-dimensional (so possibly there's a 1-to-1 mapping to a finite subset of $\mathbb{Z}$).

I propose that ``adding another dimension'' or the 2-dimension, that Prof. Owen mentions is being able to do the Cartesian product, up to 3 Cartesian products, of the block-thread index.  

\href{https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/668398860923}{Quiz: Configuring the Kernel Launch Parameters 2 }

Most general syntax:

Configuring the kernel launch
\begin{lstlisting}
  kernel<<<grid of blocks, block of threads >>>(...)

  // for example

  square<<<dim3(bx,by,bz), dim3(tx,ty,tz), shmem>>>(...)
  \end{lstlisting}
where \verb|dim3(tx,ty,tz)| is the grid of blocks $bx\cdot by \cdot bz$ \\
\phantom{ where } \verb|{dim3}(tx,ty,tz)| is the block of threads $tx \cdot ty \cdot tz$ \\
\phantom{ where } \verb|shmem| is the shared memory per block in bytes


\href{https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/673071460923#}{Quiz: Map}

I wanted to try to mathematically formulate the idea of \verb|map|.

\[
\begin{gathered}
  \verb|MAP(ELEMENTS,FUNCTION)|
  \end{gathered} \Longleftrightarrow
\begin{gathered}
\begin{gathered}
  \text{ given } x \in \mathbb{R}^N \\ 
  x[i] \xrightarrow{ f } f(x[i]) 
\end{gathered} \\
\text{ or } \\ 
\begin{gathered}
  \text{ set of elements (finite, so can be indexed) } \qquad \, \lbrace x_0 , \dots , x_{n-1} \rbrace_{\mathcal{A}} \in \text{Obj}\mathbf{ \text{Fin} } \\
  x_i \xrightarrow{ f } f(x_i) , \qquad \, \forall \, i \in \mathcal{A}
  \end{gathered}
  \end{gathered}
\]

\href{https://classroom.udacity.com/courses/cs344/lessons/55120467/concepts/967066740923}{Problem Set 1}
``Also, the image is represented as an 1D array in the kernel, not a 2D array like I mentioned in the video.''

Here's part of that code for squaring numbers:
\begin{lstlisting}
  __global__ void square(float *d_out, float *d_in) {
    int idx = threadIdx.x;
    float f = d_in[idx];
    d_out[idx] = f*f;
    }
  \end{lstlisting}

\subsubsection{Problem Set 1, Udacity CS344}

Let $\begin{aligned} & \quad \\
  & L_x \equiv \text{ total number of pixels in $x$-direction of image } \in \mathbb{Z}^+ \\
   & L_y \equiv \text{ total number of pixels in $y$-direction of image } \in \mathbb{Z}^+ \end{aligned}$

and so $L_x L_y  = $ total number of pixels in image.

The formula for ensuring that all threads will be computed, given an arbitrary choice of the number of threads in a (single) block, is the following:

\[
\begin{aligned}
  & \frac{L_x + ( M_x - 1) }{ M_x} = N_x \in \mathbb{N} \qquad \, & N_x = \text{ number of (thread) blocks in $x$-direction } \\ 
  & \frac{L_y + ( M_y - 1) }{ M_y} = N_y \in \mathbb{N} \qquad \, & N_y = \text{ number of (thread) blocks in $y$-direction } 
\end{aligned}
\]

Then
\[
(M_x, M_y, 1) \in \mathbb{N}^3  \Longleftrightarrow \verb|dim3|
\]
needs to be determined manually, empirically, and in consideration of the actual GPU hardware architecture (look up number of CUDA cores, and allowed maximum threads), where
\[
\begin{aligned}
  & M_x \equiv \text{ number of threads per block in $x$-direction } \\ 
  & M_y \equiv \text{ number of threads per block in $y$-direction } 
  \end{aligned}
\]

Consider that we want to go from the indices on each thread per block, on each block on the grid, in each of the 2 dimensions, to a global 2-dimensional position, and then ``flatten'' these coordinates to a 1-dimensional array that CUDA C can load onto global memory.  In other words, for

\[
\begin{gathered}
\begin{aligned}  
  & i_x \in \lbrace 0 , \dots , M_x - 1 \rbrace \qquad \, & \Longleftrightarrow \verb| threadIdx.x| \\ 
  & i_y \in \lbrace 0 , \dots , M_y - 1 \rbrace \qquad \, & \Longleftrightarrow \verb| threadIdx.y|
  \end{aligned} \\ 
\begin{aligned}
  & j_x \in \lbrace 0 , \dots , N_x - 1 \rbrace \qquad \, & \Longleftrightarrow \verb| blockIdx.x| \\ 
  & j_y \in \lbrace 0 , \dots , N_y - 1 \rbrace \qquad \, & \Longleftrightarrow \verb| blockIdx.y|
  \end{aligned}
\end{gathered}
\]

and so for
\[
\begin{gathered}
  (k_x,k_y) \\
\begin{aligned}
  & k_X = i_x + j_x M_x \\ 
  & k_y = i_y + j_y M_y
  \end{aligned} 
  \end{gathered}
\]

then we sought the following operations:
\[
\begin{gathered}
  (j_x,j_y)\times (i_x,i_y) \in \lbrace 0 , \dots , N_x - 1 \rbrace \times \lbrace 0 \dots N_y - 1 \rbrace \times \lbrace 0 \dots M_x - 1 \rbrace \times \lbrace 0 \dots M_y - 1 \rbrace \in \verb|dim3| \times \verb|dim3| \\
  \mapsto (k_x,k_y) \in \lbrace 0 \dots L_x -1 \rbrace \times \lbrace 0 \dots L-y - 1 \rbrace \\
  \mapsto k = k_x + L_x k_y \in \lbrace 0 \dots L_x L_y - 1 \rbrace 
  \end{gathered}
\]

\subsubsection{Grid of blocks, block of threads, thread that's indexed; (mathematical) structure of it all}

Let
\[
\begin{gathered}
  \text{grid} = \prod_{I=1}^N (\text{block})^{n_I^{\text{block}}}
\end{gathered}
\]
where $N=1,2,3$ (for CUDA) and by naming convention $\begin{aligned} & \quad \\
  & I = 1 \equiv x \\
  & I = 2 \equiv y \\
  & I = 3 \equiv z \end{aligned}$

Let's try to make it explicitly (as others had difficulty understanding the grid, block, thread model, cf. \href{http://stackoverflow.com/questions/14711668/colored-image-to-greyscale-image-using-cuda-parallel-processing}{colored image to greyscale image using CUDA parallel processing}, \href{http://stackoverflow.com/questions/16619274/cuda-griddim-and-blockdim}{Cuda gridDim and blockDim}) through commutative diagrams and categories (from math):

\[
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=4em, column sep=5em, minimum width=2em]
  {
\prod_{I=1}^N \mathbb{Z}^+ & \ni (N_x^{\text{blocks}}, N_y^{\text{blocks}} , N_z^{\text{blocks}}) \\
\text{grid} & \ni \text{gridSize}(N_x^{\text{blocks}}, N_y^{\text{blocks}} , N_z^{\text{blocks}} ) \\
};
  \path[->]
  (m-1-1) edge node [right] {$\text{dim}3$} (m-2-1)
  (m-2-1) edge [bend left=40, thick] node [left] {$\text{gridDim}$} (m-1-1)
  ;
  \path[|->]
  (m-1-2) edge node [left] {$\text{dim}3$} (m-2-2)
  (m-2-2) edge [bend right=40, thick] node [right] {$(\text{gridDim}.x$, $\text{gridDim}.y$, $\text{gridDim}.z)$} (m-1-2)
  ;  
\end{tikzpicture}
\]

\[
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=4em, column sep=5em, minimum width=2em]
  {
    \text{grid} & \ni \verb|d_rgbaImage| \\
    \prod_{I=1}^N \mathbb{Z} \supset \prod_{I=1}^N \lbrace 1 \dots N_I^{\text{blocks}} \rbrace & \ni (i^{\text{blocks}},j^{\text{blocks}}, k^{\text{blocks}} ) \\
};
  \path[->]
  (m-1-1) edge node [right] {$\text{blockIdx}$} (m-2-1)
  ;
  \path[|->]
  (m-1-2) edge node [right] {($\text{blockIdx}.x$,$\text{blockIdx}.y$,$\text{blockIdx}.z$)} (m-2-2)
  ;
  \end{tikzpicture}
\]

and then similar relations (i.e. arrows, i.e. relations) go for a block of threads:

\[
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=4em, column sep=5em, minimum width=2em]
  {
\prod_{I=1}^N \mathbb{Z}^+ & \ni (N_x^{\text{threads}}, N_y^{\text{threads}} , N_z^{\text{threads}}) \\
\text{block} & \ni \text{blockSize}(N_x^{\text{threads}}, N_y^{\text{threads}} , N_z^{\text{threads}} ) \\
};
  \path[->]
  (m-1-1) edge node [right] {$\text{dim}3$} (m-2-1)
  (m-2-1) edge [bend left=40, thick] node [left] {$\text{blockDim}$} (m-1-1)
  ;
  \path[|->]
  (m-1-2) edge node [left] {$\text{dim}3$} (m-2-2)
  (m-2-2) edge [bend right=40, thick] node [right] {$(\text{blockDim}.x$, $\text{blockDim}.y$, $\text{blockDim}.z)$} (m-1-2)
  ;  
\end{tikzpicture}
\]

\[
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=4em, column sep=5em, minimum width=2em]
  {
    \mathbf{\text{block}} & \ni \text{block} \\
    \prod_{I=1}^N \mathbb{Z} \supset \prod_{I=1}^N \lbrace 1 \dots N_I^{\text{threads}} \rbrace & \ni (i^{\text{threads}},j^{\text{threads}}, k^{\text{threads}} ) \\
};
  \path[->]
  (m-1-1) edge node [right] {$\text{threadIdx}$} (m-2-1)
  ;
  \path[|->]
  (m-1-2) edge node [right] {($\text{threadIdx}.x$,$\text{threadIdx}.y$,$\text{threadIdx}.z$)} (m-2-2)
  ;
  \end{tikzpicture}
\]

\href{https://discussions.udacity.com/t/gridsize-help-assignment-1-pp/124701}{gridsize help assignment 1 Pp} explains how threads per block is variable, and remember how Owens said Luebki says that a GPU doesn't get up for more than a 1000 threads per block.  

\subsubsection{Generalizing the model of an image}

Consider vector space $V$, e.g. $\text{dim}V=4$, vector space $V$ over field $\mathbb{K}$, so $V= \mathbb{K}^{\text{dim}V}$.

Each pixel represented by $\forall \, v \in V$.

Consider an image, or space, $M$.  $\text{dim}M = 2$ (image), $\text{dim}M=3$.  Consider a local chart (that happens to be global in our case):
\[
\begin{aligned}
  & \varphi : M \to \mathbb{Z}^{\text{dim}M} \supset \lbrace 1 \dots N_1 \rbrace \times \lbrace 1 \dots N_2 \rbrace \times \dots \times \lbrace 1 \dots N_{\text{dim}M} \rbrace \\ 
  & \varphi : x \mapsto (x^1(x), x^2(x), \dots , x^{\text{dim}M}(x) )
  \end{aligned}
\]
\[
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=4em, column sep=5em, minimum width=2em]
  {
    E & M \times V \\ 
    M &  \\
};
  \path[->]
  (m-1-1) edge node [auto] {$\varphi$} (m-1-2)
          edge node [left] {$\pi$} (m-2-1)
  (m-1-2) edge node [auto] {$$} (m-2-1)
              ;
  \end{tikzpicture}
\qquad \, 
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=4em, column sep=5em, minimum width=2em]
  {
    E & \text{grid} \times \text{ block of threads } \\ 
    \text{ grid } & \\
};
  \path[->]
  (m-1-1) edge node [auto] {$\varphi$} (m-1-2)
          edge node [left] {$\pi$} (m-2-1)
  (m-1-2) edge node [auto] {$$} (m-2-1)
              ;
  \end{tikzpicture}
\]

Consider a ``coarsing'' of underlying $M$:
\[
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=2em, column sep=3em, minimum width=1em]
  {
    M\times V & \text{proj}(M) \times \text{proj}(V) \\ 
    M = \lbrace 1 \dots N_1 \rbrace \times \lbrace 1 \dots N_2 \rbrace \times \dots \times \lbrace 1 \dots N_{\text{dim}M} \rbrace  & \text{proj}(M)  = \lbrace 1 \dots \frac{N_1}{N_1^{\text{threads}} } \rbrace \times \lbrace 1 \dots \frac{N_2}{N_2^{\text{threads}} } \rbrace \times \dots \times \lbrace 1 \dots \frac{N_{\text{dim}M}}{N_{\text{dim}M}^{\text{threads}} } \rbrace \\  
};
  \path[->]
  (m-1-1) edge node [auto] {$\text{proj}$} (m-1-2)
          edge node [left] {$\pi$} (m-2-1)
  (m-1-2) edge node [auto] {$\text{proj}(\pi)$} (m-2-2)
  (m-2-1) edge node [auto] {$\text{proj}$} (m-2-2)        
          ;
  \end{tikzpicture}
\]
e.g. $\begin{aligned} & \quad \\
  & N_1^{\text{thread}} = 12 \\
  & N_2^{\text{thread}} = 12 \end{aligned}$

Just note that in terms of syntax, you have the ``block'' model, in which you allocate blocks along each dimension.  So in
\[
\begin{aligned}
  & const \; dim3 \; blockSize(n^b_x, n^b_y, n^b_z) \\
  & const \; dim3 \; gridSize(n^{\text{gr}}_x, n^{\text{gr}}_y, n^{\text{gr}}_z)
  \end{aligned}
\]
Then the condition is
$n_x^b/\text{dim}V , n_y^b/\text{dim}V, n_z^b/\text{dim}V \in \mathbb{Z}$ (condition), \qquad \, $(n_x^{\text{gr}}-1)/\text{dim}V , n_y^{\text{gr}}/\text{dim}V, n_z^{\text{gr}}/\text{dim}V \in \mathbb{Z}$

\subsection{Unit 2, Lesson 2 GPU Hardware and Parallel Communication Patterns}

\href{https://classroom.udacity.com/courses/cs344/lessons/77202674/concepts/773931440923}{Transpose Part 1}

Now
\[
\begin{gathered}
  \begin{aligned}
    & \text{Mat}_{\mathbb{F}}(n,n) \xrightarrow{T} \text{Mat}_{\mathbb{F}}(n,n) \\ 
    & A\mapsto A^T \text{ s.t. } (A^T)_{ij} = A_{ji}
    \end{aligned} \\ 
\begin{aligned}
  &  \text{Mat}_{\mathbb{F}} \xrightarrow{T} \mathbb{F}^{n^2} \\
  & A_{ij} \mapsto A_{ij} = A_{in + j }
  \end{aligned}
\end{gathered}
\]
\[
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=2em, column sep=3em, minimum width=1em]
  {
    \text{Mat}_{\mathbb{F}}(n,n) & \mathbb{F}^{n^2} \\
    \text{Mat}_{\mathbb{F}}(n,n) & \mathbb{F}^{n^2} \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] {$$} (m-1-2)
          edge node [left] {$T$} (m-2-1)
  (m-1-2) edge node [auto] {$T$} (m-2-2)
  (m-2-1) edge node [auto] {$$} (m-2-2)        
          ;
  \end{tikzpicture} \qquad \, \begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=2em, column sep=3em, minimum width=1em]
  {
A_{ij} & A_{in+j} \\ 
(A^T)_{ij} = A_{ji} & A_{jn+i} \\
  };
  \path[|->]
  (m-1-1) edge node [auto] {$$} (m-1-2)
          edge node [left] {$T$} (m-2-1)
  (m-1-2) edge node [auto] {$T$} (m-2-2)
  (m-2-1) edge node [auto] {$$} (m-2-2)        
          ;
  \end{tikzpicture}
\]

\href{https://classroom.udacity.com/courses/cs344/lessons/77202674/concepts/773153710923}{Transpose Part 2}

Possibly, transpose is a functor.

Consider struct as a category.  In this special case, $\text{Obj}\text{struct} = \lbrace \text{arrays} \rbrace$ (a struct of arrays).  Now this struct already has a hash table for indexing upon declaration (i.e. ``creation''): so this category struct will need to be equipped with a ``diagram'' from the category of indices $J$ to struct: $J\to $ struct.

So possibly
\[
\begin{aligned}
  \text{struct} & \xrightarrow{T} & \text{ array } \\ 
 \text{Obj}\text{Struct} = \lbrace \text{ arrays } \rbrace & \xrightarrow{T} & \text{Obj}\text{array} = \lbrace \text{ struct } \rbrace \\ 
 J\to \text{ struct } & \xrightarrow{T} & J \to \text{ array } 
  \end{aligned}
\]







\href{https://classroom.udacity.com/courses/cs344/lessons/77202674/concepts/787012800923}{Quiz: What Kind Of Communication Pattern}
This quiz made a few points that clarified the characteristics of these so-called communication patterns (amongst the memory?)

\begin{itemize}
  \item map is bijective, and map $:\text{Idx} \to \text{Idx}$
  \item gather - not necessarily surjective
  \item scatter - not necessarily surjective 
  \item stencil - surjective
  \item transpose (see before)
  \end{itemize}




\href{https://classroom.udacity.com/courses/cs344/lessons/77202674/concepts/773153720923}{Parallel Communication Patterns Recap}

\begin{itemize}
\item map - bijective
\item transpose - bijective
\item gather - not necessarily surjective, and is many-to-one (by def.)
\item scatter - one-to-many (by def.) and is not necessarily surjective
\item stencil - several-to-one (not injective, by definition), and is surjective
\item reduce - all-to-one
  \item scan/sort - all-to-all
\end{itemize}

\href{https://classroom.udacity.com/courses/cs344/lessons/77202674/concepts/773153760923}{Programmer View of the GPU}

thread blocks: group of threads that cooperate to solve a (sub)problem

\href{https://classroom.udacity.com/courses/cs344/lessons/77202674/concepts/773153770923}{Thread Blocks And GPU Hardware}

CUDA GPU is a bunch of SMs:

Streaming Multiprocessors (SM)s

SMs have a bunch of simple processors and memory.

Dr. Luebki:
\[
\boxed{ \begin{gathered}
    \text{Let me say that again because it's really important} \\
    \text{GPU is responsible for allocating blocks to SMs}
  \end{gathered}
  }
\]
Programmer only gives GPU a pile of blocks.

\href{https://classroom.udacity.com/courses/cs344/lessons/77202674/concepts/787721730923}{Quiz: What Can The Programmer Specify}

I myself thought this was a revelation and was not intuitive at first:

Given a single kernel that's launched on many thread blocks include $X$, $Y$, the programmer cannot specify the sequence the blocks, e.g. block $X$, block $Y$, run (same time, or run one after the other), and which SM the block will run on (GPU does all this).  

\href{https://classroom.udacity.com/courses/cs344/lessons/77202674/concepts/787981160923}{Quiz: A Thread Block Programming Example}

Open up \verb|hello blockIdx.cu| in Lesson 2 Code Snippets (I got the repository from github, repo name is cs344).

At first, I thought you can do a single file compile and run in Eclipse without creating a new project.  No.  cf. \href{http://stackoverflow.com/questions/17164197/eclipse-creating-projects-every-time-to-run-a-single-file}{Eclipse creating projects every time to run a single file?}.  

%I opened it up in Eclipse with File, Open File (no need to create a new project).

I ended up creating a new CUDA C/C$++$ project from File -> New project, and then chose project type Executable, Empty Project, making sure to include Toolchain CUDA Toolkit (my version is 7.5), and chose an arbitrary project name (I chose cs344single).  Then, as suggested by \href{http://stackoverflow.com/users/3720356/kenny-nguyen}{Kenny Nguyen}, I dragged and dropped files into the folder, from my file directory program.

I ran the program with the ``Play'' triangle button, clicking on the green triangle button, and it ran as expected.  I also turned off Build Automatically by deselecting the option (no checkmark).

\href{https://classroom.udacity.com/courses/cs344/lessons/77202674/concepts/773883100923}{GPU Memory Model}

\[
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=4em, column sep=5em, minimum width=2em]
  {
\text{ thread } & \text{ local memory } \\
};
  \path[->]
  (m-1-2) edge node [auto] {$\text{read}$} (m-1-1)
  (m-1-2) edge [loop right] node [right] {$\text{write}$} (m-1-2)
  ;
  \end{tikzpicture}
\]

Then consider threadblock $\equiv$  thread block \\
\phantom{Then consider } $\text{Obj}\text{threadblock} \supset \lbrace \text{ threads } \rbrace$ \\
\phantom{Then consider } $\text{FinSet} \xrightarrow{ \text{ threadIdx} } \text{ thread } \in \text{Mor}\text{threadblock}$

\[
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=4em, column sep=5em, minimum width=2em]
  {
\text{ threadblock } & \text{ shared memory } \\
};
  \path[->]
  (m-1-2) edge node [auto] {$\text{read}$} (m-1-1)
  (m-1-2) edge [loop right] node [right] {$\text{write}$} (m-1-2)
  ;
  \end{tikzpicture}
\]
$\forall \, $ thread,
\[
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=4em, column sep=5em, minimum width=2em]
  {
\text{ thread } & \text{ global memory } \\
};
  \path[->]
  (m-1-2) edge node [auto] {$\text{read}$} (m-1-1)
  (m-1-2) edge [loop right] node [right] {$\text{write}$} (m-1-2)
  ;
  \end{tikzpicture}
\]

\href{https://classroom.udacity.com/courses/cs344/lessons/77202674/concepts/773883130923}{Synchronization - Barrier}

Danger: what if a thread reads a result before another thread writes it?

Threads need to \emph{synchronize}.

one of the most fundamental problems in parallel computing

\href{https://classroom.udacity.com/courses/cs344/lessons/77202674/concepts/785776150923}{Quiz: The Need For Barriers}

3 barriers were needed (wasn't obvious to me at first).  All threads need to finish the write, or initialization, so it'll need a barrier.

While
\begin{lstlisting}
array[idx] = array[idx+1];
  \end{lstlisting}
is 1 line, it'll actually need 2 barriers; first read.  Then write.

So \emph{actually} we'll need to \emph{rewrite} this code:
\begin{lstlisting}
  int temp = array[idx+1];
  __syncthreads();
  array[idx] = temp;
  __syncthreads();
  \end{lstlisting}

Make sure each \emph{read} and \emph{write} operation is completed.  

kernels have implicit barrier for each.  

\href{https://classroom.udacity.com/courses/cs344/lessons/77202674/concepts/774332060923}{Writing Efficient Programs}

\begin{enumerate}
\item Maximize \emph{arithmetic intensity}
  arithmetic intensity $:= \frac{ \text{ math } }{ \text{ memory }}$
  \end{enumerate}

\href{https://classroom.udacity.com/courses/cs344/lessons/77202674/concepts/774332070923}{video: Minimize Time Spent On Memory}

local memory is fastest; global memory is slower

\[
\text{local} > \text{ shared} >> \text{global} >> \text{CPU}
\]

kernel we know (in the code) is tagged with \verb|__global__|




\subsubsection{Coalesce global memory accesses}

\href{https://classroom.udacity.com/courses/cs344/lessons/77202674/concepts/799933660923}{Coalesce Memory Access}

We saw such access pattern is coalesced; GPU must efficient when threads read or write contiguous memory locations.  


\href{https://classroom.udacity.com/courses/cs344/lessons/77202674/concepts/814086830923}{quiz: A Quiz on Coalescing Memory Access}

Work it out as Dr. Luebki did to figure out if it's coalesced memory access or not.  


\href{https://classroom.udacity.com/courses/cs344/lessons/77202674/concepts/774332150923}{Atomic Memory Operations}

Atomic Memory Operations

atomicadd atomicmin atomicXOR atomicCAS Compare And Swap


\subsubsection{On Problem Set 2}

There is what I call the ``naive global memory'' scheme, that solves the objective of blurring a photo with a local stencil of the values, using only global memory on the GPU.

Given image of size $L_x \times L_y$, i.e. $(L_x,L_y) \in (\mathbb{Z}^+)^2$; image is really a designated or particular mapping $f$,
\[
\begin{aligned}
  & f: \lbrace 0 \dots L_x - 1 \rbrace \times \lbrace 0 \dots L_y - 1 \rbrace \to \lbrace 0 \dots 255 \rbrace^4 \\ 
  & f(x,y) = (f^{(r)}(x,y) , f^{(b)}(x,y), f^{(g)}(x,y), f^{(\alpha)}(x,y) )
\end{aligned}
\]
Consider ``naive global memory scheme'' - establishing the following notation:
\[
\begin{gathered}
  \begin{aligned}
    & i_x \in \lbrace 0 \dots M_x - 1 \rbrace \Longleftrightarrow \verb|threadIdx.x| \\
    & i_y \in \lbrace 0 \dots M_y - 1 \rbrace \Longleftrightarrow \verb|threadIdx.y|  \\
    & j_x \in \lbrace 0 \dots N_x - 1 \rbrace \Longleftrightarrow \verb|blockIdx.x|  \\
    & j_y \in \lbrace 0 \dots N_y - 1 \rbrace \Longleftrightarrow \verb|blockIdx.y|  
  \end{aligned} \\
  \begin{aligned}
    & M_x \in \lbrace 1 \dots 1024 \rbrace \Longleftrightarrow \verb|blockDim.x| \\ 
    & M_y \in \lbrace 1 \dots 1024 \rbrace \Longleftrightarrow \verb|blockDim.y| 
    \end{aligned}
\end{gathered}
\]
with
\[
\begin{aligned}
  & N_x := (L_x + M_x - 1)/M_x \in \mathbb{Z}^+ \\ 
  & N_y := (L_y + M_y - 1)/M_y \in \mathbb{Z}^+ 
  \end{aligned}
\]

There should be a functor called ``flatten'' such that we end up with the image as a 1-dimensional, contiguous array on the global memory of the GPU; so for
\[
k = k_x + k_y L_x \in \lbrace 0 \dots L_x L_y - 1 \rbrace
\]
then
\[
\begin{gathered}
  (k_x,k_y) \Longleftrightarrow (x,y) \in \lbrace 0 \dots L_x -1 \rbrace \times \lbrace 0 \dots L_y - 1 \rbrace \xmapsto{ \text{ flatten }} k \in \lbrace 0 \dots L_xL_y -1 \rbrace \\
  f:\lbrace 0 \dots L_x -1 \rbrace \times \lbrace 0 \dots L_y - 1 \rbrace \xrightarrow{ \text{ flatten }} f: \lbrace 0 \dots L_x L_y - 1 \rbrace \to \lbrace 0 \dots 255 \rbrace^4  \\
  f(x,y) = f(k_x,k_y) \xmapsto{ \text{ flatten } } f(k) 
\end{gathered}
\]

Then there should be a functor called ``separateChannels'' to represent the \verb|__global__| kernel \verb|separateChannels|.
\[
\begin{gathered}
  f: \lbrace 0 \dots L_x L_y - 1 \rbrace \to \lbrace 0 \dots 255 \rbrace^4 \xrightarrow{ \text{ separateChannels } } f^{(c)}:\lbrace 0 \dots L_xL_y -1 \rbrace \to \lbrace 0 \dots 255 \rbrace , \, c = \lbrace r,g,b \rbrace \\ 
 f(k) \xmapsto{ \text{ separateChannels } } f^{(c)}(k)
\end{gathered}
\]

Then consider a ``stencil'' of size \verb|filterWidth| $\times$ \verb|filterWidth| $\Longleftrightarrow W \times W \in (\mathbb{Z}^+)^2$.

Let $(\nu_x,\nu_y) \in \lbrace 0 \dots W-1 \rbrace^2$ and so
\[
\left( \nu_x - \frac{W}{2} , \nu_y - \frac{W}{2} \right) \in \lbrace \frac{-W}{2} , \dots \frac{W}{2} - 1 \rbrace \subset \mathbb{Z}
\]

Now let
\[
\begin{aligned}
  &  k_x^{\text{st}} = k_x  + \nu_x - \frac{W}{2} \Longleftrightarrow \verb|stencilindex_x| \\ 
  & k_y^{\text{st}} = k_y  + \nu_y - \frac{W}{2} \Longleftrightarrow \verb|stencilindex_y| 
\end{aligned}
\]
with $\begin{aligned} & \quad \\
 &  k_x^{\text{st}} \in \lbrace 0 \dots L_x -1 \rbrace \\ 
  &  k_y^{\text{st}} \in \lbrace 0 \dots L_y -1 \rbrace \\
  \end{aligned}$

We also have to apply the flatten functor on the stencil:
\[
\begin{gathered}
  (\nu_x,\nu_y) \in \lbrace 0 \dots W-1\rbrace^2 \xmapsto{ \text{ flatten } } \nu = \nu_x + W \nu_y \in \lbrace 0 \dots W^2 -1 \rbrace
  \end{gathered}
\]

And so the gist of the blurring operation is in this equation:
\begin{equation}
\begin{gathered}
  g^{(c)}(k) = \sum_{\nu_x=0}^{W-1} \sum_{\nu_y =0}^{W-1} c_{\nu = \nu_x + W\nu_y} f^{(c)}(k_x^{\text{st}} + L_x \cdot k_y^{\text{st}} ) \qquad \, \forall \, c = \lbrace r,g,b\rbrace 
  \end{gathered}
  \end{equation}
with $\begin{aligned} & \quad \\
  & k_x^{\text{st}} = k_x^{\text{st}}(\nu_x) := k_x + \nu_x - \frac{W}{2} \\ 
  & k_y^{\text{st}} = k_y^{\text{st}}(\nu_y) := k_y + \nu_y - \frac{W}{2} 
  \end{aligned}$

\subsubsection{Problem Set 2, shared memory ``tiling'' scheme}

I think the \verb|__shared__| memory ``tiling'' scheme is non-trivial due to accounting for the values ``at the edges'' of the thread block, including the ``corners'' the so-called ``halo'' cells.  Storing the value of the ``cells'' or threads within a thread block into shared memory is \emph{relatively} straightforward - it is a 1-to-1 mapping.  But taking care of the corner cases, due to the desired ``stencil'' for blurring, is nontrivial, I think.

Consider my scheme for ``tiling'' using shared memory:

Let
\[
\begin{gathered}
  \begin{aligned}
    & k_x = i_x + j_x M_x \in \lbrace 0 \dots L_x -1 \rbrace \\ 
    & k_y = i_y + j_y M_y \in \lbrace 0 \dots L_y -1 \rbrace 
    \end{aligned} \\
  k_x < L_x \text{ and } k_y < L_y \\
  0 \leq k_x < L_x \text{ and } 0 \leq k_y < L_y \\
  k := k_x + L_x k_y
\end{gathered}
\]
and let
\[
\begin{gathered}
  \begin{aligned}
    & S_x := M_x + 2r \\ 
    & S_y := M_y + 2r \\
    \end{aligned} \\
  \begin{aligned}
    & s_x := i_x + r \\ 
    & s_y := i_y + r
  \end{aligned} \\
  0 \leq s_x < S_x \text{ and } 0 \leq s_y < S_y \\
  s_k := s_x + S_x s_y 
\end{gathered}
\]
where $r$ is the ``radius'' or essentially the stencil size, out in 1-direction.

Loading the regular cells,
\[
s_{\text{in}}[s_k] = f^{(c)}(k)
\]

Loading the halo cells, \\

if ($i_x < r$),

then requiring
\[
\begin{gathered}
  \begin{aligned}
    & 0 \leq s_x -r < S_x \\ 
    & 0 \leq s_y < S_y
  \end{aligned} \quad \quad \,
  \begin{aligned}
    & 0 \leq k_x - r < L_x \\ 
    & 0 \leq k_y < L_y
    \end{aligned} \\
  s_{\text{in}}[s_x-r + S_xs_y] = f^{(c)}[k_x-r + L_x k_y] \\ 
   \begin{aligned}
    & 0 \leq s_x +M_x < S_x \\ 
    & 0 \leq s_y < S_y
  \end{aligned} \quad \quad \,
  \begin{aligned}
    & 0 \leq k_x + M_x < L_x \\ 
    & 0 \leq k_y < L_y
  \end{aligned} \\
  s_{\text{in}}[s_x + M_x + S_xs_y] = f^{(c)}[k_x + M_x + L_xk_y]
\end{gathered}
\]

If ($i_y < r$),

then requiring
\[
\begin{gathered}
  \begin{aligned}
    & 0 \leq s_x  < S_x \\ 
    & 0 \leq s_y -r < S_y
  \end{aligned} \quad \quad \,
  \begin{aligned}
    & 0 \leq k_x  < L_x \\ 
    & 0 \leq k_y -r < L_y
    \end{aligned} \\
  s_{\text{in}}[s_x + S_x(s_y-r)] = f^{(c)}[k_x + L_x (k_y-r)] \\ 
   \begin{aligned}
    & 0 \leq s_x  < S_x \\ 
    & 0 \leq s_y +M_y < S_y
  \end{aligned} \quad \quad \,
  \begin{aligned}
    & 0 \leq k_x  < L_x \\ 
    & 0 \leq k_y +M_y< L_y
  \end{aligned} \\
  s_{\text{in}}[s_x +  S_x(s_y+M_y)] = f^{(c)}[k_x  + L_x(k_y+M_y)]
\end{gathered}
\]

And now the actual stencil calculation:

$\forall \, \nu_y \in \lbrace \nu_y = 0 , 1 \dots W-1 | 0 \leq \nu_y < W \rbrace$, \\
\phantom{ \qquad \, } $k_y^{\text{st}} := s_y + \nu_y - r$ \\
\phantom{ \qquad \, } $\forall \, \nu_x \in \lbrace \nu_x = 0 , 1 \dots W-1 | 0 \leq \nu_x < W \rbrace$, \\
\phantom{ \qquad \qquad \, } $k_x^{\text{st}} := s_x + \nu_x -r $ \\
\phantom{ \qquad \qquad \, } \verb|inputvalue| $=s_{\text{in}}[k^{\text{st}}_x + S_x k_y^{\text{st}}]$ with $\begin{aligned} & \quad \\
  & 0 \leq k_x^{\text{st}} < S_x \\ 
  & 0 \leq k_y^{\text{st}} < S_y  \end{aligned}$ \\
\phantom{ \qquad \qquad \, } \verb|filtervalue| $ = c(\nu_x + W\nu_y)$  \\
\phantom{ \qquad \qquad \, } \verb|value| $+=$ \verb|filtervalue| $\cdot$ \verb|inputvalue|,

i.e.
\[
g^{(c)}(k) = \sum_{\nu_y=0}^{W-1} \sum_{\nu_x =0}^{W-1} c_{\nu = \nu_x + W \nu_y} s_{\text{in}}[k_x^{\text{st}} + k_y^{\text{st}}S_x]
\]


Unfortunately, the (literal) corner cases aren't accounted for correctly, (when $i_x < r$ \emph{and } $i_y<r$), as can be seen by the difference image and output image when it's run.

\href{https://discussions.udacity.com/users/Samuel271828}{Samuel Lin or Samuel271828} had both an elegant and \emph{correct} implementation.  It's also in \href{https://github.com/samuellin3310}{Samuel Lin or samuellin3310's github repositories}, in \href{https://github.com/samuellin3310/ro-to-Parallel-Programming_set2/blob/master/student_fuction_improved_share.cu}{student fuction improved share.cu (sic)}.  

%\href{https://github.com/samuellin3310/ro-to-Parallel-Programming_set2/blob/master/student_fuction\improved\share.cu}{}.  

Here it is, mathematically:

Let
\[
\begin{gathered}
  s_{\text{in}} \in \mathbb{R}^{(M_x + 2r)(M_y + 2r) } \\
  \begin{aligned}
    & k_x = i_x + j_x M_x \in \mathbb{Z} \\ 
    & k_y = i_y + j_y M_y \in \mathbb{Z}  
    \end{aligned} \\
  k_x < L_x \text{ and } k_y < L_y \\
  0 \leq k_x < L_x \text{ and } 0 \leq k_y < L_y \\
  k := k_x + L_x k_y
\end{gathered}
\]

Then,

$\forall \, i \in \lbrace i = i_x -r, i_x - r +M_x, i_x -r + 2M_x, \dots | i_x - r \leq i < M_x + r \rbrace$, \\
\phantom{ \qquad \, } $\forall \, j \in \lbrace j =i_y - r , i_y - r + M_y, i_y -r + 2M_y \dots | i_y - r \leq j < M_y + r \rbrace$,
\[
\begin{gathered}
  \begin{aligned}
    & l_x := i + M_x j_x \in \mathbb{Z} \text{ with (enforcing) } 0 \leq l_x < L_x \\ 
    & l_y := i + M_y j_y \in \mathbb{Z} \text{ with (enforcing) } 0 \leq l_y < L_y \\ 
    & s_{\text{in}}[i+r + (j+r)(M_x + 2r) ] = f^{(c)}(l_x + l_yL_x)
    \end{aligned}
\end{gathered}
\]
Enforce $k_x <L_x$ and $k_y <L_y$, otherwise nothing happens.  

And now the actual stencil calculation:

$\forall \, \nu_y \in \lbrace \nu_y = 0 , 1 \dots W-1 | 0 \leq \nu_y < W \rbrace$, \\
\phantom{ \qquad \, } $k_y^{\text{st}} := s_y + \nu_y - r$ \\
\phantom{ \qquad \, } $\forall \, \nu_x \in \lbrace \nu_x = 0 , 1 \dots W-1 | 0 \leq \nu_x < W \rbrace$, \\
\phantom{ \qquad \qquad \, } $k_x^{\text{st}} := s_x + \nu_x -r $ \\
\phantom{ \qquad \qquad \, } \verb|inputvalue| $=s_{\text{in}}[k^{\text{st}}_x + S_x k_y^{\text{st}}]$ with $\begin{aligned} & \quad \\
  & 0 \leq k_x^{\text{st}} < S_x \\ 
  & 0 \leq k_y^{\text{st}} < S_y  \end{aligned}$ \\
\phantom{ \qquad \qquad \, } \verb|filtervalue| $ = c(\nu_x + W\nu_y)$  \\
\phantom{ \qquad \qquad \, } \verb|value| $+=$ \verb|filtervalue| $\cdot$ \verb|inputvalue|,

i.e.
\[
g^{(c)}(k) = \sum_{\nu_y=0}^{W-1} \sum_{\nu_x =0}^{W-1} c_{\nu = \nu_x + W \nu_y} s_{\text{in}}[k_x^{\text{st}} + k_y^{\text{st}}S_x]
\]

\section{Pointers in C; Pointers in C categorified (interpreted in Category Theory)}

Suppose $v\in \text{ObjData}$, category of data \textbf{Data}, \\
\phantom{ Suppose} e.g. $v\in \text{Int} \in \text{Obj}\mathbf{\text{Type}}$, category of types $\mathbf{\text{Type}}$.

\[
\begin{aligned}
  & \text{Data}  \xrightarrow{ \& } \text{Memory}  \\
  & v \overset{\&}{\mapsto} \& v 
\end{aligned}
\]
with address $\& v \in $ Memory.

With \\
\phantom{With } assignment $pv = \& v$,
\[
\begin{aligned}
  & pv \in \text{Obj}\text{pointer}, \, \text{ category of pointers, pointer} \\ 
  & pv \in \text{Memory} \qquad \, (\text{i.e. not $pv \in \text{Dat}$, i.e. $pv \notin \text{Dat}$})
\end{aligned}
\]

\[
\text{ pointer } \ni pv \overset{ * }{ \mapsto } *pv \in \text{Dat}
\]

\[
\begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=4em, column sep=5em, minimum width=2em]
  {
    v & \& v \\
    *pv & pv \\
};
  \path[|->]
  (m-1-1) edge node [auto] {$\&$} (m-1-2)
  (m-1-2) edge node [right] {$=$} (m-2-2)
  (m-2-2) edge node [auto] {$*$} (m-2-1)
  (m-2-1) edge node [left] {$==$} (m-1-1)
  ;
  \end{tikzpicture}
\qquad \, \begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=4em, column sep=5em, minimum width=2em]
  {
    \text{Data} & \text{Memory} \\
    \text{Data} & \text{pointer} \\
};
  \path[->]
  (m-1-1) edge node [auto] {$\&$} (m-1-2)
  (m-1-2) edge node [right] {$=$} (m-2-2)
  (m-2-2) edge node [auto] {$*$} (m-2-1)
  (m-2-1) edge node [left] {$==$} (m-1-1)
  ;
  \end{tikzpicture}
\]

Examples.  Consider \verb|passfunction.c| in Fitzpatrick \cite{Fitz}.

Consider the type \verb|double|, \verb|double| $\in \text{Obj}\text{Types}$.  \\
\phantom{ Consider } $\text{fun1, fun2} \in \text{Mor}\text{Types} \qquad \, \text{ namely }$ \\
\phantom{ Consider } $\text{fun1, fun2} \in \text{Hom}(\text{double},\text{double}) \equiv \text{Hom}_{\text{Types}}(\text{double},\text{double})$

Recall that
\[
\begin{aligned}
  & \text{ pointer } \xrightarrow{ * } \text{ Dat } \\ 
  & \text{ pointer } \xrightarrow{ \& } \text{ Memory }
\end{aligned}
\]
$*, \&$ are functors with domain on the category pointer.

Pointers to functions is the ``extension'' of functor $*$ to the codomain of $\text{Mor}\text{Types}$:

\[
\begin{aligned}
  & \text{ pointer} & \xrightarrow{ * } \text{Mor}\text{Types} \\ 
  & \text{ fun1 } & \overset{*}{ \mapsto } *\text{fun}1 \in \text{Hom}_{\text{Types}}(\text{double},\text{double})
  \end{aligned}
\]

\[
 \begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=4em, column sep=5em, minimum width=2em]
  {
    \text{double} & \text{Memory} \\
    \text{double} & \text{pointer} \\
    \text{double} & \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] {$\&$} (m-1-2)
  (m-1-2) edge node [right] {$\cong$} (m-2-2)
  (m-2-2) edge node [auto] {$*$} (m-2-1)
  edge node [auto] {$*$} (m-3-1)
  (m-2-1) edge node [left] {$\text{cube}$} (m-3-1)
  ;
 \end{tikzpicture} \qquad \qquad \,
  \begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=4em, column sep=5em, minimum width=2em]
  {
    \text{res1} & \&\text{res1} \\
    *\text{res1} & \text{res1} \\
    *\text{res1} = y^3 & \\ 
  };
  \path[|->]
  (m-1-1) edge node [auto] {$\&$} (m-1-2)
  (m-1-2) edge node [right] {$\cong$} (m-2-2)
  (m-2-2) edge node [auto] {$*$} (m-2-1)
  edge node [auto] {$*$} (m-3-1)
  (m-2-1) edge node [left] {$\text{cube}$} (m-3-1)
  ;
  \end{tikzpicture}
\]

It's unclear to me how \verb|void cube| can be represented in terms of category theory, as surely it cannot be represented as a mapping (it acts upon a functor, namely the $*$ functor for pointers).  It doesn't return a value, and so one cannot be confident to say there's explicitly a domain and codomain, or range for that matter.

But what is going on is that
\[
\begin{gathered}
  \text{ pointer }, \text{ double } , \text{ pointer } \xrightarrow{ \text{ cube } } \text{ pointer }, \text{ pointer } \\ 
  \text{fun}1, x , \text{res}1 \overset{\text{cube}}{\mapsto} \text{fun}1, \text{res}1
\end{gathered}
\]
s.t. $*\text{res}1 = y^3=(*\text{fun}1(x))^3$


So I'll speculate that in this case, \verb|cube| is a functor, and in particular, is acting on $*$, the so-called deferencing operator:
\[
\begin{gathered}
  \text{ pointer } \xrightarrow{ * } \text{float} \in \text{Data} \\
  \text{ res}1 \overset{*}{\mapsto} *\text{res}1
\end{gathered} \xrightarrow{ \text{ cube } } \begin{gathered}
  \text{ pointer } \xrightarrow{ \text{cube}(*) } \text{float} \in \text{Data} \\
  \text{ res}1 \overset{\text{cube}(*)}{\mapsto} \text{cube}(*\text{res}1)=y^3
\end{gathered}
\]

cf.  Arrays, from Fitzpatrick \cite{Fitz}

\[
\text{Types} \xrightarrow{ \text{ declaration } } \text{arrays}
\]
If $x\in \text{Obj}\text{arrays}$,
\[
\& x[0] \in \text{Memory} \xrightarrow{ == } x \in \text{ pointer } (\text{to 1st element of array})
\]


cf. Section 2.13 Character Strings from Fitzpatrick \cite{Fitz}

\begin{lstlisting}
  char word[20] = ``four''
  char *word = ``four''
\end{lstlisting}

cf. C$++$ extensions for C according to Fitzpatrick \cite{Fitz}
\begin{itemize}
\item simplified syntax to pass by reference pointers into functions
\item inline functions
\item variable size arrays \begin{lstlisting}
  int n;
  double x[n];
  \end{lstlisting}
\item complex number class
\end{itemize}


\part{C++ and Computational Physics}

cf. 2.1.1 Scientific hello world from Hjorth-Jensen (2015) \cite{Hjor2015}

in C, 
\begin{lstlisting}
  int main (int argc, char* argv[])
\end{lstlisting}
\verb|argc| stands for number of command-line arguments \\
\verb|argv| is vector of strings containing the command-line arguments with \\
\phantom{argv} \verb|argv[0]| containing name of program \\
\phantom{argv} \verb|argv[1] , argv[2], ... | are command-line args, i.e. the number of lines of input to the program

``To obtain an executable file for a C++ program'' (i.e. compile (???)), 
\begin{lstlisting}
  gcc -c -Wall myprogram.c
  gcc -o myprogram myprogram.o
\end{lstlisting}
\verb|-Wall| means warning is issued in case of non-standard language \\
\verb|-c| means compilation only \\
\verb|-o| links produced object file \verb|myprogram.o| and produces executable \verb|myprogram|

\subsubsection{Create \verb|makefile|}

\begin{lstlisting}
  # General makefile for c - choose PROG =  name of given program
  
  # Here we define compiler option, libraries and the target
  CC= c++ -Wall
  PROG= myprogram

  # Here we make the executable file
  ${PROG} :          ${PROG}.o
                     ${CC} ${PROG}.o -o ${PROG}

  # whereas here we create the object file

  #{PROG}.o :        ${PROG}.cpp
                     ${CC} -c ${PROG}.cpp
\end{lstlisting}

Here's what worked for me:
\begin{lstlisting}
CC= g++ -Wall
PROG= program1

# Here we make the executable file
${PROG} :          ${PROG}.o
        ${CC} ${PROG}.o -o ${PROG}

# whereas here we create the object file

${PROG}.o :        ${PROG}.cpp
        ${CC} -c ${PROG}.cpp

# EY : 20160602notice the different suffixes, and we see the pattern for the syntax

# (note: the <tab> in the command line is necessary formake towork)
# target: dependency1 dependency2 ...
#         <tab> command
\end{lstlisting}


cf. 2.3.2 Machine numbers of Hjorth-Jensen (2015) \cite{Hjor2015}


cf. 2.5.2 Pointers and arrays in C++ of Hjorth-Jensen (2015) \cite{Hjor2015}

Initialization (diagram):
\[
  \begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=1.4em, column sep=4em, minimum width=1em]
  {
    \& \text{var} = \verb|0x7ffc97efbd8c| & \text{pointer} = \& \text{var} = \verb|0x7ffc97efbd8c| \\
     \text{Memory} & \text{pointer} \\
    \text{ (memory) addresses } & \text{Obj}(\text{pointer}) \\
  };
  \path[|->]
  (m-1-1) edge node [auto] {$=$} (m-1-2)
  ;
  \path[->]
  (m-2-1) edge node [auto] {$=$} (m-2-2)
  (m-3-1) edge node [auto] {$=$} (m-3-2)
  ;
  \end{tikzpicture}
\]

Referencing and deferencing operations on pointers to variables
\[
\begin{aligned}
  \begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{var} & \& \text{var} = \verb|0x7ffc97egbd8c| \\
    \text{var}, 421 & \text{True} \\
    421 &  \\
  };
  \path[|->]
  (m-1-1) edge node [auto] {$=$} (m-1-2)
  (m-2-1) edge node [auto] {$$} (m-1-1)
  edge node [auto] {$==$} (m-2-2)
  edge node [auto] {$$} (m-3-1)
  ;
  \end{tikzpicture}
    & 
  \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{pointer} = \verb|0x7ffc97egbd8c| & 421 & \text{int} \\
    \& \text{pointer} = \verb|0x7ffc97egbd8c| & & \\ 
  };
  \path[|->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  edge node [auto] {\&} (m-2-1)
  (m-1-2) edge node [auto] {\text{typedef}} (m-1-3)
;  
  \end{tikzpicture}   \\
    \begin{tikzpicture}
  \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{Types} &  \textbf{Memory}  \\
    \textbf{Types} \otimes \mathbf{\textbf{Dat}} & \text{Boolean} = \lbrace \text{True}, \text{False} \rbrace  \\
    \mathbf{\textbf{Dat}} &  \\
  };
  \path[->]
  (m-1-1) edge node [auto] {$=$} (m-1-2)
  (m-2-1) edge node [auto] {$$} (m-1-1)
  edge node [auto] {$==$} (m-2-2)
  edge node [auto] {$$} (m-3-1)
  ;
  \end{tikzpicture}
    & 
  \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{pointer}  & \mathbf{\textbf{Dat}} & \mathbf{\textbf{Types}} \\
    \mathbf{\textbf{Memory}} & & \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  edge node [auto] {\&} (m-2-1)
  (m-1-2) edge node [auto] {\text{typedef}} (m-1-3)
;  
  \end{tikzpicture}   
  \end{aligned}
\]

\subsection{Numerical differentiation and interpolation (in C++)}

cf. Chapter 3 ``Numerical differentiation and interpolation'' of Hjorth-Jensen (2015) \cite{Hjor2015}.

This is how I understand it.

Consider the Taylor expansion for $f(x) \in C^{\infty}(\mathbb{R})$:
\[
f(x) = f(x_0)  + \sum_{j=1}^{\infty} \frac{ f^{(j)}(x_0)}{j!} h^j 
\]
For $x = x_0 \pm h$,
\[
\begin{gathered}
  f(x) = f(x_0 \pm h ) = f(x_0) + \sum_{j=1}^{\infty} \frac{ f^{(2j)}(x_0) }{(2j)!} h^{2j} \pm \sum_{j=1}^{\infty} \frac{ f^{(2j-1)}(x_0) }{(2j-1)!} h^{2j-1}
\end{gathered}
\]
Then
\[
\begin{gathered}
  f(x_0 + 2^kh) - f(x_0 - 2^kh) = 2\sum_{j=1}^{\infty} \frac{ f^{(2j-1)}}{(2j-1)!}(x_0) 2^{k(2j-1)} h^{2j-1} = \\
  = 2\left[ f^{(1)}(x_0) 2^k h + \sum_{j=2}^{\infty} \frac{ f^{(2j-1)}(x_0)}{ (2j-1)!} 2^{k(2j-1)} h^{2j-1} \right] =  \\
  = 2 \left[ f^{(1)}(x_0) 2^k h + \frac{ f^{(3)}(x_0)}{3!} 2^{k(3)} h^3 + \sum_{j=3}^{\infty} \frac{ f^{(2j-1)}(x_0) }{(2j-1)!} 2^{k(2j-1) } h^{2j-1} \right]
\end{gathered}
\]

So for $k=1$,
\[
f(x_0 + h ) - f(x_0 - h ) = 2 \left[ f^{(1)}(x_0)h + \sum_{j=1}^{\infty} \frac{ f^{(2j+1)}(x_0) }{ (2j+1)!} h^{2j+1} \right]
\]

Now
\[
\begin{gathered}
  f(x_0 + 2^kh) + f(x_0 - 2^kh) - 2f(x_0) = \\
  = 2 \sum_{j=1}^{\infty} \frac{ f^{(2j)}(x_0) }{(2j)!} 2^{2jk} h^{2j} = \\
  = 2 \left[ \frac{ f^{(2)}(x_0)}{2} 2^{2k} h^2 + \sum_{j=2}^{\infty} \frac{ f^{(2j)}(x_0) }{ (2j)!} 2^{2jk} h^{2j} \right] = \\
  = 2 \left[ \frac{ f^{(2)}(x_0) }{2} 2^{2k} h^2 + \frac{ f^{(4)}(x_0)}{4!} 2^{4k} h^4 + \sum_{j=3}^{\infty} \frac{ f^{(2j)}(x_0) }{ (2j)!} 2^{2jk} h^{2j} \right]
  \end{gathered}
\]
Thus for the case of $k=1$,
\[
f(x_0 + h )  + f(x_0 - h) - 2f(x_0) = f^{(2)}(x_0)h^2 + 2 \sum_{j=2}^{\infty} \frac{ f^{(2j)}(x_0)}{(2j)!} h^{2j}
\]
\[
\begin{aligned}
  & \frac{ f(x_0 +h) - f(x_0-h) }{ 2h} = f^{(1)}(x_0) + \sum_{j=1}^{\infty} \frac{ f^{(2j+1)}(x_0) }{(2j+1)!}h^{2j} \\ 
  & \frac{ f(x_0 + h) + f(x_0 - h) - 2f(x_0) }{h^2} = f^{(2)}(x_0) + 2\sum_{j=2}^{\infty} \frac{ f^{(2(j+1) ) }(x_0)}{(2(j+1))!} h^{2j}
  \end{aligned}
\]

A pattern now emerges on how to include more calculations at points $x_0, x_0 \pm 2^kh$ so to obtain better accuracy $O(h^l)$.  For instance,

Given 5 pts. $\lbrace x_0, x_0 \pm h, x_0 \pm 2h \rbrace$,
\[
\begin{gathered}
  f(x_0 + 2h) - f(x_0 -2h) = 2[ f^{(1)}(x_0) 2^1h + \frac{ f^{(3)}(x_0)}{3!} 2^3 h^3 + O(h^5) ] \\ 
  f(x_0+ h) - f(x_0 - h) = 2[ f^{(1)}(x_0) h + \frac{ f^{(3)}(x_0)}{3!} h^3 + O(h^5)] \\
  \Longrightarrow f'(x_0) = \frac{ f(x_0 - 2h) - 8f(x_0-h ) + 8f(x_0 + h) - f(x_0 + 2h) }{ 12h } + O(h^4)
\end{gathered}
\]

Hjorth-Jensen (2015) \cite{Hjor2015} argues, on pp. 46-47, that the additional evaluations are time consuming, to obtain further accuracy, so it's a balance.

To summarize, for $O(h^2)$ accuracy,
\[
\begin{aligned}
  & \frac{ f(x_0 + h) - f(x_0-h) }{2h} = f^{(1)}(x_0) + \sum_{j=1}^{\infty} \frac{ f^{(2j+1)}(x_0 )}{(2j+1)!} h^{2j} & \qquad \, O(h^2) \\ 
  & \frac{ f(x_0 + h) + f(x_0 -h) - 2f(x_0) }{h^2} = f^{(2)}(x_0) + 2\sum_{j=1}^{\infty} \frac{ f^{(2j+2)}(x_0) }{ (2j+2)!} h^{2j} & \qquad \, O(h^2)
  \end{aligned}
\]

\section{Interpolation}

cf. 3.2 Numerical Interpolation and Extrapolation of Hjorth-Jensen (2015) \cite{Hjor2015}

Given $N+1$ pts. $\begin{aligned} & \quad \\
  y_0 & = f(x_0) \\
  y_1 & = f(x_1) \\
  & \vdots \\
  y_N & = f(x_N) \end{aligned}$, $x_i$'s distinct (none of $x_i$ values equal)

We want a polynomial of degree $n$ s.t. $p(x)  \in \mathbb{R}[x]$

\[
p(x_i) = f(x_i) = y_i \qquad \, i = 0,1\dots N
\]

\[
p(x) = a_0 + a_1(x-x_0) + \dots + a_i \prod_{j=0}^{i-1}(x-x_j) + \dots + a_N(x-x_0) \dots (x-x_{N-1}) = a_0 + \sum_{i=1}^N a_i \prod_{j=0}^{i-1}(x-x_j)
\]
\[
\begin{aligned}
  & a_0 = f(x_0) \\ 
  & a_0 + a_1(x_1-x_0) = f(x_1) \\
  & \vdots \\
  & a_0 + \sum_{i=1}^k a_i \prod_{j=0}^{i-1} (x_k - x_j) = f(x_k)
  \end{aligned}
\]

Hjorth-Jensen (2015) \cite{Hjor2015} mentions this Lagrange interpolation formula (I haven't found a good proof for it).  
\begin{equation}
\boxed{ p_N(x) = \sum_{i=0}^N \prod_{k\neq i} \frac{ x-x_k}{x_i - x_k} y_i  }
\end{equation}

\section{Classes (C++)}

cf. \href{http://stackoverflow.com/questions/6377786/c-operator-overloading-in-expression}{C++ Operator Overloading in expression}

Take a look at this link: \href{http://stackoverflow.com/questions/6377786/c-operator-overloading-in-expression}{C++ Operator Overloading in expression}.  This point isn't emphasized enough, as in Hjorth-Jensen (2015) \cite{Hjor2015}.  This makes doing something like
\[
d = a*c + d/b
\]
work the way we expect.  Kudos to user \href{http://stackoverflow.com/users/252000/fredoverflow}{fredoverflow} for his answer:

``The expression \verb|(e_x*u_c)| is an rvalue, and references to non-const won't bind to rvalues.

Also, member functions should be marked \verb|const| as well.''  

\subsection{What are lvalues and rvalues in C and C++?}

\href{http://thbecker.net/articles/rvalue_references/section_01.html}{C++ Rvalue References Explained}

Original definition of \emph{lvalues} and \emph{rvalues} from \emph{C}: \\
\emph{lvalue} - expression $e$ that may appear on the left or on the right hand side of an assignment \\
\emph{rvalue} - expression that can only appear on right hand side of assignment $=$.

Examples:

\begin{lstlisting}
  int a = 42;
  int b = 43;

  // a and b are both l-values
  a = b; // ok
  b = a; // ok
  a = a * b; // ok

  // a * b is an rvalue:
  int c = a * b; // ok, rvalue on right hand side of assignment
  a * b = 42; // error, rvalue on left hand side of assignment
  
  \end{lstlisting}

In \emph{C++}, this is still useful as a first, intuitive approach, but \\
\emph{lvalue} - expression that refers to a memory location and allows us to take the address of that memory location via the $\&$ operator. \\
\emph{rvalue} - expression that's not a lvalue

So $\&$ reference \emph{functor} can't act on rvalue's.



\section{Numerical Integration}

\subsubsection{Trapezoid rule (or trapezoidal rule)}

See \href{https://github.com/ernestyalumni/CompPhys/blob/master/Cpp/Integrate.ipynb}{Integrate.ipynb}.

From there, consider integration on $[a,b]$, considering $h := \frac{b-a}{N}$, and $N+1$ (grid) points, $\lbrace a, a+h, a+2h, \dots , a+ jh, \dots , a+Nh = b\rbrace_{j=0 \dots N }$.

Then $\frac{N}{2}$ pts. are our ``$x_0$''; $x_0$'s $= \lbrace a +h , a+3h, \dots , a+(2j-1)h, \dots , a+ \left( \frac{2 N}{2} - 1 \right)h \rbrace_{j=1 \dots \frac{N}{2} }$.

Notice how we really need to care about if $N$ is even or not.  If $N$ is not even, then we'd have to deal with the integration at the integration limits and choosing what to do.  

Then
\[
\begin{gathered}
  \int_a^b f(x) dx = \sum_{j=1}^{N/2} \int_{a + (2j-1)h-h}^{a+(2j-1)h+h} f(x)dx = \sum_{j=1}^{N/2} \frac{h}{2} ( 2f(a+(2j-1)h ) + f(a+2(j-1)h) + f(a+2jh) ) = \\
   = h(f(a)/2 + f(a+h) + \dots + f(b-h) + \frac{f(b)}{2} ) = h \left( \frac{f(a)}{2} + \sum_{j=1}^{N-1} f(a+jh) + \frac{f(b)}{2} \right)
  \end{gathered}
\]

\subsubsection{Midpoint method or rectangle method}.

Let $h := \frac{b-a}{N}$ be the step size.  The grid is as follows:
\[
\lbrace a , a +h , \dots , a + jh , \dots , a+Nh = b\rbrace_{j=0\dots N}
\]
The desired midpoint values are at the following $N$ points:
\[
\lbrace a + \frac{h}{2} , a + \frac{3}{2} h , \dots , a+\frac{(2j-1 ) h }{2} , \dots , a + \left( N - \frac{1}{2} \right) h \rbrace_{j=1 \dots N}
\]
and so
\begin{equation}
  \int_a^b f(x) dx \approx \sum_{j=1}^N f(x_j) h = \sum_{j=1}^N f\left( a + \frac{(2j-1) h}{2} \right)h 
\end{equation}


\subsubsection{Simpson rule}

The idea is to take the next ``order'' in the Lagrange interpolation formula, the second-order polynomial, and then we can rederive Simpson's rule.  The algebra is worked out in \href{https://github.com/ernestyalumni/CompPhys/blob/master/Cpp/Integrate.ipynb}{Integrate.ipynb}.

From there, then we can obtain Simpson's rule,
\[
\begin{gathered}
  \int_a^b f(x)dx = \sum_{j=1}^{N/2} \int_{a + 2(j-1)h }^{a+2jh} f(x) dx = \sum_{j=1}^{N/2} \frac{h}{3} ( 4 f(a+(2j-1)h ) + f(a+2(j-1)h) + f(a+2jh) ) = \\
  = \frac{h}{3} \left[ f(a) + f(b) + \sum_{j=1}^{N/2} 4f(a+(2j-1) h) + 2\sum_{j=1}^{N/2-1} f(a+2jh) \right]
\end{gathered}
\]

\subsection{Gaussian Quadrature}

cf. Hjorth-Jensen (2015) \cite{Hjor2015}, Section 5.3 Gaussian Quadrature, Chapter 5 Numerical Integration





\section{Call by reference - Call by Value, Call by reference (in C and in C++)}

cf. pp. 58, 2.10 Pointers Ch. 2 Scientific Programming in C, Fitzpatrick \cite{Fitz}
\verb|printfact3.c|, \href{https://github.com/ernestyalumni/CompPhys/blob/master/CFitz/printfact3.c}{printfact3.c}

pass pointer, pass by reference, call by pointer, call by reference 

In C: 
\begin{itemize}
  \item  \emph{function prototype} - 
\[
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{pointer}  &  \textbf{Types} \\
    \textbf{Types}  & \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] { \text{function}} (m-1-2)
  edge node [auto] {*} (m-2-1)
;  
  \end{tikzpicture}   
\qquad \qquad \, \,
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{pointer}  &  \text{void} \\
    \text{double}  & \\ 
  };
  \path[|->]
  (m-1-1) edge node [auto] { \text{function}} (m-1-2)
  edge node [auto] {*} (m-2-1)
;  
  \end{tikzpicture}   
\]
$\Longrightarrow$ 
\begin{lstlisting}
void factorial(double *)
  \end{lstlisting}
where for factorial, it's just your choice of name for \emph{function}.  

\item \emph{function definition} - 
  \[
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{pointers}  &  \textbf{Types} \\
    \textbf{Types}  & \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] { \text{function}} (m-1-2)
  edge node [auto] {*} (m-2-1)
;  
\end{tikzpicture}   \qquad  \,
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{pointer}  &  \text{void} \\
    \text{double}  & \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] { \text{function}} (m-1-2)
  edge node [auto] {*} (m-2-1)
;  
  \end{tikzpicture}   \qquad \, \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{fact}  &  \text{void} \\
    *\text{fact}  & \\ 
  };
  \path[|->]
  (m-1-1) edge node [auto] { \text{function}} (m-1-2)
  edge node [auto] {*} (m-2-1)
;  
  \end{tikzpicture}   
\]
  $\Longrightarrow$
\begin{lstlisting}
  void function(double *fact) { ... }
\end{lstlisting}

\emph{Inside} the function definition,
\[
  \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{pointer}  & \textbf{Dat}_{\text{lvalues}} & \textbf{Types} \\
    \textbf{Memory} & & \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  edge node [auto] {\&} (m-2-1)
  (m-1-2) edge node [auto] {\text{typedef}} (m-1-3)
;  
  \end{tikzpicture} \qquad \qquad \,   
  \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{fact}  & *\text{fact}  & \text{double} \\
    \& \text{fact} & & \\ 
  };
  \path[|->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  edge node [auto] {\&} (m-2-1)
  (m-1-2) edge node [auto] {\text{typedef}} (m-1-3)
;  
  \end{tikzpicture}   
  \]
  and so, for instance, in the function definition, you can do things like this:
  \begin{lstlisting}
    *fact = 1
    *fact *= (double) n 
    \end{lstlisting}
and so notice that from \verb|*fact = 1|, \verb|*fact| is a lvalue.  
  \begin{itemize}
  \item \emph{ function procedure }
\[
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{pointer}  & \textbf{Dat}_{\text{lvalues}}     \\
     \textbf{Memory} &  \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  edge node [auto] {\&} (m-2-1)
  (m-1-2) [loop right] edge node [right] {\text{function procedure}} (m-1-2)
;  
  \end{tikzpicture}    \qquad \qquad \, 
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{fact}  & * \text{fact}     \\
    \& \text{fact} &  \\ 
  };
  \path[|->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  edge node [auto] {\&} (m-2-1)
  (m-1-2) [loop right] edge node [right] {\text{function procedure}} (m-1-2)
;  
    \end{tikzpicture}
\]
$\Longrightarrow $
\begin{lstlisting}
  *fact *= (double) n 
\end{lstlisting}
    \end{itemize}

\item ``Using'' the function, function ``instantiation'', ``calling'' the function, i.e. ``running'' the function
  \[
  \begin{aligned} 
& \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{Types}  &  \textbf{Memory} &     \\
                    & \textbf{pointers} & \textbf{Types}  \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] {$\&$} (m-1-2)
  (m-1-2) edge node [auto] {$\cong$ } (m-2-2)
  (m-2-2)  edge node [auto] {\text{function}} (m-2-3)
  (m-1-1) [loop left] edge node [left] { \text{ function procedure } } (m-1-1)
  ;  
\end{tikzpicture} \qquad \, \\
    & 
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{double}  &  \text{Memory} (\text{Obj}{Memory}) &     \\
                    & \text{pointer} & \text{void}  \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] {$\&$} (m-1-2)
  (m-1-2) edge node [auto] {$\cong$ } (m-2-2)
  (m-2-2)  edge node [auto] {\text{function}} (m-2-3)
  (m-1-1) [loop left] edge node [left] { \text{ function procedure } } (m-1-1)
  ;  
\end{tikzpicture} \qquad \, \\
&
 \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{fact}  &  \& \text{fact}  &     \\
                    & \& \text{fact} & \text{function}(\& \text{ fact})  \\ 
  };
  \path[|->]
  (m-1-1) edge node [auto] {$\&$} (m-1-2)
  (m-1-2) edge node [auto] {$\cong$ } (m-2-2)
  (m-2-2)  edge node [auto] {\text{function}} (m-2-3)
  (m-1-1) [loop left] edge node [left] { \text{ function procedure } } (m-1-1)
  ;  
\end{tikzpicture}
\end{aligned}
    \]
    where, again simply note the notation, that we're using \emph{function} and \emph{factorial}, \emph{fact} for \emph{nameofpointer}, interchangeably: see \href{https://github.com/ernestyalumni/CompPhys/blob/master/CFitz/printfact3.c}{printfact3.c} for the example I'm referring to.

    
\end{itemize}

Again, \emph{in C}, consider \emph{a pointer to a function} passed to another function as an argument.  Take a look at \href{https://github.com/ernestyalumni/CompPhys/blob/master/CFitz/passfunction.c}{passfunction.c} simultaneously.

\begin{itemize}
\item  \emph{function prototype} -
\[
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{pointer}  &  \textbf{Types} \\
    \text{Mor}_{\textbf{Types}}  & \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] { \text{hostfunction}} (m-1-2)
  edge node [auto] {*} (m-2-1)
;  
  \end{tikzpicture}   
\qquad \qquad \, \,
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{pointer}  &  \text{void} \\
    \text{Mor}_{\textbf{Types}}(\text{double},\text{double})  & \\ 
  };
  \path[|->]
  (m-1-1) edge node [auto] { \text{hostfunction}} (m-1-2)
  edge node [auto] {*} (m-2-1)
;  
  \end{tikzpicture}   
\]
$\Longrightarrow$
\begin{lstlisting}
  void hostfunction(double (*)(double))
\end{lstlisting}
We could further generalize this syntax, simply for syntax and notation sake, as such:
\[
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{pointer}  &  \textbf{Types} \\
    \text{Mor}_{\textbf{Types}}  & \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] { \text{hostfunction}} (m-1-2)
  edge node [auto] {*} (m-2-1)
;  
  \end{tikzpicture}   
\qquad \qquad \, \,
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{pointer}  &  \text{data-type} \\
    \text{Mor}_{\textbf{Types}}(\text{typei},\text{typef})  & \\ 
  };
  \path[|->]
  (m-1-1) edge node [auto] { \text{hostfunction}} (m-1-2)
  edge node [auto] {*} (m-2-1)
;  
  \end{tikzpicture}   
\]
$\Longrightarrow$
\begin{lstlisting}
  data-type hostfunction(typef (*)(typei))
\end{lstlisting}

For practice, consider more than 1 argument in our function, and the other argument, for practice, is a pointer, we're ``passing by reference.''

\[
\begin{aligned}
  & 
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    & \textbf{pointers} \times \textbf{pointers}  &   & \textbf{Types} \\
    \textbf{pointers} &  & \textbf{pointers} & \\      
    \text{Mor}_{\textbf{Types}}  &  & \textbf{Types} &  \\ 
  };
  \path[->]
  (m-1-2) edge node [auto] { \text{hostfunction}} (m-1-4)
  (m-1-2) edge node [auto] { $\text{pr}_1$ } (m-2-1)
  edge node [auto] { $\text{pr}_2$ } (m-2-3)
  (m-2-1) edge node [auto] {*} (m-3-1)
  (m-2-3) edge node [auto] {*} (m-3-3)
  ;  
  \end{tikzpicture}   
\qquad \qquad \, \, \\
&
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    & pointer \times pointer  &   & \text{void} \\
    pointer &  & pointers & \\      
    \text{Mor}_{\textbf{Types}}(\text{double},\text{double})  &  & \text{double} &  \\ 
  };
  \path[|->]
  (m-1-2) edge node [auto] { \text{hostfunction}} (m-1-4)
  (m-1-2) edge node [auto] { $\text{pr}_1$ } (m-2-1)
  edge node [auto] { $\text{pr}_2$ } (m-2-3)
  (m-2-1) edge node [auto] {*} (m-3-1)
  (m-2-3) edge node [auto] {*} (m-3-3)
  ;  
  \end{tikzpicture}   
\end{aligned}
\]
$\Longrightarrow$
\begin{lstlisting}
void hostfunction( double (*)(double), double *)
\end{lstlisting}
\item \emph{function definition}

  \[
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{pointers}  &  \textbf{Types} \\
    \text{Mor}_{\textbf{Types}}  & \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] { \text{hostfunction}} (m-1-2)
  edge node [auto] {*} (m-2-1)
;  
\end{tikzpicture}   \qquad  \,
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{pointer}  &  \text{void} \\
    \text{Mor}_{\textbf{Types}}(\text{double},\text{double})  & \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] { \text{hostfunction}} (m-1-2)
  edge node [auto] {*} (m-2-1)
;  
  \end{tikzpicture}   \qquad \, \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{fun}  &  \text{void} \\
    *\text{fun}  & \\ 
  };
  \path[|->]
  (m-1-1) edge node [auto] { \text{hostfunction}} (m-1-2)
  edge node [auto] {*} (m-2-1)
;  
  \end{tikzpicture}   
\]
  $\Longrightarrow$
\begin{lstlisting}
  void hostfunction(double (*fun)(double)) { ... }
\end{lstlisting}

\item \emph{Inside} the function definition,
\[
\begin{aligned}
  & 
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{Types}  &  \textbf{Types} & \textbf{Types} \\
  };
  \path[->]
  (m-1-1) edge node [auto] { $*\text{fun}$} (m-1-2)
  (m-1-2) edge node [auto] {$=$} (m-1-3)
;  
\end{tikzpicture} \quad \, \\
& 
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{double}  &  \text{double} & \text{double} \\
  };
  \path[->]
  (m-1-1) edge node [auto] { $*\text{fun}$} (m-1-2)
  (m-1-2) edge node [auto] {$=$} (m-1-3)
;  
\end{tikzpicture}   \quad \, \\
&
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    x  &  (*\text{fun})(x) & y = (*\text{fun})(x) \\
  };
  \path[|->]
  (m-1-1) edge node [auto] { $*\text{fun}$} (m-1-2)
  (m-1-2) edge node [auto] {$=$} (m-1-3)
;  
  \end{tikzpicture}   
\end{aligned}
\]
$\Longrightarrow $
\begin{lstlisting}
y = (*fun)(x)
  \end{lstlisting}
\item ``Using'' the function - the \emph{actual} syntax for ``passing'' a function into a function is interesting (peculiar?): you only need the \emph{name} of the function.

  Let's quickly recall how a function is prototyped, ``declared'' (or, i.e., defined), and used:
  \begin{itemize}
  \item \emph{function prototype} -
    \[
\begin{aligned}
  &
  \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{Types} & \textbf{Types} \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] { $\text{fun}1$} (m-1-2)
;  
  \end{tikzpicture}   
  \qquad \, \\
  &
  \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{double} & \text{double} \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] { $\text{fun}1$} (m-1-2)
;  
  \end{tikzpicture}   
\end{aligned}
    \]
    $\Longrightarrow $
    \begin{lstlisting}
      double fun1(double)
    \end{lstlisting}
  \item \emph{ function definition } -
    \[
\begin{aligned}
  &
  \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{Types} & \textbf{Types} \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] { $\text{fun}1$} (m-1-2)
;  
  \end{tikzpicture}   
  \qquad \, \\
  &
  \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{double} & \text{double} \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] { $\text{fun}1$} (m-1-2)
;  
  \end{tikzpicture}
  \qquad \, \\
  &
    \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    z & 3.0 z*z - z (= 3z^2 - z) \\ 
  };
  \path[|->]
  (m-1-1) edge node [auto] { $\text{fun}1$} (m-1-2)
;  
  \end{tikzpicture}   
\end{aligned}
    \]
    $\Longrightarrow $
    \begin{lstlisting}
      double fun1(double z) { ... }
    \end{lstlisting}
    \item Using function - \verb| fun1(z) |

        \end{itemize}
  and so
  \[
\text{fun}1 \in \text{Mor}_{\textbf{Types}}(\text{double},\text{double})
\]

And so again, it's interesting in terms of syntax that all you need is the \emph{name} of the function to pass into the arguments of the ``host function'' when using the host function:
\[
\begin{gathered}
    \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
  \text{Mor}_{\textbf{Types}} & \textbf{Types} \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] { $\text{hostfunction}$} (m-1-2)
;  
  \end{tikzpicture}   
    \\
        \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
  \text{Mor}_{\textbf{Types}}(\text{double},\text{double}) & \text{void} \\
  };
  \path[|->]
  (m-1-1) edge node [auto] { $\text{hostfunction}$} (m-1-2)
;  
        \end{tikzpicture}   \\
                \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
  \text{fun}1 & \text{hostfunction}(\text{fun}1) \\
  };
  \path[|->]
  (m-1-1) edge node [auto] { $\text{hostfunction}$} (m-1-2)
;  
  \end{tikzpicture}   
  \end{gathered}
\]
$\Longrightarrow$
\begin{lstlisting}
hostfunction(fun1)
  \end{lstlisting}

\end{itemize}

\subsubsection{C++ extensions, or how C++ pass by reference (pass a pointer to argument) vs. C}

Recall how C passes by reference, and look at Fitzpatrick \cite{Fitz}, pp. 83-84 for the \verb|square| function:

\begin{itemize}
  \item  \emph{function prototype} - 
\[
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{pointer}  &  \textbf{Types} \\
    \textbf{Types}  & \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] { \text{square}} (m-1-2)
  edge node [auto] {*} (m-2-1)
;  
  \end{tikzpicture}   
\qquad \qquad \, \,
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{pointer}  &  \text{void} \\
    \text{double}  & \\ 
  };
  \path[|->]
  (m-1-1) edge node [auto] { \text{square}} (m-1-2)
  edge node [auto] {*} (m-2-1)
;  
  \end{tikzpicture}   
\]
$\Longrightarrow$ 
\begin{lstlisting}
void square(double *)
  \end{lstlisting}

\item \emph{function definition} - 
  \[
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{pointers}  &  \textbf{Types} \\
    \textbf{Types}  & \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] { \text{square}} (m-1-2)
  edge node [auto] {*} (m-2-1)
;  
\end{tikzpicture}   \qquad  \,
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{pointer}  &  \text{void} \\
    \text{double}  & \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] { \text{function}} (m-1-2)
  edge node [auto] {*} (m-2-1)
;  
  \end{tikzpicture}   \qquad \, \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    y  &  \text{void} \\
    *y  & \\ 
  };
  \path[|->]
  (m-1-1) edge node [auto] { \text{square}} (m-1-2)
  edge node [auto] {*} (m-2-1)
;  
  \end{tikzpicture}   
\]
  $\Longrightarrow$
\begin{lstlisting}
  void square(double *y) { ... }
\end{lstlisting}

\emph{Inside} the function definition,
\[
  \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{pointer}  & \textbf{Dat}_{\text{lvalues}} & \textbf{Types} \\
  };
  \path[->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  (m-1-2) edge node [auto] {\text{typedef}} (m-1-3)
;  
  \end{tikzpicture} \qquad \qquad \,   
  \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    y  & *y  & \text{double} \\
  };
  \path[|->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  (m-1-2) edge node [auto] {\text{typedef}} (m-1-3)
;  
  \end{tikzpicture}   
  \]
  and so, for instance, in the function definition, you can do things like this:
  \begin{lstlisting}
    *y = x*x
    \end{lstlisting}

\item ``Using'' the function, function ``instantiation'', ``calling'' the function, i.e. ``running'' the function
  \[
  \begin{aligned} 
& \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{Types}  &  \textbf{Memory} &     \\
                    & \textbf{pointers} & \textbf{Types}  \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] {$\&$} (m-1-2)
  (m-1-2) edge node [auto] {$\cong$ } (m-2-2)
  (m-2-2)  edge node [auto] {\text{square}} (m-2-3)
  (m-1-1) [loop left] edge node [left] { \text{ function procedure } } (m-1-1)
  ;  
\end{tikzpicture} \qquad \, \\
    & 
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{double}  &  \text{Memory} (\text{Obj}{Memory}) &     \\
                    & \text{pointer} & \text{void}  \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] {$\&$} (m-1-2)
  (m-1-2) edge node [auto] {$\cong$ } (m-2-2)
  (m-2-2)  edge node [auto] {\text{square}} (m-2-3)
  (m-1-1) [loop left] edge node [left] { \text{ function procedure } } (m-1-1)
  ;  
\end{tikzpicture} \qquad \, \\
&
 \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{res}  &  \& \text{res}  &     \\
                    & \& \text{res} & \text{square}(\& \text{res})  \\ 
  };
  \path[|->]
  (m-1-1) edge node [auto] {$\&$} (m-1-2)
  (m-1-2) edge node [auto] {$\cong$ } (m-2-2)
  (m-2-2)  edge node [auto] {\text{square}} (m-2-3)
  (m-1-1) [loop left] edge node [left] { \text{ function procedure } } (m-1-1)
  ;  
\end{tikzpicture}
\end{aligned}
    \]
    
\end{itemize}

\subsubsection{C++ syntax for dealing with passing pointers (and arrays) into functions}

However, in \emph{C++}, a lot of the dereferencing $*$ and referencing $\&$ is not explicitly said so in the syntax.  In this syntax, passing by reference is indicated by prepending the $\&$ ampersand to the variable name, in function declaration (prototype and definition).  We don't have to explicitly deference the argument in the function (it's done behind the scene) and syntax-wise (it seems), we only have to refer to the argument by regular local name.

Indeed, the syntax appears ``shortcutted'' greatly:
\begin{itemize}
  \item  \emph{function prototype} - 
\[
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{pointer} \times \textbf{Types}  &  \textbf{Types} \\
  };
  \path[->]
  (m-1-1) edge node [auto] { \text{function}} (m-1-2)
;  
  \end{tikzpicture}   
\qquad \qquad \, \,
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{pointer}, \text{double}  &  \text{void} \\
  };
  \path[|->]
  (m-1-1) edge node [auto] { \text{function}} (m-1-2)
;  
  \end{tikzpicture}   
\]
$\Longrightarrow$ 
\begin{lstlisting}
void function(double &)
  \end{lstlisting}

\item \emph{function definition} - 
  \[
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{pointers}\times \textbf{Types}  &  \textbf{Types} \\
  };
  \path[->]
  (m-1-1) edge node [auto] { \text{square}} (m-1-2)
;  
\end{tikzpicture}   \qquad  \,
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{pointer},\text{double}  &  \text{void} \\
  };
  \path[->]
  (m-1-1) edge node [auto] { \text{function}} (m-1-2)
;  
  \end{tikzpicture}   \qquad \, \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
   \&, y  &  \text{function}(\text{double} \; \& y) \\
  };
  \path[|->]
  (m-1-1) edge node [auto] { \text{function}} (m-1-2)
;  
  \end{tikzpicture}   
\]
  $\Longrightarrow$
\begin{lstlisting}
  void function(double &y) { ... }
\end{lstlisting}

\emph{Inside} the function definition,
\[
  \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{double}   & \text{double} \\
  };
  \path[->]
  (m-1-1) edge node [auto] {$\text{End}(\text{double},\text{double})$} (m-1-2)
;  
  \end{tikzpicture} \qquad \qquad \,   
  \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    y  & y = x*x \\
  };
  \path[|->]
  (m-1-1) edge node [auto] {$\text{End}(\text{double},\text{double})$} (m-1-2)
;  
  \end{tikzpicture}   
  \]
  and so, for instance, in the function definition, you can do things like this:
  \begin{lstlisting}
    y = x*x
    \end{lstlisting}
with no deferencing needed.  
\item ``Using'' the function, function ``instantiation'', ``calling'' the function, i.e. ``running'' the function
  \[
  \begin{aligned} 
& \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{Types}  &  \textbf{Types}      \\
  };
  \path[->]
  (m-1-1) edge node [auto] {$\text{function}$} (m-1-2)
  ;  
\end{tikzpicture} \qquad \,      
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{double}  &  \text{void}  \\
  };
  \path[->]
  (m-1-1) edge node [auto] {$\text{function}$} (m-1-2)
  ;  
\end{tikzpicture} \qquad \, 
 \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \text{res}  &   \text{function}(\text{res})       \\
                     };
  \path[|->]
  (m-1-1) edge node [auto] {$\text{function}$} (m-1-2)
  ;  
\end{tikzpicture}
\end{aligned}
    \]
    
\end{itemize}

\subsubsection{C++ note on arrays}

For dealing with arrays, Stroustrup (2013) \cite{Stro2013}, on pp. 12 of Chapter 1 The Basics, Section 1.8 Pointers, Arrays, and References, does the following:

\begin{itemize}
\item \emph{array declaration} -
  \begin{lstlisting}
    type a[n]; // type[n]; array of n type's
  \end{lstlisting}
\item ``Using'' arrays in function prototypes, i.e. passing into arguments of functions for \emph{function prototypes}
  \begin{lstlisting}
    data-type function( type * arrayname)
    \end{lstlisting}
\item ``Using'' arrays when ``using'' functions, i.e. passing into arguments when a function is ``called'' or ``executed''
  \begin{lstlisting}
    function( arrayname )
  \end{lstlisting}
  \end{itemize}


Fitzpatrick \cite{Fitz} mentions using \verb|inline| for short functions, no more than 3 lines long, because of memory cost of calling a function.  




\subsubsection{Need a CUDA, C, C$++$, IDE?  Try Eclipse!}

This website has a clear, lucid, and pedagogical tutorial for using Eclipse: \href{https://www.fayewilliams.com/2011/06/28/creating-your-first-c-program-in-eclipse/}{Creating Your First C++ Program in Eclipse}.  But it looks like I had to pay.  Other than the well-written tips on the webpage, I looked up stackexchange for my Eclipse questions (I had difficulty with the Eclipse documentation).  

Others, like myself, had questions on how to use an IDE like Eclipse when learning CUDA, and ``building'' (is that the same as compiling?) and running only single files.  

My workflow: I have a separate, in my file directory, folder with my github repository clone that's local.

I start a New Project, CUDA Project, in Eclipse.  I type up my single file (I right click on the \verb|src| folder and add a `Source File`).  I build it (with the Hammer, Hammer looking icon; yes there are a lot of new icons near the top) and it runs.  I can then run it again with the Play, triangle, icon.

I found that if I have more than 1 (2 or more) file in the \verb|src| folder, that requires the \verb|main| function, it won't build right.

So once a file builds and it's good, I, in Terminal, \verb|cp| the file into my local github repository.  Note that from there, I could use the \verb|nvcc| compiler to build, from there, if I wanted to.

Now with my file saved (for example, \verb|helloworldkernel.cu|), then I can delete it, without fear, from my, say, \verb|cuda-workplace|, from the right side, ``C/C$++$ Projects'' window in Eclipse.   

\section{On CUDA By Example}
Take a look at 3.2.2 A Kernel Call, a Hello World in CUDA C, with a simple kernel, on pp. 23 of Sanders and Kandrot (2010) \cite{SK2010} and on github, [helloworldkernel.cu](https://github.com/ernestyalumni/CompPhys/blob/master/CUDA-By-Example/helloworldkernel.cu).  Let's work out the functor interpretation for practice.

\begin{itemize}
\item \emph{function definition} - \[
  \begin{gathered}
    \textbf{Types} \xrightarrow{ \text{ kernel } } \textbf{Types } \\ 
    \text{void} \xrightarrow{ \text{kernel } } \text{ void }
  \end{gathered}
  \]
  where \verb|kernel| $\in $ \verb|__global__| \\
  $\Longrightarrow $
  \begin{lstlisting}
    __global__ void kernel(void) { }
  \end{lstlisting}

CUDA C adds the \verb|__global__| qualifier to standard C to \emph{alert the compiler that the function}, \verb|kernelfunction|, should be compiled to run on the \emph{device}, not the host (pp. 24 \cite{SK2010}).    
\item ``Using'', ``calling'', ``running'' function -
  \[
\begin{aligned}
  & <<<>>>: (n_{\text{block}} , n_{\text{threads}}) \times \text{kernelfunction} \mapsto \text{kernelfunction}<<<n_{\text{block}}, n_{\text{threads}}>>> \in \text{End}(\text{Dat}_{\textbf{Types}}) \\
  & <<<>>>:\mathbb{N}^+ \times \mathbb{N}^+ \times \text{Mor}_{\text{GPU}} \to \text{End}(\text{Dat}_{GPU})
  \end{aligned}
\]
$\Longrightarrow$
\begin{lstlisting}
  kernel<<<1,1>>>();
  \end{lstlisting}
  \end{itemize}

cf. 3.2.3 Passing Parameters of Sanders and Kandrot (2010) \cite{SK2010}

Taking a look at [add-passb.cu](https://github.com/ernestyalumni/CompPhys/blob/master/CUDA-By-Example/add-passb.cu), let's work out the functor interpretation of \verb|cudaMalloc|, \verb|cudaMemcpy|.

In \verb|main|, ``declaring'' a pointer:
\begin{lstlisting}
  int *dev_c
\end{lstlisting}
$\Longleftarrow$
\[
\begin{gathered}
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{pointers}  &   \textbf{Dat}_{\text{lvalues}} & \textbf{Types}      \\    
  };
  \path[->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  (m-1-2) edge node [auto] {\text{typedef}} (m-1-3)
  ;  
\end{tikzpicture} \\
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \verb|dev_c|  &   * \verb|dev_c| & \text{int}      \\    
  };
  \path[|->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  (m-1-2) edge node [auto] {\text{typedef}} (m-1-3)
  ;  
\end{tikzpicture}
  \end{gathered}
\]
We can also do, note, the \verb|sizeof| function (which is a well-defined mapping, for once) on $\text{Obj}\textbf{Types}$:
\[
\begin{gathered}
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{pointers}  &   \textbf{Dat}_{\text{lvalues}} & \textbf{Types} & \mathbb{N}^+     \\    
  };
  \path[->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  (m-1-2) edge node [auto] {\text{typedef}} (m-1-3)
  (m-1-3) edge node [auto] {\text{sizeof}} (m-1-4)
  ;  
\end{tikzpicture} \\
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \verb|dev_c|  &   * \verb|dev_c| & \text{int} & \text{sizeof}(\text{int})     \\    
  };
  \path[|->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  (m-1-2) edge node [auto] {\text{typedef}} (m-1-3)
  (m-1-3) edge node [auto] {\text{sizeof}} (m-1-4)
  ;  
\end{tikzpicture}
  \end{gathered}
\]

Consider what Sanders and Kandrot says about the pointer to the pointer that (you want to) holds the address of the newly allocated memory. \cite{SK2010}  Consider this diagram:

\[
\begin{aligned}
  & \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{pointers}  &   \textbf{pointers} & \textbf{Types}      \\    
  };
  \path[->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  (m-1-2) edge node [auto] {$*$} (m-1-3)
  ;  
\end{tikzpicture} \\
& \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \textbf{pointer}  &   \textbf{pointer} & \text{void}      \\    
  };
  \path[->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  (m-1-2) edge node [auto] {$*$} (m-1-3)
  ;  
  \end{tikzpicture} \\
  & \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=2em, column sep=4em, minimum width=1em]
  {
    \& \verb|dev_c|  &   *(\& \verb|dev_c| ) & (\text{void} **)(\& \verb|dev_c|)      \\    
  };
  \path[->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  (m-1-2) edge node [auto] {$*$} (m-1-3)
  ;  
\end{tikzpicture}
\end{aligned}
\]
I propose that what \verb|cudaMalloc| does (actually) is the following:

\begin{equation}\label{Eq:cudaMallocmodel}
\begin{gathered}
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=3.5em, column sep=4em, minimum width=1em]
  {
    \textbf{Memory}_{\text{GPU}}  &   \textbf{pointers} & \textbf{pointers} & \textbf{Types}      \\    
    & \textbf{pointers}_{\text{GPU}} & \textbf{Types} & \\
  };
  \path[->]
  (m-1-1) edge node [auto] {$\text{cudaMalloc}$} (m-1-2)
  (m-1-2) edge node [auto] {$*$} (m-1-3)
  edge node [right] {$*$} (m-2-2)
  (m-2-2) edge node [auto] {$*$} (m-2-3)
  (m-1-3) edge node [auto] {$*$} (m-1-4)
  ;  
\end{tikzpicture} \\
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=3.5em, column sep=4em, minimum width=1em]
  {
    \text{Memory address}_{\text{GPU}}  &   \& \verb|dev_c|  & *(\& \verb|dev_c| ) & (\text{void} **)(\& \verb|dev_c|)      \\    
    & \verb|dev_c| & *\verb|dev_c| & \\
  };
  \path[|->]
  (m-1-1) edge node [auto] {$\text{cudaMalloc}$} (m-1-2)
  (m-1-2) edge node [auto] {$*$} (m-1-3)
  edge node [right] {$*$} (m-2-2)
  (m-2-2) edge node [auto] {$*$} (m-2-3)
  (m-1-3) edge node [auto] {$*$} (m-1-4)
  ;  
\end{tikzpicture} 
  \end{gathered}
\end{equation}

\verb|dev_c| is now a \emph{device pointer}, available to kernel functions on the GPU.

Syntax-wise, we can relate this diagram to the corresponding function ``usage'':
\[
\begin{gathered}
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=3.5em, column sep=4em, minimum width=1em]
  {
    \textbf{pointers} \times \mathbb{N}^+  & \verb|cudaError_r|  \\
  };
  \path[->]
  (m-1-1) edge node [auto] {$\text{cudaMalloc}$} (m-1-2)
  ;  
\end{tikzpicture} 
\\
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=3.5em, column sep=4em, minimum width=1em]
  {
((\text{void} **)(\& \verb|dev_c|), (\text{sizeof}(\text{int})) ) & \text{cudaSuccess (for example)} \\
  };
  \path[|->]
  (m-1-1) edge node [auto] {$\text{cudaMalloc}$} (m-1-2)
  ;  
\end{tikzpicture} 
  \end{gathered}
\] $\Longrightarrow$
\begin{lstlisting}
  cudaMalloc((void**)&dev_c, sizeof(int))
  \end{lstlisting}

For practice, consider now \verb|cudaMemcpy| in the functor interpretation, and its definition as such:

\verb|cudaMemcpy| is a ``functor category'', s.t. we equip the functor \verb|cudaMemcpy| with a collection of objects $\text{Obj}_{\text{cudaMemcpy}}$, s.t., for example, \verb|cudaMemcpyDevicetoHost| $\in \text{Obj}_{\text{cudaMemcpy}}$, where
\[
( \,  \text{cudaMemcpy}(-,-,n_{\text{thread}}, \text{cudaMemcpyDevicetoHost}): \textbf{Memory}_{GPU} \to \textbf{Memory}_{CPU} \, ) \in \text{Hom}(\textbf{Memory}_{GPU}, \textbf{Memory}_{CPU} )
\]
where $\text{Obj}\textbf{Memory}_{GPU}  \equiv $ collection of all possible memory (addresses) on GPU.

It should be noted that, syntax-wise, $\& c \in \text{Obj}\textbf{Memory}_{CPU}$ and $\& c$ belongs in the ``first slot'' of the arguments for $\text{cudaMemcpy}$, whereas \verb|dev_c| $\in \textbf{pointers}_{GPU}$ a \emph{device pointer}, is ``passed in'' to the ``second slot'' of the arguments for $\text{cudaMemcpy}$.  

\section{Threads, Blocks, Grids}

cf. Chapter 5 Thread Cooperation, Section 5.2. Splitting Parallel Blocks of Sanders and Kandrot (2010) \cite{SK2010}.

Consider first a 1-dimensional block.

\begin{itemize}
\item \verb|threadIdx.x| $\Longleftarrow$ $M_x \equiv $ number of threads per block in $x$-direction.  Let $j_x = 0 \dots M_x-1$ be the index for the thread.  Note that $1 \leq M_x \leq M_x^{\text{max}}$, e.g. $M_x^{\text{max}} = 1024$, max. threads per block 
\item \verb|blockIdx.x| $\Longleftarrow$ $N_x \equiv $ number of blocks in $x$-direction.  Let $i_x = 0\dots N_x-1$
\item \verb|blockDim| stores number of threads along each dimension of the block $M_x$.  
  \end{itemize}

Then if we were to ``linearize'' or ``flatten'' in this $x$-direction,
\[
 k = j_x + i_x M_x
 \]
 where $k$ is the $k$th thread.  $k=0\dots N_xM_x -1$.

 Take a look at \href{https://github.com/ernestyalumni/CompPhys/blob/master/CUDA-By-Example/heattexture1.cu}{heattexture1.cu} which uses the GPU texture memory.  Look at how \verb|threadIdx|/\verb|blockIdx| is mapped to pixel position.

 As an exercise, let's again rewrite the code in mathematical notation:
 \begin{itemize}\label{List:threadblockdict}
\item \verb|threadIdx.x| $\Longleftarrow j_x$, $0\leq j_x \leq M_x -1$ 
\item \verb|blockIdx.x| $\Longleftarrow i_x$, $0\leq i_x \leq N_x -1$
\item \verb|blockDim.x| $\Longleftarrow M_x$, number of threads along each dimension (here dimension $x$) of a block, $1 \leq M_x \leq M_x^{\text{max}} = 1024$
\item \verb|gridDim.x| $\Longleftarrow N_x$, $1\leq N_x$
   \end{itemize}
 resulting in
 \begin{itemize}
 \item $k_x = j_x +i_x M_x$ $\Longrightarrow$
   \begin{lstlisting}
   int x =  threadIdx.x + blockIdx.x * blockDim.x ;
     \end{lstlisting}
\item $k_y = j_y +i_y M_y$ $\Longrightarrow$
   \begin{lstlisting}
   int y =  threadIdx.y + blockIdx.y * blockDim.y ;
     \end{lstlisting}
 \end{itemize}
 and so for a ``flattened'' thread index $J \in \mathbb{N}$,
 \[
J = k_x + N_x\cdot M_x \cdot k_y
\]
$\Longrightarrow $
\begin{lstlisting}
  offset = x + y * blockDim.x * gridDim.x ;
  \end{lstlisting}
 
 
 Suppose vector is of length $N$.  So we \emph{need} $N$ parallel threads to launch, in total. \\
 e.g. if $M_x = 128$ threads per block, $N/128 = N/M_x$ blocks to get our total of $N$ threads running.

 Wrinkle: integer division!  e.g. if $N=127 $, $\frac{N}{128} = 0$.

 Solution: consider $\frac{N+127}{128}$ blocks.  If $N = l\cdot 128 + r$, $l\in \mathbb{N}$, $r = 0 \dots 127$.
 \[
 \begin{gathered}
   \frac{N+127}{128} = \frac{ l \cdot 128 + r + 127 }{128} = \frac{ (l+1)128 + r- 1}{128} = \\
   = l+1 + \frac{r-1}{128} = \begin{cases}
     l & \text{ if } r= 0 \\
     l+1 & \text{ if } r = 1 \dots 127
     \end{cases}
 \end{gathered}
 \]
 \[
 \begin{gathered}
   \frac{ N + (M_x - 1) }{M_x} = \frac{ l\cdot M_x + r+ M_x - 1}{M_x} = \frac{ (l+1)M_x + r-1 }{M_x} = \\
   = l+1 + \frac{r-1}{M_x} = \begin{cases}
     l & \text{ if } r = 0 \\
     l +1 & \text{ if } r = 1 \dots M_x -1 
     \end{cases}
 \end{gathered}
 \]
 
 So $\frac{N+(M_x-1)}{M_x}$ is the smallest multiple of $M_x$ greater than or equal to $N$, so $\frac{N + (M_x- 1)}{M_x}$ \textbf{blocks are needed or more than needed to run a total of $N$ threads.}

 
 Problem: Max. grid dim. in 1-direction is 65535, $\equiv N_i^{\text{max}}$.

 So $\frac{ N+ (M_x-1)}{M_x} = N_i^{\text{max}} \Longrightarrow N = N_i^{\text{max}} M_x - (M_x-1) \leq N_i^{\text{max}} M_x$.  i.e. number of threads $N$ is limited by $N_i^{\text{max}} M_x$.

 Solution.

 \begin{itemize}
 \item number of threads per block in $x$-direction $\equiv M_x \Longrightarrow $ \verb|blockDim.x| \\
 \item number of blocks in grid $\equiv N_x \Longrightarrow $ \verb|gridDim.x| 
 \item $N_x M_x$ total number of threads in $x$-direction.  Increment by $N_xM_x$.  So next scheduled execution by GPU at the $k= N_xM_x$ thread.  
   \end{itemize}

 Sanders and Kandrot (2010) \cite{SK2010} made an important note, on pp. 176-177 Ch. 9 Atomics of Section 9.4 Computing Histograms, an important \emph{rule of thumb} on the number of blocks.

 First, consider $N^{\text{threads}}$ total threads.  The extremes are either $N^{\text{threads}}$ threads on a single block, or $N^{\text{threads}}$ blocks, each with a single thread.

 Sanders and Kandrot gave this tip:  \\

 number of blocks, i.e. \verb|gridDim.x| $\Longleftarrow N_x$ $\sim 2 \times $ number of GPU multiprocessors, i.e. twice the number of GPU multiprocessors.  In the case of my GeForce GTX 980 Ti, it has 22 Multiprocessors.  

 \subsection{global thread Indexing: 1-dim., 2-dim., 3-dim.}

 Consider the problem of \emph{global thread indexing.}  This was asked on the NVIDIA Developer's board (cf. \href{https://devtalk.nvidia.com/default/topic/498642/calculate-global-thread-id/?offset=2}{Calculate GLOBAL thread Id}).  Also, there exists a ``cheatsheet'' (cf. \href{https://cs.calvin.edu/courses/cs/374/CUDA/CUDA-Thread-Indexing-Cheatsheet.pdf}{CUDA Thread Indexing Cheatsheet}).  Let's consider a (mathematical) generalization.

Consider again (cf. \ref{List:threadblockdict}) the following notation: 
\begin{itemize}
\item \verb|threadIdx.x| $\Longleftarrow i_x$, $0\leq i_x \leq M_x -1$, \qquad \, $i_x \in \lbrace 0 \dots M_x - 1\rbrace \equiv I_x$, of ``cardinal length/size'' of $|I_x| = M_x$
\item \verb|blockIdx.x| $\Longleftarrow j_x$, $0\leq j_x \leq N_x -1$, \qquad \, $j_x \in \lbrace 0 \dots N_x - 1\rbrace \equiv J_x$, of ``cardinal length/size'' of $|J_x| = N_x$
\item \verb|blockDim.x| $\Longleftarrow M_x$
  \item \verb|gridDim.x| $\Longleftarrow N_x$
\end{itemize}

Now consider formulating the various cases, of a grid of dimensions from 1 to 3, and blocks of dimensions from 1 to 3 (for a total of 9 different cases) mathematically, as the \href{https://cs.calvin.edu/courses/cs/374/CUDA/CUDA-Thread-Indexing-Cheatsheet.pdf}{CUDA Thread Indexing Cheatsheet} did, similarly:

\begin{itemize}
\item \emph{1-dim. grid} of \emph{1-dim. blocks}.   Consider $J_x\times I_x$.  For $j_x \in J_x$, $i_x \in I_x$, then $k_x = j_x M_x + i_x$, $k_x \in \lbrace 0 \dots N_xM_x-1\rbrace \equiv K_x$.

  The condition that $k_x$ be a valid global thread index is that $K_x$ has equal cardinality or size as $J_x\times I_x$, i.e.
  \[
|J_x \times I_x| = |K_x|
\]
(this must be true).  This can be checked by checking the most extreme, maximal, case of $j_x = N_x-1$, $i_x = M_x-1$:
\[
k_x = j_xM_x + i_x = (N_x-1)M_x + M_x-1 = N_xM_x -1
\]
and so $k_x$ ranges from $0$ to $N_xM_x-1$, and so $|K_x|=N_xM_x$.

Summarizing all of this in the following manner:
\[
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=1.1em, column sep=4em, minimum width=1em]
  {
  J_x \times I_x &  K_x \equiv K^{N_xM_x}= \lbrace 0 \dots N_xM_x -1\rbrace \\ 
  (j_x,i_x) & k_x = j_xM_x +i_x \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] {$$} (m-1-2)
  ;  
  \path[|->]
  (m-2-1) edge node [auto] {$$} (m-2-2)
  ;
\end{tikzpicture} 
\]

For the other cases, this generalization we've just done is implied.  
\item \emph{1-dim. grid} of \emph{2-dim. blocks}
\[
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=1.1em, column sep=4em, minimum width=1em]
  {
  J_x \times (I_x\times I_y) &  K^{N_xM_xM_y} \equiv \lbrace 0 \dots N_xM_xM_y -1\rbrace \\ 
  (j_x,(i_x,i_y)) & k = j_xM_xM_y +(i_x+i_yM_x) = j_x |I_x\times I_y| + (i_x + i_yM_x) \in \lbrace 0 \dots N_xM_xM_y -1 \rbrace \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] {$$} (m-1-2)
  ;  
  \path[|->]
  (m-2-1) edge node [auto] {$$} (m-2-2)
  ;
\end{tikzpicture} 
\]
  %\[
%\begin{gathered}
%  (j_x,i_x)\mapsto k_x = j_xM_x + i_x
%\end{gathered}
%\]
The ``most extreme, maximal'' case that can be checked to check that the ``cardinal size'' of $K^{N_xM_xM_y}$ is equal to $J_x\times (I_x\times I_y)$ is the following, and for the other cases, will be implied (unless explicitly written or checked out):
\[
k = j_x M_xM_y + (i_x + i_y M_x) = (N_x-1)M_xM_y + ((M_x-1) + (M_y-1)M_x) = (N_xM_xM_y-1) 
\]
The thing to notice is this emerging, general pattern, what could be called a ``global view'' of understanding the threads and blocks model of the GPU (cf. \href{https://devtalk.nvidia.com/default/topic/498642/calculate-global-thread-id/?offset=2}{njuffa's answer}:
\[
\text{ total number of threads } = \text{ block index (Id) }\cdot \text{ total number of threads per blcok } + \text{ thread index on the block }
\]
But as we'll see, that's not the only way of ``flattening'' the index, or transforming into a 1-dimensional index.  

\item \emph{1-dim. grid} of \emph{3-dim. blocks}
\[
  \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=1.1em, column sep=4em, minimum width=1em]
  {
  J_x \times (I_x\times I_y \times I_z) &  K^{N_xM_xM_yM_z}  \\ 
  (j_x,(i_x,i_y,i_z)) & k = j_x(M_xM_yM_z) +(i_x+i_yM_x + i_Z M_xM_y)  \in \lbrace 0 \dots N_xM_xM_yM_z -1 \rbrace \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] {$$} (m-1-2)
  ;  
  \path[|->]
  (m-2-1) edge node [auto] {$$} (m-2-2)
  ;
  \end{tikzpicture}
  \]
\item  \emph{2-dim. grid} of \emph{1-dim. blocks}
\[
  \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=1.1em, column sep=4em, minimum width=1em]
  {
  (J_x\times J_y) \times I_x &  L^{N_xN_y} \times I_x & K^{N_xN_yM_x}  \\ 
  ((j_x,j_y), i_x ) & ((j_x+N_xj_y) , i_x ) & k = (j_x + N_xj_y)\cdot M_x + i_x \in \lbrace 0 \dots N_xN_yM_x - 1 \rbrace   \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] {$$} (m-1-2)
  (m-1-2) edge node [auto] {$$} (m-1-3) 
  ;  
  \path[|->]
  (m-2-1) edge node [auto] {$$} (m-2-2)
  (m-2-2) edge node [auto] {$$} (m-2-3)
  ;
\end{tikzpicture} 
  \]
  \item \emph{2-dim. grid} of \emph{2-dim. blocks}
\[
  \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=1.1em, column sep=4em, minimum width=1em]
  {
  (J_x\times J_y) \times (I_x,I_y) &  L^{N_xN_y} \times (I_x,I_y) & K^{N_xN_yM_x}  \\ 
  ((j_x,j_y), (i_x,i_y) ) & ((j_x+N_xj_y) , (i_x,i_y) ) & k = (j_x + N_xj_y)\cdot M_xM_y + i_x +M_xi_y     \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] {$$} (m-1-2)
  (m-1-2) edge node [auto] {$$} (m-1-3) 
  ;  
  \path[|->]
  (m-2-1) edge node [auto] {$$} (m-2-2)
  (m-2-2) edge node [auto] {$$} (m-2-3)
  ;
\end{tikzpicture} 
  \] 
  But this \emph{isn't the only way of obtaining} a ``flattened index.''  Exploit the commutativity and associativity of the Cartesian product:
\[
  \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=1.1em, column sep=4em, minimum width=1em]
  {
 J_x\times J_y \times I_x \times I_y = (J_x \times I_x) \times (J_y \times I_y) &  K^{N_xM_x} \times K^{N_yM_y}   & K^{N_xN_yM_xM_y}  \\ 
 ((j_x,j_y,i_x,i_y ) = ((j_x,i_x),(j_y,i_y)) & (i_x+M_xj_x,i_y+M_yj_j)\equiv (k_x,k_y)  & \begin{gathered} k = k_x + k_yN_xM_x = \\ = (i_x+M_xj_x)+(i_y+M_yj_y)M_xN_x  \end{gathered}   \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] {$$} (m-1-2)
  (m-1-2) edge node [auto] {$$} (m-1-3) 
  ;  
  \path[|->]
  (m-2-1) edge node [auto] {$$} (m-2-2)
  (m-2-2) edge node [auto] {$$} (m-2-3)
  ;
\end{tikzpicture} 
  \] 
  Indeed, checking the ``maximal, extreme'' case,
  \[
k = k_x + k_y N_xM_x = M_xN_x-1 + (M_yN_y - 1)(N_xM_x) = M_yM_yN_xM_x -1
\]
and so $k$ ranges from $0$ to $M_yM_yN_xM_x -1$.






















  \item \emph{3-dim. grid} of \emph{3-dim. blocks}
\[
  \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=1.1em, column sep=4em, minimum width=1em]
  {
    \begin{gathered} (J_x\times J_y \times J_z ) \times (I_x \times I_y \times I_z) = \\
      = (J_x \times I_x) \times (J_y \times I_y) \times (J_z \times I_z) \end{gathered} &  K^{N_xM_x} \times K^{N_yM_y} \times K^{N_zM_z}   & K^{N_xN_yN_zM_xM_yM_z}  \\ 
    \begin{gathered} ((j_x,j_y,j_z),(i_x,i_y,i_z )) = \\ = ((j_x,i_x),(j_y,i_y),(j_z,i_z)) \end{gathered} & \begin{gathered} (i_x+M_xj_x,i_y+M_yj_j,i_z + M_zj_z)\equiv \\
      \equiv (k_x,k_y,k_z) \end{gathered}  & \begin{gathered} k = k_x + k_yN_xM_x + k_zN_xM_xN_yM_y    \end{gathered}   \\ 
  };
  \path[->]
  (m-1-1) edge node [auto] {$$} (m-1-2)
  (m-1-2) edge node [auto] {$$} (m-1-3) 
  ;  
  \path[|->]
  (m-2-1) edge node [auto] {$$} (m-2-2)
  (m-2-2) edge node [auto] {$$} (m-2-3)
  ;
\end{tikzpicture} 
  \] 
  Indeed, checking the ``extreme, maximal'' case for $k$:
  \[
\begin{gathered}
  k =   k_x + k_y N_xM_x + k_zN_xM_xN_yN_y = \\
  = (N_xM_x-1) + (N_yM_y-1)N_xM_x + (N_zM_z-1)N_xM_xN_yM_y = N_xN_yN_zM_xM_yM_z -1
  \end{gathered}
  \]
\end{itemize}



\subsection{(CUDA) Constant Memory}
 
cf. Chapter 6 Constant Memory of Sanders and Kandrot (2010) \cite{SK2010}

Refer to the ray tracing examples in Sanders and Kandrot (2010) \cite{SK2010}, and specifically, here: \href{https://github.com/ernestyalumni/CompPhys/blob/master/CUDA-By-Example/raytrace.cu}{raytrace.cu}, \href{https://github.com/ernestyalumni/CompPhys/blob/master/CUDA-By-Example/rayconst.cu}{rayconst.cu}.

Without constant memory, then this had to be done: 

\begin{itemize}
\item \emph{definition} (in the code) - Consider $\textbf{struct}$ as a subcategory of $\textbf{Types}$ since $\textbf{struct}$ itself is a category, equipped with objects and functions (i.e. methods, modules, etc.).

  So for $\textbf{struct}$, $\text{Obj}\textbf{struct} \ni $ \verb|Sphere|.
  $\Longrightarrow $
  \begin{lstlisting}
    struct sphere { ... }
  \end{lstlisting}
\item Usage, ``instantiation'', i.e. creating, or ``making'' it (the \verb|struct|):

  \[
\begin{gathered}
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=3.5em, column sep=4em, minimum width=1em]
  {
    \textbf{pointers}     & \textbf{Types}  \\
  };
  \path[->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  ;  
\end{tikzpicture}  \\
\begin{aligned}
  & \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=3.5em, column sep=4em, minimum width=1em]
  {
    \textbf{pointer}     & \textbf{struct}  \\
    \textbf{Memory}_{CPU} &  \\
  };
  \path[->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  edge node [left] {$\&$} (m-2-1)
  ;  
\end{tikzpicture} \\
& \begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=3.5em, column sep=4em, minimum width=1em]
  {
    s     & *s & \verb|Sphere|  \\
    \textbf{Memory}\text{address}_{CPU} &  \\
  };
  \path[|->]
  (m-1-1) edge node [auto] {$*$} (m-1-2)
  edge node [left] {$\&$} (m-2-1)
  (m-1-2) edge node [auto] {$\text{typedef}$} (m-1-3)
  ;  
\end{tikzpicture} 
  \end{aligned}
  \end{gathered}
\]
$\Longrightarrow $
\begin{lstlisting}
  Sphere *s
  \end{lstlisting}
  \end{itemize}

Recalling Eq. \ref{Eq:cudaMallocmodel}, for \verb|SPHERES| $== 40$ (i.e. for example, 40 spheres)
\begin{lstlisting}
  cudaMalloc((void **) &s, sizeof(Sphere)*SPHERES)
  \end{lstlisting}
$\Longleftarrow$

\[
\begin{gathered}
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=3.5em, column sep=4em, minimum width=1em]
  {
    \textbf{Memory}_{\text{GPU}}  &   \textbf{pointers} & \textbf{pointers} & \textbf{Types}      \\    
    & \textbf{pointers}_{\text{GPU}} & \textbf{Types} & \\
  };
  \path[->]
  (m-1-1) edge node [auto] {$\text{cudaMalloc}$} (m-1-2)
  (m-1-2) edge node [auto] {$*$} (m-1-3)
  edge node [right] {$*$} (m-2-2)
  (m-2-2) edge node [auto] {$*$} (m-2-3)
  (m-1-3) edge node [auto] {$*$} (m-1-4)
  ;  
\end{tikzpicture} \\
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=3.5em, column sep=4em, minimum width=1em]
  {
    \text{Memory address}_{\text{GPU}}  &   \& s  & *(\& s ) & (\text{void} **)(\& s )      \\    
    & s & *s & \\
  };
  \path[|->]
  (m-1-1) edge node [auto] {$\text{cudaMalloc}$} (m-1-2)
  (m-1-2) edge node [auto] {$*$} (m-1-3)
  edge node [right] {$*$} (m-2-2)
  (m-2-2) edge node [auto] {$*$} (m-2-3)
  (m-1-3) edge node [auto] {$*$} (m-1-4)
  ;  
\end{tikzpicture} 
  \end{gathered}
\]
and syntax-wise,
\[
\begin{gathered}
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=3.5em, column sep=4em, minimum width=1em]
  {
    \textbf{pointers} \times \mathbb{N}^+     & \verb|cudaError_r|  \\
  };
  \path[->]
  (m-1-1) edge node [auto] {\text{cudaMalloc}} (m-1-2)
  ;  
\end{tikzpicture}  \\
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=3.5em, column sep=4em, minimum width=1em]
  {
    ((\text{void} **)(\* s), \text{sizeof}(\text{Sphere}) * \text{SPHERES})     & \text{cudaSuccess (for example) }  \\
  };
  \path[|->]
  (m-1-1) edge node [auto] {\text{cudaMalloc}} (m-1-2)
  ;  
\end{tikzpicture}  \\
\end{gathered}
\]

Now consider
\begin{lstlisting}
  cudaMemcpy(s, temp_s, sizeof(Sphere) * SPHERES, cudaMemcpyHostToDevice)
  \end{lstlisting}

\[
\begin{gathered}
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=3.5em, column sep=12em, minimum width=1em]
  {
    \textbf{Memory}_{CPU}     & \textbf{Memory}_{GPU}  \\
  };
  \path[->]
  (m-1-1) edge node [auto] {$\text{cudaMemcpy}(s, temps ,\text{sizeof}(\text{Sphere}) * \text{SPHERES}, \text{cudaMemcpyHostToDevice} ) $ } (m-1-2)
  ;  
\end{tikzpicture} \\
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=3.5em, column sep=12em, minimum width=1em]
  {
    \textbf{Memory}\text{address}_{CPU}     & \textbf{Memory}\text{address}_{GPU}     \\
    \verb|temp_s| & s \\
  };
  \path[|->]
  (m-1-1) edge node [auto] {$$} (m-1-2)
  edge node [left] {$*$} (m-2-1)
  (m-1-2) edge node [left] {$*$} (m-2-2)
  (m-2-1) edge node [auto] {$$} (m-2-2)
  ;  
\end{tikzpicture} 
  \end{gathered}
\]

The lesson then is this, in light of how long ray tracing takes with constant memory and without constant memory - \verb|cudaMemcpy| between host to device, CPU to GPU, is a costly operation.  Here, in this case, we're copying from the host memory to  memory on the GPU.  It copies to a global memory on the GPU.

Now, using \textbf{constant memory}, \\
we no longer need to do \verb|cudaMalloc|, allocate memory on the GPU, for $s$, pointer to a \verb|Sphere|.

Instead, we have

\begin{lstlisting}
  __constant__ Sphere s[SPHERES];
  \end{lstlisting}
In this particular case, we want it to have global scope.  \\
Note, it is still on host memory.

Notice that
\[
\begin{gathered}
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=3.5em, column sep=18em, minimum width=1em]
  {
    \textbf{Memory}_{CPU}     & \textbf{Memory}_{GPU}  \\
  };
  \path[->]
  (m-1-1) edge node [auto] {$\text{cudaMemcpyHostToDevice}(-, - ,\text{sizeof}(\text{Sphere}) * \text{SPHERES}) $ } (m-1-2)
  ;  
\end{tikzpicture} \\
\begin{tikzpicture}
 \matrix (m) [matrix of math nodes, row sep=3.5em, column sep=18em, minimum width=1em]
  {
    \textbf{Memory}\text{address}_{CPU}     & \textbf{Memory}\text{adddress}_{GPU}  \\
    \verb|temp_s| & s \\
    \text{array of Sphere's} & \text{array of Sphere's} \\
  };
  \path[|->]
  (m-1-1) edge node [auto] {$\text{cudaMemcpyHostToDevice}(-, - ,\text{sizeof}(\text{Sphere}) * \text{SPHERES}) $ } (m-1-2)
  edge node [left] {$*$} (m-2-1)
  (m-1-2) edge node [left] {$*$} (m-2-2)
  (m-2-1) edge node [auto] {$$} (m-2-2)
  edge node [left] {$\text{typedef}$} (m-3-1)
  (m-2-2) edge node [left] {$\text{typedef}$} (m-3-2)
  (m-3-1) edge node [auto] {$$} (m-3-2)
  ;  
\end{tikzpicture}
\end{gathered}
\]
So notice that we have a bijection, and on one level, we can think of the bijection from \verb|temp_s|, an array of Sphere's to $s$, an array of Sphere's.  So notice that the types and memory size of \verb|temp_s| and $s$ must match. 

And for this case, that's all there is to \emph{constant memory}.  What's going on involves the so-called \emph{warp}, a collection of threads, ``woven together'' and get executed in lockstep.  NVIDIA hardware broadcasts a single memory read to each half-warp. ``If every thread in a half-warp requests data from the same address in constant memory, your GPU will generate only a single read request and subsequently broadcast the data to every thread.'' (cf. Sanders and Kandrot (2010) \cite{SK2010}).  Furthermore, ``the hardware can aggressively cache the constant data on the GPU.''

\subsection{(CUDA) Texture Memory}

\subsection{Do (smooth) manifolds admit a triangulation?}

Topics in Geometric Topology (18.937)


\href{http://www.math.harvard.edu/~lurie/937notes/937Lecture2.pdf}{Piecewise Linear Topology (Lecture 2)}

\part{Computational Fluid Dynamics (CFD); Computational Methods} 

\section{On Computational Methods for Aerospace Engineering, via Darmofal, Spring 2005}

Notes to follow along Darmofal (2005) \cite{Darm2005}

\subsection{On Lecture 1, Numerical Integration of Ordinary Differential Equations}

For the 1-dim. case,
\[
m_p \frac{du}{dt} = m_pg - D(u)
\]
Recall the velocity vector field $u=u(t,x) \in \mathfrak{X}(\mathbb{R}\times \mathbb{R})$.  This is \emph{not} what we want in this case; we want for particles the tangent bundle.

\[
\begin{aligned}
  & D = D(u) = \frac{1}{2} \rho_g \pi a^2 u^2 C_D(\text{Re}) \\ 
  & \text{Re} = \frac{2\rho_g u a}{\mu_g} \\
  & C_D = \frac{24}{\text{Re}} + \frac{6}{1+ \sqrt{\text{Re}} } + 0.4
\end{aligned}
\]

Darmofal (2005) \cite{Darm2005} then made a brief aside/note on linearization.

Consider perturbation method (linearization)
\[
u(t) = u_0 + \widetilde{u}(t)
\]
e.g. constant (in time).

If $\frac{du}{dt} = f(u,t)$,
\[
\begin{gathered}
  \frac{d\widetilde{u}}{dt} = f(u_0 + \widetilde{u},t) = f(u_0,t) + \left. \frac{ \partial f}{ \partial u} \right|_{u_0,0} \widetilde{u} + \left. \frac{ \partial f}{ \partial t} \right|_{u_0,0} t + \mathcal{O}(t^2, \widetilde{u}t, \widetilde{u}^2 )
  \end{gathered}
\]

$a = 0.01 \, m$, $\rho_p = 917 \, \text{kg}/m^3$ \qquad $\rho_g = 0.9 \, \text{kg}/m^3$

$m_p = \rho_p \frac{4}{3} \pi a^3 = 0.0038 \, \text{kg}$

$\mu_g = 1.69 \times 10^{-5} \, \text{kg}/(m \, \text{sec})$

$g=9.8 \, m/s^2$

In the 3-dim. case,
\[
m_p \frac{d\mathbf{u}}{dt} = m_p g - D(u) \frac{\mathbf{u}}{ |u| }
\]

Consider curve $\begin{aligned} & x: \mathbb{R} \to N = \mathbb{R} \\
  & x(t) \in \mathbb{R} \end{aligned}$,  $u(t) \equiv \frac{dx}{dt} \in \Gamma(TN) = \Gamma(T\mathbb{R})$

\[
\frac{du}{dt} = g - \frac{D(u)}{m_p}
\]

\[
\frac{du}{dt} = (u(t+\Delta t) - u(t)) \frac{1}{\Delta t} + \mathcal{O}(\Delta t)
\]



\subsection{Multi-step methods generalized}

This subsection corresponds to \href{http://ocw.mit.edu/courses/aeronautics-and-astronautics/16-901-computational-methods-in-aerospace-engineering-spring-2005/lecture-notes/lect3.pdf}{Lecture 3: Convergence of Multi-Step Methods}, but is a further generalization to the presented multi-step methods.

The problem to solve, the ODE to compute out, is
\begin{equation}
\frac{du}{dt}(t) = f(u(t),t)
\end{equation}

Make the following ansatz:
\begin{equation}
  \frac{du}{dt}(t) = \sum_{\nu = 0 }^N \frac{1}{h} C_{\nu} u(t-\nu h) = \sum_{\xi = 1}^P \beta_{\xi} f(u(t-\xi h) , t-\xi h)
\end{equation}
Do the Taylor expansion:  
\[
\begin{gathered}
  \sum_{\nu =0}^N \frac{1}{h} C_{\nu} \left[ u(t) + \left( \frac{du}{dt} \right)(t) \cdot (-\nu h) + \sum_{j=2}^n \frac{u^{(j)}(t) }{j!} (-\nu h)^j + \mathcal{O}(h^n) \right] = \sum_{\xi = 1}^P \beta_{\xi} \frac{du}{dt}(t-\xi h) = \\
  = \sum_{\xi =1}^P \beta_{\xi} \left[ \frac{du}{dt} + \sum_{j=2}^n \frac{u^{(j)}(t) }{j!} (-\xi h)^j + \mathcal{O}(h^n) \right]
  \end{gathered}
\]


\subsection{Convection (Discretized)}

While I am following Lecture 7 of Darmofal (2005) \cite{Darm2005}, I will generalize to a ``foliated, spatial'' (smooth) manifold $N$, parametrized by time $t\in \mathbb{R}$, $\mathbb{R}\times N$, with $\text{dim}N = n = 1,2 \text{ or } 3$ and to \emph{CUDA} C/C++ parallel programming.

Consider $n$-form $m \in \Omega^N(\mathbb{R}\times N)$, $\text{dim}N = n$.  Then
\begin{equation}\label{Eq:massconservation}
  \begin{gathered}
    \frac{d}{dt} m = \frac{d}{dt} \int_V \rho \text{vol}^n = \int_V \mathcal{L}_{\frac{ \partial }{ \partial t} + \mathbf{u} } \rho \text{vol}^n = \int_V \frac{ \partial \rho }{ \partial t} \text{vol}^n + \mathbf{d}i_{\mathbf{u}} \rho \text{vol}^n = \int_V \left( \frac{ \partial \rho }{ \partial t} + \text{div}( \rho u ) \right) \text{vol}^n = \\
    = \int_V \frac{ \partial \rho }{ \partial t} \text{vol}^n + \int_{\partial V} \rho i_{\mathbf{u}} \text{vol}^n = \dot{m}
\end{gathered}
  \end{equation}
where recall
\[
\begin{aligned}
  \text{div} : \mathfrak{X}(\mathbb{R}\times N) \to C^{\infty}(\mathbb{R} \times N ) \\ 
 \text{div}(\rho \mathbf{u} ) = \frac{1}{\sqrt{g}} \frac{ \partial ( \sqrt{g} u^i \rho ) }{ \partial x^i } 
  \end{aligned}
\]

\subsubsection{1-dimensional case for Convection from mass (scalar) conservation}

Consider Cell $i$, between $x_{i-\frac{1}{2} }$ and $ x_{i+\frac{1}{2} }$, i.e. $[x_{i-\frac{1}{2} }, x_{i+\frac{1}{2} }]\subset \mathbb{R}$.  In this case, Eq. \ref{Eq:massconservation}, for mass conservation with sources, becomes
\[
\begin{gathered}
  \int_V \frac{ \partial \rho}{ \partial t} \text{vol}^n + \int_{\partial V} \rho i_{\mathbf{u}} \text{vol}^n = \int_V \frac{ \partial \rho }{ \partial t} dx + \int_{\partial V} \rho u^i = \int_{x_L}^{x_R} \frac{ \partial \rho }{ \partial t} dx + (\rho(x_R)u(x_R) - \rho(x_L)u(x_L) ) = \frac{d}{dt} \int_{x_L}^{x_R} \rho(x) dx
  \end{gathered}
\]

In the case of $\frac{d}{dt}m = 0$, on a single cell $i$,
\[
\begin{gathered}
  \int_{x_{i-\frac{1}{2} }}^{x_{i+\frac{1}{2} } } \frac{ \partial \rho }{ \partial t} dx + \left. \rho(x) u(x) \right|_{x_i + \frac{1}{2} } - \left. \rho(x)u(x) \right|_{x_i-\frac{1}{2} } = 0 
  \end{gathered}
\]

This is one of the first main approximations Darmofal (2005) \cite{Darm2005} makes, in Eq. 7.10, Section 7.3 Finite Volume Method for Convection, for the \emph{finite volume method}:
\begin{equation}
  \overline{m}_i := \frac{1}{\Delta x_i} \int_{x_{i - \frac{1}{2} }}^{x_{i+\frac{1}{2} } } \rho(x)dx
\end{equation}
where $\Delta x_i \equiv x_{i+\frac{1}{2}} - x_{i+\frac{1}{2}}$.

And so
\begin{equation}
  \begin{gathered}
    \Delta x_i \frac{\partial }{ \partial t} \overline{m}_i + \left. \rho(x)u(x) \right|_{x_{i+\frac{1}{2} } } - \left. \rho(x)u(x) \right|_{x_{i-\frac{1}{2} } } =0
  \end{gathered}
  \end{equation}
We want to discretize this equation also in time.

Consider as first approximation,
\begin{equation}
  \overline{m}(x,t) = \overline{m}_i(t) \qquad \, \forall \, x_{i-\frac{1}{2}} < x < x_{i+\frac{1}{2}}
\end{equation}

Consider then initial time $t$, time step $\Delta t$.   

\subsubsection{1-dimensional ``Upwind'' Interpolation for Finite Volume}

This is the ``major'' approximation for the so-called ``Upwind'' interpolation approximation:
\begin{equation}\label{Eq:rhoUpwindInterp}
  \rho(x_{i+\frac{1}{2} },t+\Delta t) = \begin{cases} \overline{m}_i(t) & \text{ if } u(x_{i+\frac{1}{2} },t) > 0 \\
    \overline{m}_{i+1}(t) & \text{ if } u(x_{i+\frac{1}{2} },t) < 0
    \end{cases}
\end{equation}

Then use the so-called ``forward'' time approximation for $\frac{d}{dt} \overline{m}_i(t)$:
\[
\begin{gathered}
\Delta x_i \frac{ \overline{m}_i(t+\Delta t) - \overline{m}_i(t) }{ \Delta t} + (\rho u)(t,x_{i+\frac{1}{2} } ) - (\rho u)(t,x_{i-\frac{1}{2} } ) = 0
  \end{gathered}
\]

Darmofal (2005) \cite{Darm2005} didn't make this explicit in Lecture 7, but in the approximation for  $\rho(x_{i+\frac{1}{2} },t+\Delta t)$, Eq. \ref{Eq:rhoUpwindInterp}, it's supposed that it's valid at time $t$: $\rho(x_{i+\frac{1}{2} },t) \approx  \rho(x_{i+\frac{1}{2} },t+\Delta t)$, since it's the value of $\rho$ for time moving forward from $t$ (this is implied in Darmofal's code \href{http://ocw.mit.edu/courses/aeronautics-and-astronautics/16-901-computational-methods-in-aerospace-engineering-spring-2005/lecture-notes/convect1d.m}{convect1d}
  %\verb|convect1d.m|}.
\[
\begin{gathered}
  \rho(x_{i+\frac{1}{2} },t)u(x_{i+\frac{1}{2} },t) = \begin{cases} \overline{m}_i(t)u(x_{i+\frac{1}{2} },t) & \text{ if } u(x_{i+\frac{1}{2}},t )>0 \\
    \overline{m}_{i+1}(t)u(x_{i+\frac{1}{2} },t) & \text{ if } u(x_{i+\frac{1}{2}},t )<0
    \end{cases}
  \end{gathered}
\]

Then
\begin{equation}
\begin{gathered}
\frac{ \Delta x_i }{ \Delta t}( \overline{m}_i(t+\Delta t) - \overline{m}_i(t) ) + \\
+ \begin{cases} \overline{m}_i(t)u(x_{i+\frac{1}{2} },t) & \text{ if } u(x_{i+\frac{1}{2}},t )>0 \\
    \overline{m}_{i+1}(t)u(x_{i+\frac{1}{2} },t) & \text{ if } u(x_{i+\frac{1}{2}},t )<0
\end{cases} - \\
- \begin{cases} \overline{m}_{i-1}(t)u(x_{i-\frac{1}{2} },t) & \text{ if } u(x_{i-\frac{1}{2}},t )>0 \\
    \overline{m}_{i}(t)u(x_{i-\frac{1}{2} },t) & \text{ if } u(x_{i-\frac{1}{2}},t )<0
\end{cases} = \\
= 0
  \end{gathered}
  \end{equation}

A note on 1-dimensional gridding: Consider total length $L_0 \in \mathbb{R}^+$.  \\
For $N^{\text{cells}}$ total cells in $x$-direction.  $i=0\dots N^{\text{cells}}-1$.
\[
\begin{aligned}
  & x_{i-\frac{1}{2}} = i \Delta x \qquad \, & i = 0, 1 \dots N^{\text{cells}} - 1 \\ 
  & x_{i+\frac{1}{2}} = (i+1) \Delta x \qquad \, & i = 0, 1 \dots N^{\text{cells}} - 1 \\ 
  & x_i = x_{i-\frac{1}{2} } + \frac{ x_{i+\frac{1}{2}} - x_{i-\frac{1}{2} } }{2} = \frac{ x_{i + \frac{1}{2} } + x_{i-\frac{1}{2} } }{2} = (2 i + 1)\frac{  \Delta x }{2} \qquad \, & i = 0, 1 \dots N^{\text{cells}} - 1 \\ 
\end{aligned}
\]



At this point, instead of what is essentially the so-called ``Upwind Interpolation'', which Darmofal is doing in Lecture 7 of Darmofal (2005) \cite{Darm2005}, and on pp. 76, Chapter 4 Finite Volume Methods, Subsection 4.4.1 Upwind Interpolation (UDS) of Ferziger and Peric (2002) \cite{FP2013}, which is essentially a zero-order approximation, let's try to do better.  

Consider the interval $[x_{i-\frac{1}{2}}, x_{i+\frac{1}{2} } ] \subset \mathbb{R}$.

For the 1-dimensional case of (pure) convection,
\[
\begin{gathered}
  \int_{x_{i - \frac{1}{2}}}^{x_{i+\frac{1}{2} } } \frac{ \partial \rho (t,x) }{ \partial t } dx + \rho(t,x_{ i +\frac{1}{2} } ) u(t,x_{i +\frac{1}{2} } ) - \rho(t, x_{i -\frac{1}{2} } )u(t,x_{i -\frac{1}{2}} ) = \frac{d}{dt} \int_{x_{i -\frac{1}{2} } }^{ x_{i +\frac{1}{2} } } \rho(x) dx
  \end{gathered}
\]
Given $\rho(t,x_{i -\frac{1}{2} }), \rho(t, x_{i +\frac{1}{2} }) \in \mathbb{R}$, do (polynomial) interpolation:
\[
\begin{gathered}
  \mathbb{R} \times \mathbb{R} \xrightarrow{ \text{ interpolation }} \mathbb{R}[x] \equiv \mathcal{P}_{n=1}(\mathbb{R}) \\ 
 \rho(t,x_{i -\frac{1}{2} }), \rho(t, x_{i +\frac{1}{2} }) \mapsto \frac{ (x - x_{ i -\frac{1}{2}}) \rho(t, x_{i +\frac{1}{2}}) - (x - x_{i +\frac{1}{2}} )\rho(t,x_{i -\frac{1}{2}}) }{ h } = \rho_{n=1}(t,x)
\end{gathered}
\]
where $h \equiv x_{i +\frac{1}{2}} - x_{i -\frac{1}{2}}$ and $\mathcal{P}_{n=1}(\mathbb{R})$ is the set of all polynomials of order $n=1$ over field $\mathbb{R}$ (real numbers).

In general,
\[
\begin{gathered}
  \mathbb{R} \times \mathbb{R} \xrightarrow{ \text{ interpolation }} \mathbb{R}[x] \equiv \mathcal{P}_{n=1}(\mathbb{R}) \\ 
 \rho(t,x_L), \rho(t, x_R) \mapsto \frac{ (x - x_L ) \rho(t, x_R) - (x - x_R )\rho(t,x_L) }{ (x_R-x_L) } = \rho_{n=1}(t,x)
\end{gathered}
\]


We interchange the operations of integration and partial derivative - I (correct me if I'm wrong) give two possible reasons why we can do this: the spatial manifold $N$ is fixed in time $t$, and if the grid cell itself is fixed in time, then the partial derivative in time can be moved out of the integration limits.

So, interchanging $\int_{x_{i -\frac{1}{2}}}^{x_{i +\frac{1}{2}}} dx$ and $\frac{ \partial }{ \partial t}$:
\[
\int_{x_{i -\frac{1}{2}}}^{x_{i +\frac{1}{2}}} \frac{ \partial \rho(t,x) }{ \partial t} dx = \frac{ \partial }{ \partial t} \int_{x_{i -\frac{1}{2}}}^{x_{i +\frac{1}{2}}} \rho(t,x)dx
\]
So then
\[
\Longrightarrow \frac{ \partial }{ \partial t} \int_{x_{i -\frac{1}{2}}}^{x_{i +\frac{1}{2}}} \rho_{n=1}(t,x) = \frac{ \partial }{ \partial t} ( \rho(t,x_{i+\frac{1}{2} }  )  + \rho(t,x_{i-\frac{1}{2} } ) ) \frac{ \Delta x}{2}
\]
where $\Delta x = x_{i+\frac{1}{2} } - x_{i+\frac{1}{2} } $.

In general,
\[
\frac{ \partial }{ \partial t} \int_{x_L}^{x_R} \rho_{n=1}(t,x) = \frac{ \partial }{ \partial t}( \rho(t,x_R) + \rho(t,x_L) ) \frac{ (x_R - x_L) }{2}
\]


Then, discretizing,
\begin{equation}
  \Longrightarrow \begin{gathered}
    \left[ (\rho(t+\Delta t, x_{i+\frac{1}{2} } ) + \rho(t+\Delta t, x_{i-\frac{1}{2} } ) ) - (\rho(t, x_{i+\frac{1}{2} } ) + \rho(t, x_{i-\frac{1}{2} } ) ) \right] \frac{\Delta x}{2} \left( \frac{1}{\Delta t} \right) + \rho(t,x_{i+\frac{1}{2} } )u(t,x_{i+\frac{1}{2} } ) - \rho(t,x_{i-\frac{1}{2} } )u(t,x_{i-\frac{1}{2} } ) = \\
    = \dot{m}_{[x_{i-\frac{1}{2} }, x_{i+\frac{1}{2} }] }(t)
  \end{gathered}
  \end{equation}

To obtain $\rho(t,x_{i-\frac{1}{2}})$, consider
\[
\frac{ \partial \rho}{ \partial t} + \text{div}(\rho u) = \frac{d\rho}{dt} = 0 
\]
which is valid at every point on $N$.

Consider for $\text{dim}N=1$,
\[
\frac{ \partial \rho}{ \partial t}(t,x) + \frac{ \partial (\rho u) }{ \partial x}(t,x)
\]

Now, we want $x = x_{i-\frac{1}{2} }$.

Consider
\[
\frac{ \partial \rho(t,x_{i-\frac{1}{2} }) }{ \partial t} \approx \frac{ \rho(t+\Delta t,x_{i-\frac{1}{2} })  - \rho(t,x_{i-\frac{1}{2} } ) }{\Delta t}
\]

Next, consider the (polynomial) interpolation for the $ \frac{ \partial (\rho u) }{ \partial x}(t,x)$ term:
\[
\begin{gathered}
  \mathbb{R} \times \mathbb{R}\times\mathbb{R} \xrightarrow{ \text{interpolate} } \mathbb{R}[x] \equiv \mathcal{P}_{n=2}(\mathbb{R}) \\ 
 \rho(t,x_{i-\frac{3}{2} } ) u(t,x_{i-\frac{3}{2} }) , \rho(t,x_{i-\frac{1}{2} })u(t,x_{i-\frac{1}{2} } ), \rho(t,x_{i+\frac{1}{2} })u(t,x_{i+\frac{1}{2} } ) \xmapsto{ \text{interpolate} } (\rho u)_{n=2}(t,x)
  \end{gathered}
\]
Thus, we can calculate, by plugging into,
\[
\frac{ \partial (\rho u)_{n=2}(t,x_{i-\frac{1}{2} } ) }{ \partial x}
\]

In general, for
\[
\begin{gathered}
  \mathbb{R} \times \mathbb{R}\times\mathbb{R} \xrightarrow{ \text{interpolate} } \mathbb{R}[x] \equiv \mathcal{P}_{n=2}(\mathbb{R}) \\ 
 \rho(t,x_{LL } ) u(t,x_{LL }) , \rho(t,x_{L })u(t,x_{L } ), \rho(t,x_{R })u(t,x_{R } ) \xmapsto{ \text{interpolate} } (\rho u)_{n=2}(t,x)
  \end{gathered}
\]
we have
\[
\begin{gathered}
  \frac{ \partial (\rho u)_{n=2}(t,x_{L } ) }{ \partial x} = 
  \frac{1}{\left(x_{L} - x_{LL}\right) \left(x_{L} - x_{R}\right) \left(x_{LL} - x_{R}\right)} \cdot \\
  \cdot \left(\left(x_{L} - x_{LL}\right)^{2} (\rho u){\left (x_{R} \right )} + \left(x_{L} - x_{LL}\right) \left(x_{LL} - x_{R}\right) (\rho u){\left (x_{L} \right )} - \left(x_{L} - x_{R}\right)^{2} (\rho u){\left (x_{LL} \right )} + \left(x_{L} - x_{R}\right) \left(x_{LL} - x_{R}\right) (\rho u){\left (x_{L} \right )}\right)
\end{gathered}
\]

Thus,
\begin{equation}
\Longrightarrow \begin{gathered}
  \rho(t+\Delta t, x_{i-\frac{1}{2} } ) - \rho(t,x_{i-\frac{1}{2} } ) + \frac{ \partial (\rho u)_{n=2} }{\partial x}(t,x_{i-\frac{1}{2} } )\Delta t = 0  \text{ or } \\
  \rho(t+\Delta t,x_{i-\frac{1}{2} } ) = \rho(t,x_{i-\frac{1}{2} } ) - \frac{ \partial (\rho u)_{n=2} }{ \partial x}(t,x_{i-\frac{1}{2} })\Delta t
  \end{gathered}
\end{equation}

Now a note on the 1-dimensional grid, ``gridding'': for cell $i=0, \dots N^{\text{cell}} -1$, $N^{\text{cell}}$ cells total in the $x$-direction, then
\[
\begin{aligned}
  & x_{i-\frac{1}{2} } = ih \\ 
  & x_{i-\frac{1}{2} } = (i+1)h 
  \end{aligned}
\]
and so $x_{i+\frac{1}{2} } - x_{i-\frac{1}{2} } = h$, meaning the cell width or cell size is $h$.

Thus, in summary, 
\begin{equation}
\begin{gathered}
  \rho(t+\Delta t,x_{i-\frac{1}{2} }) = \rho(t,x_{i-\frac{1}{2} }) - ( \rho(t,x_{i+\frac{1}{2} })u(t,x_{i+\frac{1}{2} } ) - \rho(t,x_{i-\frac{3}{2} })u(t,x_{i-\frac{3}{2} } ) )\left( \frac{1}{2h} \right) \Delta t \\
  \left[ (\rho(t+\Delta t, x_{i+\frac{1}{2} } ) + \rho(t+\Delta t, x_{i-\frac{1}{2} } ) ) - (\rho(t, x_{i+\frac{1}{2} } ) + \rho(t, x_{i-\frac{1}{2} } ) ) \right] \frac{h}{2} \left( \frac{1}{\Delta t} \right) + \rho(t,x_{i+\frac{1}{2} } )u(t,x_{i+\frac{1}{2} } ) - \rho(t,x_{i-\frac{1}{2} } )u(t,x_{i-\frac{1}{2} } ) = \\
    = \dot{m}_{[x_{i-\frac{1}{2} }, x_{i+\frac{1}{2} }] }(t)
  \end{gathered}
  \end{equation}

If one was to include Newtonian gravity, consider this general expression for the time derivative of the momentum flux $\Pi$:
\begin{equation}
\begin{aligned}
  & \Pi = \int_{B(t)} \rho u^i \text{vol}^n \otimes e_i \\ 
  & \dot{\Pi} = \int_{B(t)} \frac{ \partial (\rho u^i ) }{ \partial t} \text{vol}^n \otimes e_i + \int_{B(t)} d(\rho u^i i_u \text{vol}^n ) \otimes e_i = \int_{B(t)} \frac{ \partial (\rho u^i )}{ \partial t} \text{vol}^n \otimes e_i + \int_{\partial B(t)} \rho u^i i_u \text{vol}^n \otimes e_i
\end{aligned}
  \end{equation}
In 1-dim.,
\[
\dot{\Pi} = \int_{B(t)} \frac{ \partial (\rho u)}{\partial t } dx + \int_{\partial B} \rho u^2 = \int_B \frac{GM dm }{r^2} = GM \int_B \frac{ \rho \text{vol}^n }{r^2} = GM \int_B \frac{ \rho dx}{(R-x)^2}
\]
Considering a first-order polynomial interpolation for $\rho$,$\rho_{n=1}$,
\[
\begin{gathered}
  \frac{ \partial }{ \partial t} ((\rho u)(t,x_{i+\frac{1}{2} } ) + (\rho u)(t,x_{i-\frac{1}{2} }) )\frac{h}{2} + \rho u^2(t,x_{i+\frac{1}{2} }) - \rho u^2(t,x_{i-\frac{1}{2} }) = GM \int \frac{ \rho_{n=2} dx}{ (R-x)^2}
  \end{gathered}
\]
Note that we need another equation, at $x=x_{i-\frac{1}{2}}$, similar to above:
\[
\begin{gathered}
  \frac{ \partial (\rho u) }{ \partial t} + \frac{ \partial (\rho u^2) }{ \partial x} = \frac{GM\rho}{(R-x)^2 } \\ 
    \Longrightarrow \begin{gathered}
      \rho u(t+\Delta t,x_{i-\frac{1}{2} } ) - \rho u(t,x_{i-\frac{1}{2} } ) + (\rho u^2(t,x_{i + \frac{1}{2} } ) - \rho u^2(t,x_{i - \frac{3}{2} } ) ) \left( \frac{1}{2h } \right) \Delta t = \Delta t \int GM \frac{ \rho dx }{ (R-x)^2 }
      \end{gathered}
  \end{gathered}
\]

As a recap, the 1-dimensional setup is as follows:
\[
\begin{aligned}
  & \mathbb{R} \times N = \mathbb{R} \times \mathbb{R} \xrightarrow{ \text{discretization }} \mathbb{Z} \times \mathbb{Z} \\ 
  & (t,x) \xmapsto{ \text{discretization} } (t_0 + (\Delta t)j, x_{i-\frac{1}{2} } = ih), \qquad \, i,j \in \mathbb{Z}
  \end{aligned}
\]
Initial conditions for $\rho \in C^{\infty}(\mathbb{R}\times \mathbb{R})$: $\rho(t_0,x) \in C^{\infty}(\mathbb{R}\times \mathbb{R})$.

Choices for $u\in \mathfrak{X}(\mathbb{R}\times \mathbb{R})$:
\begin{itemize}
\item $u(t,x) = u(x)$ (i.e. time-independent velocity vector field)
\item $u(t,x)$ determined by Newtonian gravity (that's an external force on the fluid)
  \end{itemize}

\subsubsection{Note on 1-dimensional gridding}

For, $[0,1] \subset \mathbb{R}$ \\
\phantom{For, } $N$ cells,

Then $1/N = \Delta x$.  Then consider
\[
x_j = j\Delta x \qquad \, j = 0, 1, \dots N
\]

\subsection{2-dim. and 3-dim. ``Upwind'' interpolation for Finite Volume}

I build on Lecture 7 of Darmofal (2005) \cite{Darm2005}.

Consider a rectangular grid.

Consider cell $C^2_{ij}$, $i=0\dots N_x - 1$, $j=0\dots N_y -1$.  Then there's $N_x\cdot N_y$ total cells, $\begin{aligned} & \quad \\
  & N_x \text{ cells in $x$-direction } \\ 
  & N_y \text{ cells in $y$-direction } \end{aligned}$.

There are 2 possibilities: rectangles of all the same size, with width $l^x$ and length $l^t$ each, or each rectangle for each cell $C^2_{ij}$ is different, of dimensions $l^x_i \times l^y_j$.  

Consider cells centered at $x_{2i+1} = l^x\frac{(2i+1)}{2} = \sum_{k=0}^{i-1}l_k^x + \frac{l_i^2}{2}$.  

On the ``left'' sides, $x_{2i} = l^x i = \sum_{k=0}^{i-1} l_k^x$ \\
\phantom{On the } ``right'' sides, $x_{2(i+1)} = l^x(i+1) = \sum_{k=0}^i l_k^x$.

So cells are centered at
\[
(x_{2i+1}, y_{2j+1}) = (l^x \frac{ (2i+1)}{2} , l^y \frac{(2j+1)}{2} ) = \left( \sum_{k=0}^{i-1} l_k^x + \frac{l_i^x}{2} , \sum_{k=0}^{j-1}l_k^y + \frac{l_j^y}{2} \right)
\]

So this cell $C^2_{ij}$, a 2-(cubic) simplex has 4 1-(cubic) simplices (edges): so 1-(cubic) simplices $\lbrace C_{i\pm 1, j }^1, C^1_{i,j\pm1}\rbrace$

The center of these simplices are the following:
\[
\begin{aligned}
  & x_{C^1_{i+1,j}} = (x_{2i+1+1},y_{2j+1}) = (l^x(i+1), l^y\frac{(2j+1)}{2} ) = \left( \sum_{k=0}^i l_k^x, \sum_{k=0}^{j-1} l_k^y + \frac{l_j^y}{2} \right) \text{ so then } \\ 
  & x_{C^1_{i\pm 1,j}} = (x_{2i+1\pm 1},y_{2j+1}) = (l^x\left( \frac{2i+1\pm 1}{2}\right) , l^y\frac{(2j+1)}{2} ) = \left( \sum_{k=0}^{ \frac{2i - 1 \pm 1 }{2} } l_k^x, \sum_{k=0}^{j-1} l_k^y + \frac{l_j^y}{2} \right) \\ 
  &  x_{C^1_{i,j\pm 1}} = (x_{2i+1},y_{2j+1 \pm 1}) = (l^x\left( \frac{2i+1}{2}\right) , l^y\frac{(2j+1 \pm 1)}{2} ) = \left(  \sum_{k=0}^{i-1} l_k^x + \frac{l_i^x}{2} , \sum_{k=0}^{ \frac{2j - 1 \pm 1 }{2} } l_k^y  \right)
\end{aligned}
\]

We want the flux.  So for
\[
\overline{\rho}_{ij} := \frac{1}{l_i^x l_j^y} \int_{C^2_{ij}} \rho \text{vol}^2
\]
then the flux through 1-(cubic) simplices (faces), $\int \rho i_{\mathbf{u}}\text{vol}^2$,
\[
\begin{aligned}
 &  \int_{C^1_{i+1,j}} \rho i_{\mathbf{u}} \text{vol}^2 = \begin{cases} l_j^y \overline{\rho}_{ij} u^x(x_{C^1_{i+1, j} } ) & \text{ if } u^x(x_{C^1_{i+1,j} }) > 0 \\ 
 l_j^y \overline{\rho}_{i+1,j} u^x(x_{C^1_{i+1, j} } ) & \text{ if } u^x(x_{C^1_{i+1,j} }) < 0 \end{cases} \\
&      \int_{C^1_{i-1,j}} \rho i_{\mathbf{u}} \text{vol}^2 = \begin{cases} -l_j^y \overline{\rho}_{i-1,j} u^x(x_{C^1_{i-1, j} } ) & \text{ if } u^x(x_{C^1_{i-1,j} }) > 0 \\ 
 -l_j^y \overline{\rho}_{i,j} u^x(x_{C^1_{i-1, j} } ) & \text{ if } u^x(x_{C^1_{i-1,j} }) < 0 \end{cases}
\end{aligned}
\]
Likewise,
\[
\int_{C^1_{i,j+1}} \rho i_{\mathbf{u}} \text{vol}^2 = \begin{cases} l_i^x \overline{\rho}_{ij} u^y(x_{C^1_{i, j+1} } ) & \text{ if } u^y(x_{C^1_{i,j+1} }) > 0 \\ 
 l_i^x \overline{\rho}_{i,j+1} u^y(x_{C^1_{i, j+1} } ) & \text{ if } u^y(x_{C^1_{i,j+1} }) < 0 \end{cases}
\]
and so on.



\subsubsection{3-dim. ``Upwind'' interpolation for finite volume }

For a rectangular prism (cubic),

for cell $C^3_{ijk}$, $i=0 \dots N_x-1$, $j=0 \dots N_y-1$ , $k=0 \dots N_z-1$, $N_x\cdot N_y \cdot N_z$ total cells.

Cells centered at
\[
(x_{2i+1}, y_{2j+1}, z_{2j+1}) = (l^x \frac{ (2i+1)}{2} , l^y \frac{(2j+1)}{2}, l^z \frac{(2k+1)}{2} ) = \left( \sum_{l=0}^{i-1} l_l^x + \frac{l_i^x}{2} , \sum_{l=0}^{j-1}l_l^y + \frac{l_j^y}{2}, \sum_{l=0}^{k-1}l_l^z + \frac{l_k^y}{2}  \right)
\]
For the 3-(cubic) simplex, $C^3_{ijk}$, it has 6 2-(cubic) simplices (faces).  So for $C_{ijk}^3$, consider $\lbrace C^2_{i\pm 1, jk}, C^2_{ij\pm 1,k}, C^2_{ijk\pm 1}\rbrace$.

The center of these faces, such as for $C^2_{i\pm 1, jk}$, $x_{ C^2_{i\pm 1, jk} }$, for instance,
\[
\begin{aligned}
 x_{C^2_{i\pm 1,jk}} = (x_{2i+1\pm 1},y_{2j+1},z_{2k+1}) = (l^x\left( \frac{2i+1\pm 1}{2}\right) , l^y\frac{(2j+1)}{2},  l^z\frac{(2j+1)}{2} ) = \left( \sum_{l=0}^{ \frac{2i - 1 \pm 1 }{2} } l_l^x, \sum_{l=0}^{j-1} l_l^y + \frac{l_j^y}{2}, \sum_{l=0}^{l-1} l_l^z + \frac{l_k^z}{2} \right) 
  \end{aligned}
\]

We want the flux.  So for
\[
\overline{\rho}_{ijk} := \frac{1}{l_i^x l_j^yl^z_k} \int_{C^3_{ijk}} \rho \text{vol}^3
\]
then the flux through 2-(cubic) simplices (faces), $\int \rho i_{\mathbf{u}}\text{vol}^3$,
\[
\begin{aligned}
 &  \int_{C^2_{i+1,jk}} \rho i_{\mathbf{u}} \text{vol}^3 = \begin{cases} l_j^yl_k^z \overline{\rho}_{ijk} u^x(x_{C^2_{i+1, jk} } ) & \text{ if } u^x(x_{C^2_{i+1,jk} }) > 0 \\ 
 l_j^y l^z_k \overline{\rho}_{i+1,jk} u^x(x_{C^2_{i+1, jk} } ) & \text{ if } u^x(x_{C^2_{i+1,jk} }) < 0 \end{cases} \\
\end{aligned}
\]
and so on.  

To reiterate the so-called ``upwind'' interpolation method, in generality, recall that we are taking this equation:
\[
\int_{C^n_{ij}} \frac{ \partial \rho }{ \partial t} \text{vol}^n + \int_{ \partial C^n_{ij} } \rho u_{\mathbf{u}} \text{vol}^n = \dot{M}_{ij}
\]
and discretizing it to obtain
\[
\begin{gathered}
  \frac{ \partial }{ \partial t} \overline{\rho}_{ij} | \text{vol}^n | + \int_{ \partial C_{ij}^n } \rho i_{\mathbf{u}} \text{vol}^n = \dot{M}_{ij } \\ 
\Longrightarrow  \frac{ \partial }{ \partial t} \overline{\rho}_{ij}  = \frac{-1}{ | \text{vol}^n | } \int_{ \partial C_{ij}^n } \rho i_{\mathbf{u}} \text{vol}^n + \frac{1}{ | \text{vol}^n | } \dot{M}_{ij }
  \end{gathered}
\]

\section{Finite Difference}

References/Links that I used:
\begin{itemize}
\item \href{http://www.ann.jussieu.fr/~frey/cours/UdC/ma691/ma691_ch6.pdf}{Chapter 6 The finite difference method, by Pascal Frey}
\item \href{https://www.wias-berlin.de/people/john/LEHRE/NUM_PDE_FUB/num_pde_fub.pdf}{Numerical Methods for Partial Differential Equations by Volker John}
\item \href{https://en.wikipedia.org/wiki/Finite_difference}{\emph{Wikipedia} ``Finite Difference''}.  Wikipedia has a section on Difference operators which appears powerful and general, but I haven't understood how to apply it.  In fact, see my jupyter notebook on the \verb|CompPhys| github, \verb|finitediff.ipynb| on how to calculate the coefficients in arbitrary (differential) order, and (error) order (of precision, error, i.e. $O(h^p)$) for finite differences, approximations of derivatives.  
\end{itemize}

From \verb|finitediff.ipynb|, I derived this formula
\begin{equation}
  \begin{gathered}
    f'(x) = \frac{1}{h} \sum_{\nu =1}^3 C_{\nu} \cdot (f(x+\nu h)-f(x-\nu h)) + \mathcal{O}(h^7) \text{ for } \\
    \begin{aligned}
      & C_1 = \frac{3}{4} \\ 
      & C_2 = \frac{-3}{20 } \\ 
      & C_3 = \frac{1}{60}
      \end{aligned}
  \end{gathered}
  \end{equation}

\subsection{Finite Difference with Shared Memory (CUDA C/C++)}

References/Links that I used:
\begin{itemize}
\item \href{https://devblogs.nvidia.com/parallelforall/finite-difference-methods-cuda-c-part-2/}{Finite Difference Methods in CUDA C++, Part 2, by Dr. Mark Harris}
\item \href{http://www.bu.edu/pasi/files/2011/07/Lecture31.pdf}{GPU Computing with CUDA
Lecture 3 - Efficient Shared Memory Use, Christopher Cooper of Boston University}, August, 2011.  UTFSM, Valpara\'{i}so, Chile.   
  \end{itemize}

cf. \href{https://devblogs.nvidia.com/parallelforall/finite-difference-methods-cuda-c-part-2/}{Finite Difference Methods in CUDA C++, Part 2, by Dr. Mark Harris}

In $x$-derivative, $\frac{ \partial f}{ \partial x}$, $\forall \, $ thread block, $(j_x,j_y) \in \lbrace \lbrace 0 \dots N_x - 1 \rbrace \times \lbrace 0 \dots N_y-1 \rbrace$.  $m_x\times s_{\text{Pencils}}$ elements $\in$ tile e.g. $64 \times \text{sPencils}$.

In $y$-derivative, $\frac{ \partial f}{ \partial y}$, $(x,y)$-tile of $\text{sPencils} \times 64 = s_{\text{Pencils}} \times m_y$.

Likewise, $\frac{ \partial f}{ \partial z} \to (x,z)-$tile of $\text{sPencils} \times 64 = s_{\text{Pencils}} \times m_z$.  

Consider for the $y$ derivative, the code for \verb|__global__ void derivative_y(*f, *d_f)| (\href{https://github.com/ernestyalumni/CompPhys/blob/master/moreCUDA/finitediff.cu}{finitediff.cu}):

\verb|int i| $\Longleftarrow i = j_x M_x + i_x \in \lbrace 0 \dots N_xM_x-1\rbrace$ (since $j_xM_x + i_x = (M_x-1)+(N_x-1)M_x$, i.e. the ``maximal'' case) \\
\verb|int j| $\Longleftarrow j = i_y \in \lbrace 0 \dots M_y-1 \rbrace$ \\
\verb|int k| $\Longleftarrow k = j_y \in \lbrace 0 \dots N_y-1 \rbrace$ \\
\verb|int si| $\Longleftarrow si = i_x \in \lbrace 0 \dots M_x-1\brace$ \\
\verb|int sj| $\Longleftarrow sj = j+4 \in \lbrace 4 \dots M_y+3 \rbrace$.  Notice that $r=4$.  Then generalize to $s_j = j + r\in \lbrace r, \dots , M_y + r-1 \rbrace$ \\
\verb|int globalIdx| $\Longleftarrow l = km_xm_y + jm_x + i\in \lbrace 0 , \dots , (N_y -1)m_xm_y + (M_y-1)m_x + N_xM_x -1 \rbrace$ since
\[
(N_y-1)m_xm_y + (M_y-1)m_X + N_xM_x -1
\]
\verb|s_f[sj][si]| $\Longleftarrow s_f[s_j][s_i] \equiv (s_f)_{s_j,s_i} = f(l)$, $l\in \lbrace 0 \dots (N_y - 1)m_xm_y + (M_y-1)m_x + N_xM_x -1 \rbrace$ with
\[
s_f\in \text{Mat}_{\mathbb{R}}(M_y + r, M_x)
\]

If $j< 4$, $j<r$, $j = s_j-r \in \lbrace 0 \dots r-1 \rbrace$ and so
\[
\begin{gathered}
(s_f)_{ (s_j-r), s_i } = (s_f)_{ s_j + m_y - 1-r, s_i} \\ 
  \Longleftrightarrow \\
  \lbrace 0 , \dots r-1\rbrace \times \lbrace 0 \dots M_x-1\rbrace \xleftarrow{} \lbrace m_y - 1, \dots M_y + m_y -2 \rbrace \times \lbrace 0 \dots M_x - 1 \brace
  \end{gathered}
\]
Then, the actual approximation method:
\[
\frac{ \partial f}{ \partial y}(l) = \frac{ \partial f}{ \partial y}(i,j,k) = \sum_{\nu =1}^r c_{\nu} ((s_f)_{s_j + \nu, s_i} - (s_f)_{s_j-\nu,s_i} )
\]

The shared memory tile here is
\[
\begin{gathered}
\verb|__shared__ float s_f[m_y+8][sPencils]| \Longleftarrow s_f \in \text{Mat}_{\mathbb{R}}(m_y+2r, s_{\text{Pencil}})
  \end{gathered}
\]
By using the shared memory tile, each element from global memory is read only once. (cf. \href{https://devblogs.nvidia.com/parallelforall/finite-difference-methods-cuda-c-part-2/}{Finite Difference Methods in CUDA C++, Part 2, by Dr. Mark Harris}) 

Consider expanding the number of pencils in the shared memory tile, e.g. 32 pencils.

Harris says that ``with a 1-to-1 mapping of threads to elements where the derivative is calculated, a thread block of 2048 threads would be required.''  Consider then letting each thread calculate the derivative for multiple points.  

So Harris uses a thread block of $32\times 8 \times 1 = 256$ threads per block, and have each thread calculate the derivative at 8 points, as opposed to a thread block of $4*64*1=256$ thread block, with each thread calculate the derivative at only 1 point.

Perfect coalescing is then regained.





\href{http://www.bu.edu/pasi/files/2011/07/Lecture31.pdf}{GPU Computing with CUDA
Lecture 3 - Efficient Shared Memory Use, Christopher Cooper}

\pagebreak

\subsection{Note on finite-difference methods on the shared memory of the device GPU, in particular, the pencil method, that attempts to improve upon the double loading of boundary ``halo'' cells (of the grid)}

cf. \href{https://devblogs.nvidia.com/parallelforall/finite-difference-methods-cuda-c-part-1/}{Finite Difference Methods in CUDA C++, Part 1, by Dr. Mark Harris}

Take a look at the code \href{https://github.com/parallel-forall/code-samples/blob/master/series/cuda-cpp/finite-difference/finite-difference.cu}{finite\_difference.cu}.  The full code is there.  In particular, consider how it launches blocks and threads in the kernel function (and call) \verb|__global__| \verb|derivative_x|, \verb|derivative_y|, \verb|derivative_z|.  \verb|setDerivativeParameters| has the arrays containing \verb|dim3| ``instantiations'' that have the grid and block dimensions, for x-,y-,z-derivatives and for ``small'' and ``long pencils''.  Consider ``small pencils'' for now.  The relevant code is as follows:
\begin{lstlisting}
 grid[0][0]  = dim3(my / sPencils, mz, 1);
  block[0][0] = dim3(mx, sPencils, 1);

  grid[0][1]  = dim3(my / lPencils, mz, 1);
  block[0][1] = dim3(mx, sPencils, 1);

  grid[1][0]  = dim3(mx / sPencils, mz, 1);
  block[1][0] = dim3(sPencils, my, 1);

  grid[1][1]  = dim3(mx / lPencils, mz, 1);
  // we want to use the same number of threads as above,
  // so when we use lPencils instead of sPencils in one
  // dimension, we multiply the other by sPencils/lPencils
  block[1][1] = dim3(lPencils, my * sPencils / lPencils, 1);

  grid[2][0]  = dim3(mx / sPencils, my, 1);
  block[2][0] = dim3(sPencils, mz, 1);

  grid[2][1]  = dim3(mx / lPencils, my, 1);
  block[2][1] = dim3(lPencils, mz * sPencils / lPencils, 1);
\end{lstlisting}

Let $N_i \equiv $ total number of cells in the grid in the $i$th direction, $i=x,y,z$.  $N_i$ corresponds to \verb|m*| in the code, e.g. $N_x$ is \verb|mx|.  

Note that in this code, what seems to be attempted is calculating the derivatives of a 3-dimensional grid, but using only 2-dimensions on the memory of the device GPU.  In my experience, with the NVIDIA GeForce GTX 980 Ti, the maximum number of threads per block in the $z$-direction and the maximum number of blocks that can be launched in the $z$-direction is severely limited compared to the $x$ and $y$ directions (use \verb|cudaGetDeviceProperties|, or run the code \href{https://github.com/ernestyalumni/CompPhys/blob/master/CUDA-By-Example/queryb.cu}{queryb.cu}; I find
\begin{equation}\label{Code:GTX980Tiproperties}
\end{equation}
  \begin{lstlisting}
 Max threads per block: 1024
Max thread dimensions: (1024, 1024, 64) 
Max grid dimensions:   (2147483647, 65535, 65535) 
\end{lstlisting}
).  

Let $M_i$ be the number of threads on a block in the $i$th direction.  Let $N^{\text{threads}}_i$ be the total number of threads in the $i$th direction on the grid, i.e. the number of threads in the $i$th grid-direction.  $i=x,y$. This is \emph{not} the desired grid dimension $N_i$.   

Surely, for a desired grid of size $N_x\times N_y \times N_z \equiv N_xN_yN_z$, then a total of $N_xN_yN_z$ threads are to be computed.

Denote $s_{\text{pencil}} \in \mathbb{Z}^+$ to be \verb|sPencils|; example value is $s_{\text{pencil}} = 4$.  Likewise, denote $l_{\text{pencil}} \in \mathbb{Z}^+$ to be \verb|lPencils|; example value is $l_{\text{pencil}} = 32 > s_{\text{pencil}}=4$.  

Then, for instance the small pencil case, for the $x$-derivative, we have
\begin{equation}\label{Eq:gridblockspencilx}
\begin{aligned}
  \text{ grid dimensions } & (N_y/s_{\text{pencil}}, N_z,1) \\ 
\text{ block dimensions } &  (N_x, s_{\text{pencil}}, 1)
  \end{aligned}
\end{equation}
Then the total number of threads launched in each direction, $x$ and $y$, is
\[
\begin{aligned}
  & N_x^{\text{threads}} = \frac{N_y}{s_{\text{pencil}}} N_x \\ 
   & N_y^{\text{threads}} = \frac{N_z}{s_{\text{pencil}}}  \\ 
  \end{aligned}
\]
While it is true that the total number of threads computed matches our desired grid:
\[
N_x^{\text{threads}} \cdot N_y^{\text{threads}} = \frac{N_y}{s_{\text{pencil}}} N_x N_z s_{\text{pencil}} = N_x N_y N_z
\]
take a look at the block dimensions that were demanded in Eq. \ref{Eq:gridblockpencilx}, $(N_x,s_{\text{pencil}},1)$.  The total number of threads to be launched in this block is $N_x \cdot s_{\text{pencil}}$.  Suppose $N_x = 1920$.  Then easily $N_x\cdot s_{\text{pencil}} > $ allowed maximum number of threads per block.  In my case, this number is 1024.

Likewise for the case of $x$-direction, but with long pencils.  The blocks and threads to be launched on the grid and blocks for the kernel function (\verb|derivative_x|) is
\begin{equation}\label{Eq:gridblocklpencilx}
\begin{aligned}
  \text{ grid dimensions } & (N_y/l_{\text{pencil}}, N_z,1) \\ 
\text{ block dimensions } &  (N_x, s_{\text{pencil}}, 1)
  \end{aligned}
\end{equation}
The total number of threads to be launched in each block is also $N_x \cdot s_{\text{pencil}}$ and for large $N_x$, this could easily exceed the maximum number of threads per block allowed.

Also, be aware that the shared memory declaration is
\begin{lstlisting}
  __shared__ float s_f[sPencils][mx+8]
\end{lstlisting}

$N_x$ (i.e. \verb|mx|) can be large and we're requiring a 2-dim. array of size $(N_x+8)*s_{\text{pencil}}$ of floats, for each block.  As, from Code listing \ref{Code:GTX980Tiproperties}, much more blocks can be launched than threads on a block, and so trying to launch more blocks could possibly be a better solution.  



\pagebreak

\section{Mapping scalar (data) to colors}

Links I found useful:

Taku Komura has good lectures on visualization with computers; it was heavily based on using VTK, but I found the principles and overview he gave to be helpful: here's \href{http://www.inf.ed.ac.uk/teaching/courses/vis/lecture_notes/lecture6.pdf}{Lecture 6 Scalar Algorithms: Colour Mapping}.  (Komura's teaching in the UK, hence spelling ``colour'')

Good article on practical implementation of a rainbow: \url{https://www.particleincell.com/2014/colormap/}


\end{multicols*}
\begin{thebibliography}{9}
\bibitem{HTF2009}
Trevor Hastie, Robert Tibshirani, Jerome Friedman.   \textbf{The Elements of Statistical Learning: Data Mining, Inference, and Prediction}, Second Edition (Springer Series in Statistics) 2nd ed. 2009. Corr. 7th printing 2013 Edition.  ISBN-13: 978-0387848570.  \url{https://web.stanford.edu/~hastie/local.ftp/Springer/OLD/ESLII_print4.pdf}

\bibitem{CS2013}
Jared Culbertson, Kirk Sturtz.  \emph{Bayesian machine learning via category theory}.  \href{http://arxiv.org/abs/1312.1445}{arXiv:1312.1445} [math.CT]

\bibitem{CS344}
John Owens.  David Luebki.  \emph{Intro to Parallel Programming}.  \emph{CS344}.  \textbf{\href{https://www.udacity.com/}{Udacity}}  
  
\url{http://arxiv.org/abs/1312.1445} Also, \url{https://github.com/udacity/cs344}  

\bibitem{CS229}
CS229 Stanford University.  \url{http://cs229.stanford.edu/materials.html}


\bibitem{Fitz}
Richard Fitzpatrick.  ``Computational Physics.''  \url{http://farside.ph.utexas.edu/teaching/329/329.pdf}

\bibitem{Hjor2015}
 M. Hjorth-Jensen, \textbf{Computational Physics}, University of Oslo (2015) \url{http://www.mn.uio.no/fysikk/english/people/aca/mhjensen/}

\bibitem{Stro2013}
 Bjarne Stroustrup.  \textbf{A Tour of C++} (C++ In-Depth Series). Addison-Wesley Professional.   2013. 
 
\bibitem{SK2010}
Jason Sanders, Edward Kandrot.  \textbf{CUDA by Example: An Introduction to General-Purpose GPU Programming} 1st Edition.  Addison-Wesley Professional; 1 edition (July 29, 2010).  ISBN-13: 978-0131387683

\bibitem{Darm2005}
  David Darmofal. *16.901 Computational Methods in Aerospace Engineering, Spring 2005.* (Massachusetts Institute of Technology: MIT OpenCourseWare), \url{http://ocw.mit.edu} (Accessed 12 Jun, 2016). \href{http://creativecommons.org/licenses/by-nc-sa/4.0/}{License: Creative Commons BY-NC-SA}

\bibitem{FP2013}
Joel H. Ferziger and Milovan Peric.  \textbf{Computational Methods for Fluid Dynamics}.  Springer; 3rd edition (October 4, 2013).  ISBN-13: 978-3540420743

I used the 2002 edition since that was the only copy I had available.  

\end{thebibliography}

\end{document}
